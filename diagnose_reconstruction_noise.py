#!/usr/bin/env python3
"""
è¯Šæ–­MAEé‡å»ºå™ªå£°é—®é¢˜
åˆ†æå¯èƒ½çš„åŸå› ï¼šæ¨¡å‹ã€è§£ç å™¨ã€æ•°æ®é¢„å¤„ç†ç­‰
"""

import os
import sys
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torchvision.transforms as transforms
import models_mae
from animediffusion_dataset_loader import create_animediffusion_dataloader
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_mae_model_variants():
    """åŠ è½½ä¸åŒé…ç½®çš„MAEæ¨¡å‹è¿›è¡Œå¯¹æ¯”"""
    print("\nğŸ” åŠ è½½ä¸åŒMAEæ¨¡å‹é…ç½®...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")
    
    models = {}
    
    # 1. å®Œå…¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹
    print("  ğŸ“¦ åˆ›å»ºéšæœºåˆå§‹åŒ–æ¨¡å‹...")
    random_model = models_mae.mae_vit_base_patch16()
    random_model = random_model.to(device)
    random_model.eval()
    models['random'] = random_model
    
    # 2. åªåŠ è½½ç¼–ç å™¨é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹
    print("  ğŸ“¦ åˆ›å»ºç¼–ç å™¨é¢„è®­ç»ƒæ¨¡å‹...")
    encoder_pretrained = models_mae.mae_vit_base_patch16()
    
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    if os.path.exists(pretrain_path):
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        
        # åªåŠ è½½ç¼–ç å™¨æƒé‡
        encoder_state_dict = {}
        for key, value in checkpoint['model'].items():
            if not key.startswith('decoder') and key != 'mask_token':
                encoder_state_dict[key] = value
        
        encoder_pretrained.load_state_dict(encoder_state_dict, strict=False)
        print("    âœ… ç¼–ç å™¨é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
    
    encoder_pretrained = encoder_pretrained.to(device)
    encoder_pretrained.eval()
    models['encoder_pretrained'] = encoder_pretrained
    
    # 3. æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆåŒ…å«è§£ç å™¨ï¼‰
    print("  ğŸ“¦ æ£€æŸ¥å®Œæ•´é¢„è®­ç»ƒæ¨¡å‹...")
    try:
        # å°è¯•åŠ è½½å¯èƒ½åŒ…å«è§£ç å™¨çš„æ¨¡å‹
        full_model = models_mae.mae_vit_base_patch16()
        
        # æ£€æŸ¥checkpointä¸­çš„keys
        if os.path.exists(pretrain_path):
            checkpoint = torch.load(pretrain_path, map_location='cpu')
            decoder_keys = [k for k in checkpoint['model'].keys() if k.startswith('decoder')]
            
            if len(decoder_keys) > 0:
                print(f"    âœ… å‘ç°è§£ç å™¨æƒé‡: {len(decoder_keys)} ä¸ªå‚æ•°")
                full_model.load_state_dict(checkpoint['model'])
                full_model = full_model.to(device)
                full_model.eval()
                models['full_pretrained'] = full_model
            else:
                print("    âŒ é¢„è®­ç»ƒæ¨¡å‹ä¸­æ²¡æœ‰è§£ç å™¨æƒé‡")
        
    except Exception as e:
        print(f"    âŒ åŠ è½½å®Œæ•´æ¨¡å‹å¤±è´¥: {e}")
    
    return models, device

def analyze_model_components(models, device):
    """åˆ†ææ¨¡å‹å„ç»„ä»¶çš„çŠ¶æ€"""
    print("\nğŸ”¬ åˆ†ææ¨¡å‹ç»„ä»¶...")
    
    for model_name, model in models.items():
        print(f"\n  ğŸ“Š {model_name} æ¨¡å‹åˆ†æ:")
        
        # æ£€æŸ¥ç¼–ç å™¨å‚æ•°ç»Ÿè®¡
        encoder_params = []
        for name, param in model.named_parameters():
            if not name.startswith('decoder') and name != 'mask_token':
                encoder_params.append(param.data.flatten())
        
        if encoder_params:
            encoder_tensor = torch.cat(encoder_params)
            print(f"    ç¼–ç å™¨å‚æ•°ç»Ÿè®¡:")
            print(f"      å‡å€¼: {encoder_tensor.mean().item():.6f}")
            print(f"      æ ‡å‡†å·®: {encoder_tensor.std().item():.6f}")
            print(f"      èŒƒå›´: [{encoder_tensor.min().item():.6f}, {encoder_tensor.max().item():.6f}]")
        
        # æ£€æŸ¥è§£ç å™¨å‚æ•°ç»Ÿè®¡
        decoder_params = []
        for name, param in model.named_parameters():
            if name.startswith('decoder') or name == 'mask_token':
                decoder_params.append(param.data.flatten())
        
        if decoder_params:
            decoder_tensor = torch.cat(decoder_params)
            print(f"    è§£ç å™¨å‚æ•°ç»Ÿè®¡:")
            print(f"      å‡å€¼: {decoder_tensor.mean().item():.6f}")
            print(f"      æ ‡å‡†å·®: {decoder_tensor.std().item():.6f}")
            print(f"      èŒƒå›´: [{decoder_tensor.min().item():.6f}, {decoder_tensor.max().item():.6f}]")
        else:
            print("    âŒ æ²¡æœ‰è§£ç å™¨å‚æ•°")

def test_reconstruction_quality(models, device):
    """æµ‹è¯•ä¸åŒæ¨¡å‹çš„é‡å»ºè´¨é‡"""
    print("\nğŸ¯ æµ‹è¯•é‡å»ºè´¨é‡...")
    
    # åˆ›å»ºç®€å•æµ‹è¯•å›¾åƒ
    test_img = create_simple_test_image(device)
    
    # åå½’ä¸€åŒ–ç”¨äºæ˜¾ç¤º
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    results = {}
    
    for model_name, model in models.items():
        print(f"\n  ğŸ” æµ‹è¯• {model_name} æ¨¡å‹...")
        
        with torch.no_grad():
            try:
                # æµ‹è¯•é‡å»º
                loss, pred, mask = model(test_img, mask_ratio=0.75)
                reconstructed = model.unpatchify(pred)
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                pred_stats = {
                    'mean': pred.mean().item(),
                    'std': pred.std().item(),
                    'min': pred.min().item(),
                    'max': pred.max().item()
                }
                
                reconstructed_stats = {
                    'mean': reconstructed.mean().item(),
                    'std': reconstructed.std().item(),
                    'min': reconstructed.min().item(),
                    'max': reconstructed.max().item()
                }
                
                results[model_name] = {
                    'loss': loss.item(),
                    'pred_stats': pred_stats,
                    'reconstructed_stats': reconstructed_stats,
                    'success': True
                }
                
                print(f"    æŸå¤±: {loss.item():.4f}")
                print(f"    é¢„æµ‹ç»Ÿè®¡: å‡å€¼={pred_stats['mean']:.4f}, æ ‡å‡†å·®={pred_stats['std']:.4f}")
                print(f"    é‡å»ºç»Ÿè®¡: å‡å€¼={reconstructed_stats['mean']:.4f}, æ ‡å‡†å·®={reconstructed_stats['std']:.4f}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
                if abs(pred_stats['mean']) > 10 or pred_stats['std'] > 10:
                    print(f"    âš ï¸  é¢„æµ‹å€¼å¼‚å¸¸ï¼å¯èƒ½å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸æˆ–å‚æ•°åˆå§‹åŒ–é—®é¢˜")
                
                if abs(reconstructed_stats['mean']) > 5 or reconstructed_stats['std'] > 5:
                    print(f"    âš ï¸  é‡å»ºå€¼å¼‚å¸¸ï¼å¯èƒ½å¯¼è‡´å™ªå£°")
                
            except Exception as e:
                print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                results[model_name] = {'success': False, 'error': str(e)}
    
    return results

def create_simple_test_image(device):
    """åˆ›å»ºç®€å•çš„æµ‹è¯•å›¾åƒ"""
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¸å˜å›¾åƒ
    img = torch.zeros(1, 3, 224, 224, device=device)
    
    for i in range(224):
        for j in range(224):
            # æ ‡å‡†åŒ–åçš„æ¸å˜
            r = (i / 224 - 0.485) / 0.229
            g = (j / 224 - 0.456) / 0.224
            b = (0.5 - 0.406) / 0.225
            
            img[0, 0, i, j] = r
            img[0, 1, i, j] = g
            img[0, 2, i, j] = b
    
    return img

def diagnose_noise_sources(models, device):
    """è¯Šæ–­å™ªå£°æ¥æº"""
    print("\nğŸ” è¯Šæ–­å™ªå£°æ¥æº...")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_img = create_simple_test_image(device)
    
    # åå½’ä¸€åŒ–
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    fig, axes = plt.subplots(len(models), 5, figsize=(20, len(models)*4))
    
    if len(models) == 1:
        axes = axes.reshape(1, -1)
    
    for i, (model_name, model) in enumerate(models.items()):
        print(f"\n  ğŸ”¬ åˆ†æ {model_name}...")
        
        with torch.no_grad():
            try:
                # 1. åŸå§‹å›¾åƒ
                original_display = torch.clamp(inv_normalize(test_img[0]).cpu(), 0, 1)
                axes[i, 0].imshow(original_display.permute(1, 2, 0))
                axes[i, 0].set_title(f'{model_name}\nåŸå§‹å›¾åƒ')
                axes[i, 0].axis('off')
                
                # 2. MAEå‰å‘ä¼ æ’­
                loss, pred, mask = model(test_img, mask_ratio=0.75)
                
                # 3. æ˜¾ç¤ºé¢„æµ‹çš„åŸå§‹å€¼ï¼ˆæœªunpatchifyï¼‰
                pred_vis = pred[0, :16].cpu().numpy()  # å–å‰16ä¸ªpatchçš„é¢„æµ‹
                pred_2d = pred_vis.reshape(4, 4)
                im1 = axes[i, 1].imshow(pred_2d, cmap='RdBu_r')
                axes[i, 1].set_title(f'é¢„æµ‹åŸå§‹å€¼\nèŒƒå›´:[{pred.min():.2f}, {pred.max():.2f}]')
                axes[i, 1].axis('off')
                plt.colorbar(im1, ax=axes[i, 1], fraction=0.046, pad=0.04)
                
                # 4. é‡å»ºå›¾åƒ
                reconstructed = model.unpatchify(pred)
                reconstructed_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
                axes[i, 2].imshow(reconstructed_display.permute(1, 2, 0))
                axes[i, 2].set_title(f'é‡å»ºå›¾åƒ\næŸå¤±:{loss.item():.3f}')
                axes[i, 2].axis('off')
                
                # 5. é‡å»ºè¯¯å·®
                error = torch.abs(original_display - reconstructed_display)
                error_display = error.mean(dim=0)
                im2 = axes[i, 3].imshow(error_display, cmap='hot')
                axes[i, 3].set_title(f'é‡å»ºè¯¯å·®\nå‡å€¼:{error.mean():.3f}')
                axes[i, 3].axis('off')
                plt.colorbar(im2, ax=axes[i, 3], fraction=0.046, pad=0.04)
                
                # 6. è¯Šæ–­ä¿¡æ¯
                axes[i, 4].text(0.1, 0.9, f'æ¨¡å‹: {model_name}', transform=axes[i, 4].transAxes, fontsize=10, weight='bold')
                axes[i, 4].text(0.1, 0.8, f'æŸå¤±: {loss.item():.4f}', transform=axes[i, 4].transAxes, fontsize=9)
                axes[i, 4].text(0.1, 0.7, f'é¢„æµ‹èŒƒå›´: [{pred.min():.2f}, {pred.max():.2f}]', transform=axes[i, 4].transAxes, fontsize=9)
                axes[i, 4].text(0.1, 0.6, f'é¢„æµ‹å‡å€¼: {pred.mean():.4f}', transform=axes[i, 4].transAxes, fontsize=9)
                axes[i, 4].text(0.1, 0.5, f'é¢„æµ‹æ ‡å‡†å·®: {pred.std():.4f}', transform=axes[i, 4].transAxes, fontsize=9)
                
                # è¯Šæ–­ç»“æœ
                if abs(pred.mean().item()) > 1.0:
                    axes[i, 4].text(0.1, 0.3, 'âš ï¸ é¢„æµ‹å‡å€¼å¼‚å¸¸', transform=axes[i, 4].transAxes, fontsize=9, color='red')
                
                if pred.std().item() > 2.0:
                    axes[i, 4].text(0.1, 0.2, 'âš ï¸ é¢„æµ‹æ–¹å·®è¿‡å¤§', transform=axes[i, 4].transAxes, fontsize=9, color='red')
                
                if error.mean().item() > 0.5:
                    axes[i, 4].text(0.1, 0.1, 'âš ï¸ é‡å»ºè¯¯å·®è¿‡å¤§', transform=axes[i, 4].transAxes, fontsize=9, color='red')
                
                axes[i, 4].set_xlim(0, 1)
                axes[i, 4].set_ylim(0, 1)
                axes[i, 4].axis('off')
                axes[i, 4].set_title('è¯Šæ–­ä¿¡æ¯')
                
            except Exception as e:
                axes[i, 0].text(0.5, 0.5, f'é”™è¯¯: {str(e)}', ha='center', va='center', transform=axes[i, 0].transAxes)
                for j in range(5):
                    axes[i, j].axis('off')
    
    plt.tight_layout()
    
    # ä¿å­˜è¯Šæ–­ç»“æœ
    diagnosis_path = 'mae_noise_diagnosis.png'
    plt.savefig(diagnosis_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… è¯Šæ–­ç»“æœä¿å­˜: {diagnosis_path}")
    
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return diagnosis_path

def provide_solutions():
    """æä¾›è§£å†³æ–¹æ¡ˆ"""
    print("\nğŸ’¡ å™ªå£°é—®é¢˜è§£å†³æ–¹æ¡ˆ:")
    print("=" * 50)
    
    print("ğŸ” å¯èƒ½çš„åŸå› :")
    print("1. è§£ç å™¨æœªé¢„è®­ç»ƒ - éšæœºåˆå§‹åŒ–å¯¼è‡´è¾“å‡ºä¸ç¨³å®š")
    print("2. é¢„æµ‹å€¼èŒƒå›´å¼‚å¸¸ - å¯èƒ½éœ€è¦è°ƒæ•´æŸå¤±å‡½æ•°æˆ–å½’ä¸€åŒ–")
    print("3. æ¨¡å‹æ¶æ„ä¸åŒ¹é… - ç¼–ç å™¨å’Œè§£ç å™¨ç‰ˆæœ¬ä¸å…¼å®¹")
    print("4. æ•°æ®é¢„å¤„ç†é—®é¢˜ - å½’ä¸€åŒ–å‚æ•°ä¸æ­£ç¡®")
    print()
    
    print("ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ:")
    print("1. ä½¿ç”¨å®Œæ•´é¢„è®­ç»ƒæ¨¡å‹:")
    print("   - ä¸‹è½½åŒ…å«è§£ç å™¨æƒé‡çš„å®Œæ•´MAEæ¨¡å‹")
    print("   - æˆ–è€…åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒè§£ç å™¨")
    print()
    
    print("2. è°ƒæ•´è§£ç å™¨åˆå§‹åŒ–:")
    print("   - ä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–æƒé‡")
    print("   - æ·»åŠ æƒé‡æ­£åˆ™åŒ–")
    print()
    
    print("3. ä¿®æ”¹æŸå¤±å‡½æ•°:")
    print("   - ä½¿ç”¨æ›´ç¨³å®šçš„é‡å»ºæŸå¤±")
    print("   - æ·»åŠ å¹³æ»‘æ­£åˆ™é¡¹")
    print()
    
    print("4. æ•°æ®é¢„å¤„ç†ä¼˜åŒ–:")
    print("   - æ£€æŸ¥å½’ä¸€åŒ–å‚æ•°")
    print("   - ä½¿ç”¨æ›´é€‚åˆçš„æ•°æ®èŒƒå›´")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” MAEé‡å»ºå™ªå£°è¯Šæ–­")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    # åŠ è½½ä¸åŒé…ç½®çš„æ¨¡å‹
    models, device = load_mae_model_variants()
    
    # åˆ†ææ¨¡å‹ç»„ä»¶
    analyze_model_components(models, device)
    
    # æµ‹è¯•é‡å»ºè´¨é‡
    test_results = test_reconstruction_quality(models, device)
    
    # è¯Šæ–­å™ªå£°æ¥æº
    diagnosis_path = diagnose_noise_sources(models, device)
    
    # æä¾›è§£å†³æ–¹æ¡ˆ
    provide_solutions()
    
    print(f"\nğŸ¯ è¯Šæ–­ç»“è®º:")
    print("æ ¹æ®ä½ çœ‹åˆ°çš„å™ªå£°å›¾åƒï¼Œæœ€å¯èƒ½çš„åŸå› æ˜¯:")
    print("âŒ è§£ç å™¨ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼Œæ²¡æœ‰ç»è¿‡é¢„è®­ç»ƒ")
    print("ğŸ¨ ç¼–ç å™¨è™½ç„¶é¢„è®­ç»ƒäº†ï¼Œä½†è§£ç å™¨ä¸çŸ¥é“å¦‚ä½•æ­£ç¡®é‡å»ºåƒç´ ")
    print("ğŸ’¡ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé‡å»ºç»“æœæ˜¯å™ªå£°è€Œä¸æ˜¯æ¸…æ™°å›¾åƒ")
    
    print(f"\nğŸ“ è¯Šæ–­ç»“æœ: {diagnosis_path}")

if __name__ == "__main__":
    main()

