#!/usr/bin/env python3
"""
è¯Šæ–­MAEé‡å»ºå®Œå…¨å¤±è´¥çš„é—®é¢˜
åˆ†æä¸ºä»€ä¹ˆè¿æœªæ©ç›–çš„åƒç´ éƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤º
"""

import os
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from datasets import load_dataset

# è§£å†³ OpenMP å†²çª
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import models_mae

def diagnose_reconstruction_pipeline():
    """è¯Šæ–­é‡å»ºæµç¨‹çš„æ¯ä¸ªæ­¥éª¤"""
    print("ğŸ” è¯Šæ–­MAEé‡å»ºæµç¨‹...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # åŠ è½½æ¨¡å‹
    model = models_mae.mae_vit_base_patch16(norm_pix_loss=True)
    
    # å°è¯•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    checkpoint_path = './output_image_repair_v1/checkpoint-19.pth'
    if os.path.exists(checkpoint_path):
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            model.load_state_dict(checkpoint['model'])
            print("âœ… åŠ è½½è®­ç»ƒæ¨¡å‹")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨éšæœºæ¨¡å‹: {e}")
    else:
        print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
    
    model.to(device)
    model.eval()
    
    # åŠ è½½æµ‹è¯•å›¾ç‰‡
    try:
        ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
        sample = ds['train'][0]
        original_img = sample['image']
        
        if original_img.mode != 'RGB':
            original_img = original_img.convert('RGB')
        
        print(f"âœ… åŸå§‹å›¾ç‰‡: {original_img.size}, æ¨¡å¼: {original_img.mode}")
    except Exception as e:
        print(f"âŒ å›¾ç‰‡åŠ è½½å¤±è´¥: {e}")
        return
    
    # æ­¥éª¤1: æ£€æŸ¥å›¾åƒé¢„å¤„ç†
    print(f"\nğŸ” æ­¥éª¤1: æ£€æŸ¥å›¾åƒé¢„å¤„ç†...")
    
    transform = transforms.Compose([
        transforms.Resize(int(224 * 1.15), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åªåšresizeå’Œcropï¼Œä¸åšå½’ä¸€åŒ–
    transform_no_norm = transforms.Compose([
        transforms.Resize(int(224 * 1.15), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor()
    ])
    
    img_tensor = transform(original_img).unsqueeze(0).to(device)
    img_tensor_no_norm = transform_no_norm(original_img).unsqueeze(0).to(device)
    
    print(f"  é¢„å¤„ç†åå½¢çŠ¶: {img_tensor.shape}")
    print(f"  å½’ä¸€åŒ–åèŒƒå›´: [{img_tensor.min():.3f}, {img_tensor.max():.3f}]")
    print(f"  æœªå½’ä¸€åŒ–èŒƒå›´: [{img_tensor_no_norm.min():.3f}, {img_tensor_no_norm.max():.3f}]")
    
    # æ­¥éª¤2: æ£€æŸ¥æ¨¡å‹å‰å‘ä¼ æ’­
    print(f"\nğŸ” æ­¥éª¤2: æ£€æŸ¥æ¨¡å‹å‰å‘ä¼ æ’­...")
    
    with torch.no_grad():
        # è·å–ä¸­é—´ç»“æœ
        x = model.patch_embed(img_tensor)
        print(f"  Patch embeddingå½¢çŠ¶: {x.shape}")
        print(f"  Patch embeddingèŒƒå›´: [{x.min():.3f}, {x.max():.3f}]")
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + model.pos_embed[:, 1:, :]
        print(f"  æ·»åŠ ä½ç½®ç¼–ç åèŒƒå›´: [{x.min():.3f}, {x.max():.3f}]")
        
        # æ‰§è¡Œå®Œæ•´çš„å‰å‘ä¼ æ’­
        loss, pred, mask = model(img_tensor, mask_ratio=0.25)
        print(f"  é¢„æµ‹å½¢çŠ¶: {pred.shape}")
        print(f"  é¢„æµ‹èŒƒå›´: [{pred.min():.3f}, {pred.max():.3f}]")
        print(f"  æ©ç å½¢çŠ¶: {mask.shape}")
        print(f"  æ©ç å€¼: {mask.unique()}")
    
    # æ­¥éª¤3: æ£€æŸ¥unpatchifyè¿‡ç¨‹
    print(f"\nğŸ” æ­¥éª¤3: æ£€æŸ¥unpatchifyè¿‡ç¨‹...")
    
    reconstructed = model.unpatchify(pred)
    print(f"  é‡å»ºå›¾åƒå½¢çŠ¶: {reconstructed.shape}")
    print(f"  é‡å»ºå›¾åƒèŒƒå›´: [{reconstructed.min():.3f}, {reconstructed.max():.3f}]")
    
    # æ­¥éª¤4: æ£€æŸ¥åå½’ä¸€åŒ–
    print(f"\nğŸ” æ­¥éª¤4: æ£€æŸ¥åå½’ä¸€åŒ–...")
    
    # æ ‡å‡†åå½’ä¸€åŒ–
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    original_display = torch.clamp(inv_normalize(img_tensor[0]).cpu(), 0, 1)
    recon_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
    
    print(f"  åŸå›¾åå½’ä¸€åŒ–èŒƒå›´: [{original_display.min():.3f}, {original_display.max():.3f}]")
    print(f"  é‡å»ºåå½’ä¸€åŒ–èŒƒå›´: [{recon_display.min():.3f}, {recon_display.max():.3f}]")
    
    # æ­¥éª¤5: å¯è§†åŒ–è¯Šæ–­
    print(f"\nğŸ” æ­¥éª¤5: åˆ›å»ºè¯Šæ–­å¯è§†åŒ–...")
    
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    
    # ç¬¬ä¸€è¡Œï¼šå¤„ç†æµç¨‹
    axes[0, 0].imshow(original_img.resize((224, 224)))
    axes[0, 0].set_title('Original (resized)')
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(img_tensor_no_norm[0].permute(1, 2, 0).cpu())
    axes[0, 1].set_title('After Transform\n(no normalization)')
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(original_display.permute(1, 2, 0))
    axes[0, 2].set_title('After Normalization\n(inv-normalized)')
    axes[0, 2].axis('off')
    
    # æ˜¾ç¤ºé¢„æµ‹çš„åŸå§‹å€¼ï¼ˆä¸åšclampï¼‰
    recon_raw = inv_normalize(reconstructed[0]).cpu()
    axes[0, 3].imshow(recon_raw.permute(1, 2, 0))
    axes[0, 3].set_title('Raw Reconstruction\n(no clamp)')
    axes[0, 3].axis('off')
    
    # ç¬¬äºŒè¡Œï¼šé—®é¢˜åˆ†æ
    axes[1, 0].imshow(recon_display.permute(1, 2, 0))
    axes[1, 0].set_title(f'Clamped Reconstruction\nLoss: {loss.item():.3f}')
    axes[1, 0].axis('off')
    
    # æ˜¾ç¤ºé¢„æµ‹å€¼çš„åˆ†å¸ƒ
    pred_flat = pred.flatten().cpu().numpy()
    axes[1, 1].hist(pred_flat, bins=50, alpha=0.7)
    axes[1, 1].set_title('Prediction Distribution')
    axes[1, 1].set_xlabel('Prediction Value')
    axes[1, 1].set_ylabel('Frequency')
    
    # æ˜¾ç¤ºé‡å»ºå€¼çš„åˆ†å¸ƒ
    recon_flat = reconstructed.flatten().cpu().numpy()
    axes[1, 2].hist(recon_flat, bins=50, alpha=0.7, color='orange')
    axes[1, 2].set_title('Reconstruction Distribution')
    axes[1, 2].set_xlabel('Pixel Value')
    axes[1, 2].set_ylabel('Frequency')
    
    # æ˜¾ç¤ºæ©ç 
    mask_vis = mask.detach().unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
    mask_vis = model.unpatchify(mask_vis)
    axes[1, 3].imshow(mask_vis[0].cpu().permute(1, 2, 0), cmap='gray')
    axes[1, 3].set_title(f'Mask Visualization\n{mask.float().mean().item():.1%} masked')
    axes[1, 3].axis('off')
    
    plt.tight_layout()
    plt.savefig('reconstruction_diagnosis.png', dpi=150, bbox_inches='tight')
    print("âœ… è¯Šæ–­ç»“æœä¿å­˜: reconstruction_diagnosis.png")
    plt.close()
    
    return {
        'loss': loss.item(),
        'pred_range': (pred.min().item(), pred.max().item()),
        'recon_range': (reconstructed.min().item(), reconstructed.max().item()),
        'mask_ratio': mask.float().mean().item()
    }

def test_simple_reconstruction():
    """æµ‹è¯•ç®€å•çš„é‡å»ºæµç¨‹"""
    print(f"\nğŸ§ª æµ‹è¯•ç®€åŒ–çš„é‡å»ºæµç¨‹...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
    test_img = torch.ones(1, 3, 224, 224, device=device) * 0.5  # ç°è‰²å›¾åƒ
    
    # æ·»åŠ ä¸€äº›ç®€å•çš„å›¾æ¡ˆ
    test_img[0, 0, 50:100, 50:100] = 1.0  # çº¢è‰²æ–¹å—
    test_img[0, 1, 150:200, 150:200] = 1.0  # ç»¿è‰²æ–¹å—
    test_img[0, 2, 100:150, 100:150] = 1.0  # è“è‰²æ–¹å—
    
    print(f"  æµ‹è¯•å›¾åƒèŒƒå›´: [{test_img.min():.3f}, {test_img.max():.3f}]")
    
    # æµ‹è¯•ä¸åŒçš„æ¨¡å‹
    models_to_test = [
        ('éšæœºåˆå§‹åŒ–', None),
        ('è®­ç»ƒæ¨¡å‹', './output_image_repair_v1/checkpoint-19.pth')
    ]
    
    fig, axes = plt.subplots(len(models_to_test), 4, figsize=(16, len(models_to_test)*4))
    
    for i, (model_name, checkpoint_path) in enumerate(models_to_test):
        print(f"\n  æµ‹è¯• {model_name}...")
        
        model = models_mae.mae_vit_base_patch16(norm_pix_loss=True)
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                model.load_state_dict(checkpoint['model'])
                print(f"    âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"    âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        model.to(device)
        model.eval()
        
        with torch.no_grad():
            loss, pred, mask = model(test_img, mask_ratio=0.25)
            reconstructed = model.unpatchify(pred)
            
            # åˆ›å»ºæ©ç å¯è§†åŒ–
            mask_vis = mask.detach().unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
            mask_vis = model.unpatchify(mask_vis)
        
        # æ˜¾ç¤ºç»“æœ
        masked_img = test_img[0].cpu() * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
        
        axes[i, 0].imshow(test_img[0].cpu().permute(1, 2, 0))
        axes[i, 0].set_title(f'{model_name}\nOriginal Test Image')
        axes[i, 0].axis('off')
        
        axes[i, 1].imshow(masked_img.permute(1, 2, 0))
        axes[i, 1].set_title('25% Masked')
        axes[i, 1].axis('off')
        
        axes[i, 2].imshow(torch.clamp(reconstructed[0].cpu(), 0, 1).permute(1, 2, 0))
        axes[i, 2].set_title(f'Reconstructed\nLoss: {loss.item():.3f}')
        axes[i, 2].axis('off')
        
        # æ˜¾ç¤ºé¢„æµ‹å€¼çš„ç»Ÿè®¡
        axes[i, 3].text(0.1, 0.8, f'Prediction Stats:', transform=axes[i, 3].transAxes, fontweight='bold')
        axes[i, 3].text(0.1, 0.7, f'Min: {pred.min():.3f}', transform=axes[i, 3].transAxes)
        axes[i, 3].text(0.1, 0.6, f'Max: {pred.max():.3f}', transform=axes[i, 3].transAxes)
        axes[i, 3].text(0.1, 0.5, f'Mean: {pred.mean():.3f}', transform=axes[i, 3].transAxes)
        axes[i, 3].text(0.1, 0.4, f'Std: {pred.std():.3f}', transform=axes[i, 3].transAxes)
        axes[i, 3].text(0.1, 0.2, f'Recon Range:', transform=axes[i, 3].transAxes, fontweight='bold')
        axes[i, 3].text(0.1, 0.1, f'[{reconstructed.min():.3f}, {reconstructed.max():.3f}]', transform=axes[i, 3].transAxes)
        axes[i, 3].axis('off')
        
        print(f"    æŸå¤±: {loss.item():.4f}")
        print(f"    é¢„æµ‹èŒƒå›´: [{pred.min():.3f}, {pred.max():.3f}]")
        print(f"    é‡å»ºèŒƒå›´: [{reconstructed.min():.3f}, {reconstructed.max():.3f}]")
    
    plt.tight_layout()
    plt.savefig('simple_reconstruction_test.png', dpi=150, bbox_inches='tight')
    print("âœ… ç®€å•é‡å»ºæµ‹è¯•ä¿å­˜: simple_reconstruction_test.png")
    plt.close()

def check_model_components():
    """æ£€æŸ¥æ¨¡å‹å„ä¸ªç»„ä»¶çš„çŠ¶æ€"""
    print(f"\nğŸ” æ£€æŸ¥æ¨¡å‹ç»„ä»¶çŠ¶æ€...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    model = models_mae.mae_vit_base_patch16(norm_pix_loss=True)
    
    # åŠ è½½è®­ç»ƒæ¨¡å‹
    checkpoint_path = './output_image_repair_v1/checkpoint-19.pth'
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        model.load_state_dict(checkpoint['model'])
    
    model.to(device)
    model.eval()
    
    # æ£€æŸ¥å…³é”®ç»„ä»¶çš„æƒé‡
    print(f"ğŸ“Š æ¨¡å‹ç»„ä»¶æ£€æŸ¥:")
    
    # æ£€æŸ¥patch embedding
    patch_embed_weight = model.patch_embed.proj.weight
    print(f"  Patch embeddingæƒé‡èŒƒå›´: [{patch_embed_weight.min():.3f}, {patch_embed_weight.max():.3f}]")
    print(f"  Patch embeddingæƒé‡æ ‡å‡†å·®: {patch_embed_weight.std():.3f}")
    
    # æ£€æŸ¥decoder predictionå±‚
    decoder_pred_weight = model.decoder_pred.weight
    decoder_pred_bias = model.decoder_pred.bias
    print(f"  Decoder predictionæƒé‡èŒƒå›´: [{decoder_pred_weight.min():.3f}, {decoder_pred_weight.max():.3f}]")
    print(f"  Decoder predictionåç½®èŒƒå›´: [{decoder_pred_bias.min():.3f}, {decoder_pred_bias.max():.3f}]")
    
    # æ£€æŸ¥ä½ç½®ç¼–ç 
    pos_embed = model.pos_embed
    print(f"  ä½ç½®ç¼–ç èŒƒå›´: [{pos_embed.min():.3f}, {pos_embed.max():.3f}]")
    
    # æµ‹è¯•ä¸€ä¸ªç®€å•çš„å‰å‘ä¼ æ’­
    test_input = torch.randn(1, 3, 224, 224, device=device)
    
    with torch.no_grad():
        loss, pred, mask = model(test_input, mask_ratio=0.25)
        
        print(f"\nğŸ§ª ç®€å•æµ‹è¯•ç»“æœ:")
        print(f"  è¾“å…¥èŒƒå›´: [{test_input.min():.3f}, {test_input.max():.3f}]")
        print(f"  æŸå¤±: {loss.item():.4f}")
        print(f"  é¢„æµ‹èŒƒå›´: [{pred.min():.3f}, {pred.max():.3f}]")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        if torch.isnan(pred).any():
            print("  âŒ é¢„æµ‹ä¸­åŒ…å«NaNå€¼!")
        if torch.isinf(pred).any():
            print("  âŒ é¢„æµ‹ä¸­åŒ…å«æ— ç©·å€¼!")
        
        # æ£€æŸ¥æ¢¯åº¦
        if hasattr(model, 'decoder_pred'):
            if model.decoder_pred.weight.grad is not None:
                grad_norm = model.decoder_pred.weight.grad.norm()
                print(f"  æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")

def identify_core_issues():
    """è¯†åˆ«æ ¸å¿ƒé—®é¢˜"""
    print(f"\nğŸ¯ æ ¸å¿ƒé—®é¢˜åˆ†æ:")
    
    issues = [
        {
            'issue': 'norm_pix_loss è®¾ç½®é—®é¢˜',
            'description': 'norm_pix_loss=True ä¼šæ”¹å˜ç›®æ ‡çš„è®¡ç®—æ–¹å¼',
            'solution': 'å°è¯• norm_pix_loss=False',
            'severity': 'HIGH'
        },
        {
            'issue': 'è®­ç»ƒç›®æ ‡ä¸åŒ¹é…',
            'description': 'æ¨¡å‹å¯èƒ½å­¦ä¹ çš„æ˜¯å½’ä¸€åŒ–åƒç´ è€Œä¸æ˜¯åŸå§‹åƒç´ ',
            'solution': 'æ£€æŸ¥æŸå¤±å‡½æ•°çš„è®¡ç®—',
            'severity': 'HIGH'
        },
        {
            'issue': 'åå½’ä¸€åŒ–é”™è¯¯',
            'description': 'é‡å»ºç»“æœçš„åå½’ä¸€åŒ–å¯èƒ½ä¸æ­£ç¡®',
            'solution': 'éªŒè¯å½’ä¸€åŒ–/åå½’ä¸€åŒ–çš„ä¸€è‡´æ€§',
            'severity': 'MEDIUM'
        },
        {
            'issue': 'æ¨¡å‹æœªå……åˆ†æ”¶æ•›',
            'description': 'å³ä½¿è®­ç»ƒ20ä¸ªepochï¼Œæ¨¡å‹å¯èƒ½ä»æœªå­¦ä¼šæ­£ç¡®é‡å»º',
            'solution': 'å¤§å¹…å¢åŠ è®­ç»ƒæ—¶é—´æˆ–é™ä½å­¦ä¹ ç‡',
            'severity': 'MEDIUM'
        }
    ]
    
    severity_colors = {'HIGH': 'ğŸ”´', 'MEDIUM': 'ğŸŸ¡', 'LOW': 'ğŸŸ¢'}
    
    for i, issue in enumerate(issues, 1):
        print(f"\n{i}. {severity_colors[issue['severity']]} {issue['issue']}")
        print(f"   é—®é¢˜: {issue['description']}")
        print(f"   è§£å†³: {issue['solution']}")

def create_fix_attempt():
    """åˆ›å»ºä¿®å¤å°è¯•"""
    print(f"\nğŸ› ï¸ åˆ›å»ºä¿®å¤å°è¯•é…ç½®...")
    
    # æœ€å¯èƒ½çš„ä¿®å¤æ–¹æ¡ˆ
    fix_config = """# ä¿®å¤é‡å»ºé—®é¢˜çš„å°è¯•
python main_pretrain_animediffusion.py \\
    --mask_ratio 0.25 \\
    --epochs 50 \\
    --batch_size 4 \\
    --accum_iter 16 \\
    --blr 1e-5 \\
    --warmup_epochs 15 \\
    --max_samples 1000 \\
    --weight_decay 0.01 \\
    --output_dir ./output_fix_attempt \\
    --log_dir ./output_fix_attempt \\
    --norm_pix_loss  # å°è¯•å…³é—­è¿™ä¸ªé€‰é¡¹
"""
    
    print(fix_config)
    
    # ä¿å­˜ä¿®å¤é…ç½®
    with open('fix_reconstruction.sh', 'w') as f:
        f.write("#!/bin/bash\n")
        f.write("export KMP_DUPLICATE_LIB_OK=TRUE\n\n")
        f.write("# å°è¯•ä¿®å¤é‡å»ºé—®é¢˜\n")
        f.write("echo 'ğŸ› ï¸  å°è¯•ä¿®å¤MAEé‡å»ºé—®é¢˜...'\n\n")
        f.write(fix_config.replace('\\', '\\'))
    
    os.chmod('fix_reconstruction.sh', 0o755)
    print(f"âœ… ä¿®å¤é…ç½®ä¿å­˜: fix_reconstruction.sh")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” MAEé‡å»ºå¤±è´¥è¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    # 1. è¯Šæ–­é‡å»ºæµç¨‹
    diagnosis_result = diagnose_reconstruction_pipeline()
    
    # 2. æµ‹è¯•ç®€å•é‡å»º
    test_simple_reconstruction()
    
    # 3. æ£€æŸ¥æ¨¡å‹ç»„ä»¶
    check_model_components()
    
    # 4. è¯†åˆ«æ ¸å¿ƒé—®é¢˜
    identify_core_issues()
    
    # 5. åˆ›å»ºä¿®å¤å°è¯•
    create_fix_attempt()
    
    print(f"\nğŸ’¡ æœ€å¯èƒ½çš„é—®é¢˜:")
    print(f"  ğŸ”´ norm_pix_loss=True å¯èƒ½å¯¼è‡´ç›®æ ‡è®¡ç®—é”™è¯¯")
    print(f"  ğŸ”´ æ¨¡å‹è¾“å‡ºå€¼èŒƒå›´å¼‚å¸¸")
    print(f"  ğŸ”´ è®­ç»ƒæ—¶é—´ä»ç„¶ä¸è¶³")
    
    print(f"\nğŸ¯ ç«‹å³å°è¯•çš„è§£å†³æ–¹æ¡ˆ:")
    print(f"  1. å…³é—­ norm_pix_loss")
    print(f"  2. ä½¿ç”¨æ›´ä½çš„å­¦ä¹ ç‡")
    print(f"  3. å¢åŠ è®­ç»ƒæ—¶é—´")

if __name__ == "__main__":
    main()


