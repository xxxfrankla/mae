#!/usr/bin/env python3
"""
ç®€å•çš„MAEé¢„è®­ç»ƒæ¨¡å‹æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•åŠ è½½å’Œä½¿ç”¨ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹
"""

import os
import sys
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# æ£€æŸ¥æ˜¯å¦å®‰è£…äº†å¿…è¦çš„åŒ…
try:
    import torch
    import torchvision.transforms as transforms
    print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£… PyTorch:")
    print("pip install torch torchvision")
    sys.exit(1)

try:
    import models_mae
    print("âœ… MAE æ¨¡å‹æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥ models_mae æ¨¡å—")
    sys.exit(1)

def load_pretrained_mae():
    """åŠ è½½é¢„è®­ç»ƒçš„MAEæ¨¡å‹"""
    print("\nğŸ”„ åŠ è½½é¢„è®­ç»ƒMAEæ¨¡å‹...")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print(f"âœ… ä½¿ç”¨ Apple Silicon MPS: {device}")
    else:
        device = torch.device('cpu')
        print(f"âš ï¸  ä½¿ç”¨ CPU: {device}")
    
    # æ¨¡å‹è·¯å¾„
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    
    if not os.path.exists(pretrain_path):
        print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {pretrain_path}")
        print("è¯·å…ˆè¿è¡Œ: ./download_models.sh")
        return None, None
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = models_mae.mae_vit_base_patch16()
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        print(f"ğŸ“¥ ä» {pretrain_path} åŠ è½½æƒé‡...")
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(device)
        model.eval()
        
        print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, device
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def create_test_image():
    """åˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ"""
    print("\nğŸ¨ åˆ›å»ºæµ‹è¯•å›¾åƒ...")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å½©è‰²æµ‹è¯•å›¾åƒ
    img = np.zeros((224, 224, 3))
    
    # æ·»åŠ ä¸€äº›å‡ ä½•å›¾æ¡ˆ
    # çº¢è‰²çŸ©å½¢
    img[50:100, 50:150, 0] = 1.0
    
    # ç»¿è‰²åœ†å½¢
    y, x = np.ogrid[:224, :224]
    center_y, center_x = 112, 112
    mask = (x - center_x)**2 + (y - center_y)**2 <= 30**2
    img[mask, 1] = 1.0
    
    # è“è‰²å¯¹è§’çº¿
    for i in range(224):
        if i < 224:
            img[i, i, 2] = 1.0
            if i > 0:
                img[i-1, i, 2] = 0.5
            if i < 223:
                img[i+1, i, 2] = 0.5
    
    return img

def demonstrate_mae_reconstruction(model, device):
    """æ¼”ç¤ºMAEé‡å»ºè¿‡ç¨‹"""
    print("\nğŸ” æ¼”ç¤ºMAEé‡å»ºè¿‡ç¨‹...")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_img = create_test_image()
    
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åå½’ä¸€åŒ–ç”¨äºæ˜¾ç¤º
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    # è½¬æ¢ä¸ºtensor
    img_tensor = transform(test_img).unsqueeze(0).to(device)
    
    # æµ‹è¯•ä¸åŒçš„æ©ç æ¯”ä¾‹
    mask_ratios = [0.25, 0.5, 0.75]
    
    fig, axes = plt.subplots(len(mask_ratios), 4, figsize=(16, len(mask_ratios)*4))
    
    for i, mask_ratio in enumerate(mask_ratios):
        print(f"  æµ‹è¯•æ©ç æ¯”ä¾‹: {mask_ratio*100:.0f}%")
        
        with torch.no_grad():
            # MAEå‰å‘ä¼ æ’­
            loss, pred, mask = model(img_tensor, mask_ratio=mask_ratio)
            
            # é‡å»ºå›¾åƒ
            reconstructed = model.unpatchify(pred)
            
            # åˆ›å»ºæ©ç å¯è§†åŒ–
            mask_vis = mask.detach()
            mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
            mask_vis = model.unpatchify(mask_vis)
        
        # è½¬æ¢ä¸ºæ˜¾ç¤ºæ ¼å¼
        original_display = torch.clamp(inv_normalize(img_tensor[0]).cpu(), 0, 1)
        reconstructed_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
        masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
        
        # è®¡ç®—é‡å»ºè¯¯å·®
        error = torch.abs(original_display - reconstructed_display)
        error_display = error.mean(dim=0)
        
        # æ˜¾ç¤ºç»“æœ
        axes[i, 0].imshow(original_display.permute(1, 2, 0))
        axes[i, 0].set_title('åŸå§‹å›¾åƒ')
        axes[i, 0].axis('off')
        
        axes[i, 1].imshow(masked_img.permute(1, 2, 0))
        axes[i, 1].set_title(f'æ©ç å›¾åƒ ({mask_ratio*100:.0f}%)')
        axes[i, 1].axis('off')
        
        axes[i, 2].imshow(reconstructed_display.permute(1, 2, 0))
        axes[i, 2].set_title(f'é‡å»ºå›¾åƒ\næŸå¤±: {loss.item():.3f}')
        axes[i, 2].axis('off')
        
        im = axes[i, 3].imshow(error_display, cmap='hot')
        axes[i, 3].set_title('é‡å»ºè¯¯å·®')
        axes[i, 3].axis('off')
        plt.colorbar(im, ax=axes[i, 3], fraction=0.046, pad=0.04)
        
        print(f"    æŸå¤±å€¼: {loss.item():.4f}")
        print(f"    å®é™…æ©ç æ¯”ä¾‹: {mask.float().mean().item():.2%}")
    
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    output_path = 'mae_pretrained_demo.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… æ¼”ç¤ºç»“æœå·²ä¿å­˜: {output_path}")
    
    # æ˜¾ç¤ºå›¾åƒï¼ˆå¦‚æœåœ¨æ”¯æŒçš„ç¯å¢ƒä¸­ï¼‰
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return output_path

def print_model_info(model):
    """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
    print("\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"  æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1e6:.1f}M)")
    
    # æ¨¡å‹ç»“æ„ä¿¡æ¯
    print(f"  ç¼–ç å™¨å±‚æ•°: {model.depth}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {model.num_heads}")
    print(f"  åµŒå…¥ç»´åº¦: {model.embed_dim}")
    print(f"  è¡¥ä¸å¤§å°: {model.patch_embed.patch_size}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ MAE é¢„è®­ç»ƒæ¨¡å‹æ¼”ç¤º")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    # åŠ è½½æ¨¡å‹
    model, device = load_pretrained_mae()
    if model is None:
        return
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print_model_info(model)
    
    # æ¼”ç¤ºé‡å»ºè¿‡ç¨‹
    output_path = demonstrate_mae_reconstruction(model, device)
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœå›¾åƒ: {output_path}")
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. è¿™æ˜¯ä½¿ç”¨Facebookå®˜æ–¹é¢„è®­ç»ƒçš„ViT-Base MAEæ¨¡å‹")
    print("2. æ¨¡å‹åœ¨ImageNetä¸Šé¢„è®­ç»ƒï¼Œå…·æœ‰å¼ºå¤§çš„å›¾åƒé‡å»ºèƒ½åŠ›")
    print("3. æ©ç æ¯”ä¾‹è¶Šé«˜ï¼Œé‡å»ºä»»åŠ¡è¶Šå›°éš¾")
    print("4. å¯ä»¥ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒ")

if __name__ == "__main__":
    main()

