#!/usr/bin/env python3
"""
åœ¨ AnimeDiffusion æ•°æ®é›†ä¸Šè¿›è¡Œ MAE é¢„è®­ç»ƒ
ä¼˜åŒ–å¤„ç†é«˜åˆ†è¾¨ç‡åŠ¨æ¼«å›¾ç‰‡
"""

import argparse
import datetime
import json
import numpy as np
import os
import time
from pathlib import Path

import torch
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from datasets import load_dataset
from PIL import Image

import timm
assert timm.__version__ == "0.3.2"
import timm.optim.optim_factory as optim_factory

import util.misc as misc
from util.misc_mps import NativeScalerWithGradNormCount as NativeScaler

import models_mae
from engine_pretrain_mps import train_one_epoch

class AnimeDiffusionDataset(Dataset):
    """AnimeDiffusion æ•°æ®é›†åŒ…è£…å™¨"""
    
    def __init__(self, hf_dataset, transform=None, max_samples=None):
        self.dataset = hf_dataset
        self.transform = transform
        
        if max_samples is not None:
            self.length = min(max_samples, len(hf_dataset))
        else:
            self.length = len(hf_dataset)
        
        print(f"ğŸ“Š AnimeDiffusion æ•°æ®é›†: {self.length} å¼ å›¾ç‰‡ (åŸå§‹: 1920Ã—1080)")

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        try:
            sample = self.dataset[idx]
            image = sample['image']
            
            if not isinstance(image, Image.Image):
                if isinstance(image, np.ndarray):
                    image = Image.fromarray(image)
                else:
                    image = Image.new('RGB', (1920, 1080), color='black')
            
            # å¤„ç†RGBA
            if image.mode == 'RGBA':
                background = Image.new('RGB', image.size, (255, 255, 255))
                background.paste(image, mask=image.split()[-1])
                image = background
            elif image.mode != 'RGB':
                image = image.convert('RGB')
            
            if self.transform:
                image = self.transform(image)
            
            return image, 0
            
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}")
            default_img = Image.new('RGB', (224, 224), color='black')
            if self.transform:
                default_img = self.transform(default_img)
            return default_img, 0

def get_args_parser():
    parser = argparse.ArgumentParser('MAE pre-training on AnimeDiffusion Dataset', add_help=False)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', default=8, type=int,
                        help='Batch size per GPU')
    parser.add_argument('--accum_iter', default=8, type=int,
                        help='Accumulate gradient iterations')
    parser.add_argument('--epochs', default=20, type=int)
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', default='mae_vit_base_patch16', type=str,
                        help='Name of model to train')
    parser.add_argument('--input_size', default=224, type=int,
                        help='images input size (fixed at 224 for MAE)')
    parser.add_argument('--mask_ratio', default=0.75, type=float,
                        help='Masking ratio')
    parser.add_argument('--norm_pix_loss', action='store_true',
                        help='Use normalized pixels as targets')
    parser.set_defaults(norm_pix_loss=True)
    
    # ä¼˜åŒ–å™¨å‚æ•°
    parser.add_argument('--weight_decay', type=float, default=0.05)
    parser.add_argument('--lr', type=float, default=None)
    parser.add_argument('--blr', type=float, default=1.5e-4,
                        help='base learning rate')
    parser.add_argument('--min_lr', type=float, default=0.)
    parser.add_argument('--warmup_epochs', type=int, default=5)
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--max_samples', type=int, default=None,
                        help='Maximum samples to use (None for all 8202)')
    parser.add_argument('--resize_strategy', default='smart_crop', 
                        choices=['smart_crop', 'center_crop', 'resize_only'],
                        help='How to handle 1920x1080 -> 224x224')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', default='./output_animediffusion')
    parser.add_argument('--log_dir', default='./output_animediffusion')
    parser.add_argument('--device', default='mps')
    parser.add_argument('--seed', default=0, type=int)
    parser.add_argument('--resume', default='')
    parser.add_argument('--start_epoch', default=0, type=int)
    parser.add_argument('--num_workers', default=4, type=int)
    parser.add_argument('--save_freq', default=5, type=int)
    
    return parser

def create_animediffusion_dataloader(args):
    """åˆ›å»º AnimeDiffusion æ•°æ®åŠ è½½å™¨"""
    
    print(f"ğŸŒ åˆ›å»º AnimeDiffusion æ•°æ®åŠ è½½å™¨...")
    print(f"  åŸå§‹åˆ†è¾¨ç‡: 1920Ã—1080")
    print(f"  ç›®æ ‡åˆ†è¾¨ç‡: {args.input_size}Ã—{args.input_size}")
    print(f"  ç¼©æ”¾ç­–ç•¥: {args.resize_strategy}")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    
    # åŠ è½½æ•°æ®é›†
    try:
        ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
        train_dataset = ds['train']
        print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(train_dataset)} å¼ å›¾ç‰‡")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    # æ ¹æ®ç­–ç•¥å®šä¹‰å˜æ¢
    if args.resize_strategy == 'smart_crop':
        # æ™ºèƒ½è£å‰ªï¼šå…ˆç¼©æ”¾åˆ°ç¨å¤§å°ºå¯¸ï¼Œå†éšæœºè£å‰ª
        transform = transforms.Compose([
            transforms.Resize(int(args.input_size * 1.15), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.RandomCrop(args.input_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    elif args.resize_strategy == 'center_crop':
        # ä¸­å¿ƒè£å‰ª
        transform = transforms.Compose([
            transforms.Resize(int(args.input_size * 1.1), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(args.input_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    else:  # resize_only
        # ç›´æ¥ç¼©æ”¾
        transform = transforms.Compose([
            transforms.Resize((args.input_size, args.input_size), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    # åˆ›å»ºæ•°æ®é›†
    anime_dataset = AnimeDiffusionDataset(train_dataset, transform=transform, max_samples=args.max_samples)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        anime_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=False,
        drop_last=True
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ: {len(dataloader)} ä¸ªæ‰¹æ¬¡")
    
    return dataloader, anime_dataset

def main(args):
    print('ğŸŒ MAE AnimeDiffusion æ•°æ®é›†é¢„è®­ç»ƒ')
    print('=' * 60)
    print("{}".format(args).replace(', ', ',\n'))

    device = torch.device(args.device)
    print(f'ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}')

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    Path(args.log_dir).mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_loader_train, dataset_train = create_animediffusion_dataloader(args)
    
    if data_loader_train is None:
        print("âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥")
        return
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ¤– åˆ›å»º {args.model} æ¨¡å‹...")
    model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)
    model.to(device)
    
    # è®¡ç®—å­¦ä¹ ç‡
    eff_batch_size = args.batch_size * args.accum_iter
    if args.lr is None:
        args.lr = args.blr * eff_batch_size / 256

    print(f"\nğŸ“ˆ è®­ç»ƒé…ç½®:")
    print(f"  æ•°æ®é›†: AnimeDiffusion ({len(dataset_train)} å¼ å›¾ç‰‡)")
    print(f"  åˆ†è¾¨ç‡: 1920Ã—1080 â†’ {args.input_size}Ã—{args.input_size}")
    print(f"  ç¼©æ”¾ç­–ç•¥: {args.resize_strategy}")
    print(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {eff_batch_size}")
    print(f"  å­¦ä¹ ç‡: {args.lr:.2e}")
    print(f"  æ©ç æ¯”ä¾‹: {args.mask_ratio}")

    # åˆ›å»ºä¼˜åŒ–å™¨
    param_groups = optim_factory.add_weight_decay(model, args.weight_decay)
    optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))
    
    # åˆ›å»ºæŸå¤±ç¼©æ”¾å™¨
    loss_scaler = NativeScaler(device_type=device.type)

    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    log_writer = SummaryWriter(log_dir=args.log_dir) if args.log_dir else None

    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {args.epochs} ä¸ªepoch...")
    start_time = time.time()
    best_loss = float('inf')
    
    for epoch in range(args.start_epoch, args.epochs):
        print(f"\nğŸ“… Epoch {epoch+1}/{args.epochs}")
        
        # è®­ç»ƒä¸€ä¸ªepoch
        train_stats = train_one_epoch(
            model, data_loader_train,
            optimizer, device, epoch, loss_scaler,
            log_writer=log_writer,
            args=args
        )
        
        # æ›´æ–°æœ€ä½³æŸå¤±
        current_loss = train_stats['loss']
        if current_loss < best_loss:
            best_loss = current_loss
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æŸå¤±: {best_loss:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if args.output_dir and (epoch % args.save_freq == 0 or epoch + 1 == args.epochs):
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: epoch {epoch}")
            misc.save_model(
                args=args, model=model, model_without_ddp=model, 
                optimizer=optimizer, loss_scaler=loss_scaler, epoch=epoch)

        # è®°å½•æ—¥å¿—
        log_stats = {
            **{f'train_{k}': v for k, v in train_stats.items()},
            'epoch': epoch,
            'best_loss': best_loss
        }

        if args.output_dir:
            if log_writer is not None:
                log_writer.flush()
            with open(os.path.join(args.output_dir, "log.txt"), mode="a", encoding="utf-8") as f:
                f.write(json.dumps(log_stats) + "\n")
        
        # æ˜¾ç¤ºè¿›åº¦
        if epoch > 0:
            elapsed_time = time.time() - start_time
            remaining_epochs = args.epochs - epoch - 1
            eta = elapsed_time / (epoch + 1 - args.start_epoch) * remaining_epochs
            eta_str = str(datetime.timedelta(seconds=int(eta)))
            print(f"â±ï¸  é¢„è®¡å‰©ä½™æ—¶é—´: {eta_str}")

    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    
    print(f'\nğŸ‰ AnimeDiffusion é¢„è®­ç»ƒå®Œæˆ!')
    print(f'â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time_str}')
    print(f'ğŸ† æœ€ä½³æŸå¤±: {best_loss:.4f}')
    print(f'ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {args.output_dir}')
    
    # ç”Ÿæˆè®­ç»ƒæ€»ç»“
    summary = {
        'dataset': 'AnimeDiffusion',
        'dataset_size': len(dataset_train),
        'original_resolution': '1920x1080',
        'target_resolution': f'{args.input_size}x{args.input_size}',
        'resize_strategy': args.resize_strategy,
        'model': args.model,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'effective_batch_size': eff_batch_size,
        'learning_rate': args.lr,
        'mask_ratio': args.mask_ratio,
        'best_loss': best_loss,
        'training_time': total_time_str,
        'device': str(device)
    }
    
    summary_path = os.path.join(args.output_dir, 'animediffusion_training_summary.json')
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"ğŸ“‹ è®­ç»ƒæ€»ç»“ä¿å­˜: {summary_path}")

if __name__ == '__main__':
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    args = get_args_parser()
    args = args.parse_args()
    
    # å¼ºåˆ¶ä½¿ç”¨224Ã—224ï¼ˆMAEæ¨¡å‹é™åˆ¶ï¼‰
    if args.input_size != 224:
        print(f"âš ï¸  MAEæ¨¡å‹åªæ”¯æŒ224Ã—224ï¼Œè‡ªåŠ¨è°ƒæ•´ {args.input_size} â†’ 224")
        args.input_size = 224
    
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    main(args)


