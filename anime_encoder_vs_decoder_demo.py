#!/usr/bin/env python3
"""
ä½¿ç”¨Anime Diffusionæ•°æ®é›†æ¼”ç¤ºMAEç¼–ç å™¨vsè§£ç å™¨
å›ç­”é—®é¢˜ï¼šåªç”¨ç¼–ç å™¨èƒ½å¦é‡å»ºå›¾åƒï¼Ÿ
"""

import os
import sys
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torchvision.transforms as transforms
import models_mae

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_anime_dataset_sample():
    """ä»anime diffusionæ•°æ®é›†åŠ è½½æ ·æœ¬"""
    print("ğŸ¨ ä»Anime Diffusionæ•°æ®é›†åŠ è½½æ ·æœ¬...")
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    dataset_paths = [
        '/Users/tdu/Documents/GitHub/mae/test_dataset',
        './test_dataset'
    ]
    
    dataset_path = None
    for path in dataset_paths:
        if os.path.exists(path):
            dataset_path = path
            break
    
    if dataset_path is None:
        print("âŒ æœªæ‰¾åˆ°animeæ•°æ®é›†ï¼Œåˆ›å»ºç¤ºä¾‹å›¾åƒ...")
        return create_anime_style_examples()
    
    # è·å–å›¾åƒæ–‡ä»¶
    image_files = [f for f in os.listdir(dataset_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    
    if len(image_files) == 0:
        print("âŒ æ•°æ®é›†ä¸­æ²¡æœ‰å›¾åƒæ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹å›¾åƒ...")
        return create_anime_style_examples()
    
    # éšæœºé€‰æ‹©å‡ å¼ å›¾åƒ
    selected_files = np.random.choice(image_files, min(3, len(image_files)), replace=False)
    
    images = []
    for filename in selected_files:
        try:
            img_path = os.path.join(dataset_path, filename)
            img = Image.open(img_path).convert('RGB')
            # è°ƒæ•´å¤§å°åˆ°224x224
            img = img.resize((224, 224), Image.Resampling.LANCZOS)
            images.append((f"Anime_{filename[:10]}", np.array(img) / 255.0))
            print(f"  âœ… åŠ è½½: {filename}")
        except Exception as e:
            print(f"  âŒ åŠ è½½å¤±è´¥ {filename}: {e}")
    
    return images

def create_anime_style_examples():
    """åˆ›å»ºåŠ¨æ¼«é£æ ¼çš„ç¤ºä¾‹å›¾åƒ"""
    print("  åˆ›å»ºåŠ¨æ¼«é£æ ¼ç¤ºä¾‹å›¾åƒ...")
    
    images = []
    
    # ç¤ºä¾‹1ï¼šç®€å•çš„åŠ¨æ¼«è„¸éƒ¨è½®å»“
    img1 = np.ones((224, 224, 3)) * 0.95  # æµ…è‰²èƒŒæ™¯
    
    # è„¸éƒ¨è½®å»“ï¼ˆæ¤­åœ†ï¼‰
    y, x = np.ogrid[:224, :224]
    center_y, center_x = 112, 112
    face_mask = ((x - center_x)/60)**2 + ((y - center_y)/80)**2 <= 1
    img1[face_mask] = [1.0, 0.9, 0.8]  # è‚¤è‰²
    
    # çœ¼ç›
    eye1_mask = ((x - 90)/8)**2 + ((y - 90)/12)**2 <= 1
    eye2_mask = ((x - 134)/8)**2 + ((y - 90)/12)**2 <= 1
    img1[eye1_mask] = [0.1, 0.1, 0.1]  # é»‘è‰²çœ¼ç›
    img1[eye2_mask] = [0.1, 0.1, 0.1]
    
    # å˜´å·´
    mouth_mask = ((x - 112)/15)**2 + ((y - 140)/5)**2 <= 1
    img1[mouth_mask] = [0.8, 0.3, 0.3]  # çº¢è‰²å˜´å·´
    
    images.append(("åŠ¨æ¼«è„¸éƒ¨", img1))
    
    # ç¤ºä¾‹2ï¼šå½©è‰²å‡ ä½•å›¾æ¡ˆ
    img2 = np.zeros((224, 224, 3))
    for i in range(0, 224, 28):
        for j in range(0, 224, 28):
            color = [(i/224), (j/224), 0.8]
            img2[i:i+28, j:j+28] = color
    images.append(("å½©è‰²æ–¹æ ¼", img2))
    
    # ç¤ºä¾‹3ï¼šæ˜Ÿç©ºèƒŒæ™¯
    img3 = np.zeros((224, 224, 3))
    img3[:, :, 2] = 0.2  # æ·±è“èƒŒæ™¯
    
    # éšæœºæ˜Ÿæ˜Ÿ
    np.random.seed(42)
    for _ in range(50):
        x_star = np.random.randint(0, 224)
        y_star = np.random.randint(0, 224)
        size = np.random.randint(1, 4)
        brightness = np.random.uniform(0.5, 1.0)
        img3[max(0, y_star-size):min(224, y_star+size), 
             max(0, x_star-size):min(224, x_star+size)] = [brightness, brightness, brightness]
    
    images.append(("æ˜Ÿç©ºèƒŒæ™¯", img3))
    
    return images

def load_mae_model():
    """åŠ è½½MAEæ¨¡å‹"""
    print("\nğŸ¤– åŠ è½½MAEæ¨¡å‹...")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print(f"âœ… ä½¿ç”¨ Apple Silicon MPS")
    else:
        device = torch.device('cpu')
        print(f"âœ… ä½¿ç”¨ CPU")
    
    # åˆ›å»ºæ¨¡å‹
    model = models_mae.mae_vit_base_patch16()
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆåªæœ‰ç¼–ç å™¨éƒ¨åˆ†ï¼‰
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    if os.path.exists(pretrain_path):
        print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrain_path}")
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        
        # åªåŠ è½½ç¼–ç å™¨æƒé‡
        encoder_state_dict = {}
        for key, value in checkpoint['model'].items():
            if not key.startswith('decoder') and key != 'mask_token':
                encoder_state_dict[key] = value
        
        model.load_state_dict(encoder_state_dict, strict=False)
        print("âœ… ç¼–ç å™¨æƒé‡åŠ è½½æˆåŠŸ")
    else:
        print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
    
    model = model.to(device)
    model.eval()
    
    return model, device

def demonstrate_encoder_vs_decoder(model, device, images):
    """æ¼”ç¤ºç¼–ç å™¨ vs å®Œæ•´MAEçš„åŒºåˆ«"""
    print("\nğŸ” æ¼”ç¤ºï¼šç¼–ç å™¨èƒ½å¦é‡å»ºå›¾åƒï¼Ÿ")
    
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åå½’ä¸€åŒ–
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    num_images = len(images)
    fig, axes = plt.subplots(4, num_images, figsize=(num_images*4, 16))
    
    if num_images == 1:
        axes = axes.reshape(-1, 1)
    
    for i, (name, img) in enumerate(images):
        print(f"\n  å¤„ç†å›¾åƒ: {name}")
        
        # è½¬æ¢ä¸ºtensor
        img_tensor = transform(img.astype(np.float32)).unsqueeze(0).to(device)
        
        with torch.no_grad():
            # 1. æ˜¾ç¤ºåŸå§‹å›¾åƒ
            axes[0, i].imshow(img)
            axes[0, i].set_title(f'{name}\nåŸå§‹å›¾åƒ')
            axes[0, i].axis('off')
            
            # 2. åªç”¨ç¼–ç å™¨æå–ç‰¹å¾
            print("    ğŸ”„ ç¼–ç å™¨ç‰¹å¾æå–...")
            x = model.patch_embed(img_tensor)
            x = x + model.pos_embed[:, 1:, :]
            
            # æ·»åŠ cls token
            cls_token = model.cls_token + model.pos_embed[:, :1, :]
            cls_tokens = cls_token.expand(x.shape[0], -1, -1)
            x = torch.cat((cls_tokens, x), dim=1)
            
            # é€šè¿‡ç¼–ç å™¨
            for blk in model.blocks:
                x = blk(x)
            encoded_features = model.norm(x)
            
            # å°è¯•ç›´æ¥ä»ç¼–ç å™¨ç‰¹å¾"é‡å»º"ï¼ˆè¿™æ˜¯ä¸å¯èƒ½çš„ï¼ï¼‰
            # æˆ‘ä»¬åªèƒ½å¯è§†åŒ–ç‰¹å¾
            patch_features = encoded_features[:, 1:, :]  # å»æ‰cls token
            feature_map = patch_features.mean(dim=2).cpu().numpy().reshape(14, 14)
            
            im1 = axes[1, i].imshow(feature_map, cmap='viridis')
            axes[1, i].set_title('ç¼–ç å™¨ç‰¹å¾\n(æ— æ³•é‡å»ºå›¾åƒ!)')
            axes[1, i].axis('off')
            plt.colorbar(im1, ax=axes[1, i], fraction=0.046, pad=0.04)
            
            # 3. æ¨¡æ‹Ÿå®Œæ•´MAEçš„æ©ç è¿‡ç¨‹
            print("    ğŸ­ æ¨¡æ‹Ÿæ©ç è¿‡ç¨‹...")
            mask_ratio = 0.75
            
            # åˆ›å»ºéšæœºæ©ç 
            N, L, D = encoded_features.shape
            len_keep = int(L * (1 - mask_ratio))
            
            noise = torch.rand(N, L, device=device)
            ids_shuffle = torch.argsort(noise, dim=1)
            ids_restore = torch.argsort(ids_shuffle, dim=1)
            
            # åˆ›å»ºæ©ç å¯è§†åŒ–
            mask = torch.ones([N, L], device=device)
            mask[:, :len_keep] = 0
            mask = torch.gather(mask, dim=1, index=ids_restore)
            
            # å¯è§†åŒ–æ©ç 
            mask_vis = mask[:, 1:].cpu().numpy().reshape(14, 14)  # å»æ‰cls token
            
            axes[2, i].imshow(mask_vis, cmap='RdYlBu_r', vmin=0, vmax=1)
            axes[2, i].set_title(f'æ©ç æ¨¡å¼\n({mask_ratio*100:.0f}% è¢«æ©ç›–)')
            axes[2, i].axis('off')
            
            # 4. è¯´æ˜éœ€è¦è§£ç å™¨
            axes[3, i].text(0.5, 0.7, 'âŒ åªæœ‰ç¼–ç å™¨', ha='center', va='center', 
                           transform=axes[3, i].transAxes, fontsize=14, color='red')
            axes[3, i].text(0.5, 0.5, 'æ— æ³•é‡å»ºå›¾åƒ!', ha='center', va='center',
                           transform=axes[3, i].transAxes, fontsize=12)
            axes[3, i].text(0.5, 0.3, 'âœ… éœ€è¦è§£ç å™¨', ha='center', va='center',
                           transform=axes[3, i].transAxes, fontsize=14, color='green')
            axes[3, i].text(0.5, 0.1, 'æ‰èƒ½é‡å»ºåƒç´ ', ha='center', va='center',
                           transform=axes[3, i].transAxes, fontsize=12)
            axes[3, i].set_xlim(0, 1)
            axes[3, i].set_ylim(0, 1)
            axes[3, i].axis('off')
            axes[3, i].set_title('é‡å»ºéœ€è¦è§£ç å™¨!')
            
            print(f"    ç¼–ç å™¨ç‰¹å¾ç»´åº¦: {encoded_features.shape}")
            print(f"    ç‰¹å¾å‡å€¼: {encoded_features.mean().item():.4f}")
    
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    output_path = 'anime_encoder_vs_decoder_demo.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… æ¼”ç¤ºç»“æœå·²ä¿å­˜: {output_path}")
    
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return output_path

def explain_mae_architecture():
    """è§£é‡ŠMAEæ¶æ„"""
    print("\nğŸ“š MAEæ¶æ„è§£é‡Š:")
    print("=" * 50)
    print("ğŸ” ç¼–ç å™¨ (Encoder):")
    print("  â€¢ è¾“å…¥: åŸå§‹å›¾åƒçš„å¯è§patches")
    print("  â€¢ åŠŸèƒ½: æå–é«˜çº§è¯­ä¹‰ç‰¹å¾")
    print("  â€¢ è¾“å‡º: æŠ½è±¡çš„ç‰¹å¾è¡¨ç¤º")
    print("  â€¢ âŒ æ— æ³•ç›´æ¥é‡å»ºåƒç´ !")
    print()
    print("ğŸ¨ è§£ç å™¨ (Decoder):")
    print("  â€¢ è¾“å…¥: ç¼–ç å™¨ç‰¹å¾ + mask tokens")
    print("  â€¢ åŠŸèƒ½: ä»ç‰¹å¾é‡å»ºåƒç´ ")
    print("  â€¢ è¾“å‡º: é‡å»ºçš„å›¾åƒpatches")
    print("  â€¢ âœ… è´Ÿè´£åƒç´ çº§é‡å»º!")
    print()
    print("ğŸ­ å®Œæ•´MAEæµç¨‹:")
    print("  1. å›¾åƒåˆ†patch â†’ éšæœºæ©ç 75%")
    print("  2. å¯è§patches â†’ ç¼–ç å™¨ â†’ ç‰¹å¾")
    print("  3. ç‰¹å¾ + mask tokens â†’ è§£ç å™¨ â†’ é‡å»º")
    print("  4. è®¡ç®—é‡å»ºæŸå¤±ï¼Œè®­ç»ƒæ¨¡å‹")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ Anime Diffusionæ•°æ®é›† - MAEç¼–ç å™¨vsè§£ç å™¨æ¼”ç¤º")
    print("=" * 60)
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    # åŠ è½½animeæ•°æ®é›†æ ·æœ¬
    anime_images = load_anime_dataset_sample()
    
    # åŠ è½½MAEæ¨¡å‹
    model, device = load_mae_model()
    
    # æ¼”ç¤ºç¼–ç å™¨vsè§£ç å™¨
    output_path = demonstrate_encoder_vs_decoder(model, device, anime_images)
    
    # è§£é‡Šæ¶æ„
    explain_mae_architecture()
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ ç»“æœå›¾åƒ: {output_path}")
    print("\nğŸ’¡ å…³é”®ç»“è®º:")
    print("âŒ åªç”¨ç¼–ç å™¨æ— æ³•é‡å»ºå›¾åƒ")
    print("âœ… ç¼–ç å™¨åªèƒ½æå–æŠ½è±¡ç‰¹å¾")
    print("ğŸ¨ è§£ç å™¨è´Ÿè´£ä»ç‰¹å¾é‡å»ºåƒç´ ")
    print("ğŸ”„ å®Œæ•´çš„MAE = ç¼–ç å™¨ + è§£ç å™¨")

if __name__ == "__main__":
    main()

