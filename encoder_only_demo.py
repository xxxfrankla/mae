#!/usr/bin/env python3
"""
MAEç¼–ç å™¨æ¼”ç¤ºè„šæœ¬
åªä½¿ç”¨é¢„è®­ç»ƒçš„ç¼–ç å™¨éƒ¨åˆ†ï¼Œå±•ç¤ºç‰¹å¾æå–èƒ½åŠ›
"""

import os
import sys
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

try:
    import torch
    import torchvision.transforms as transforms
    import models_mae
    print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£… PyTorch: pip install torch torchvision")
    sys.exit(1)

def load_encoder_only():
    """åŠ è½½é¢„è®­ç»ƒçš„MAEç¼–ç å™¨"""
    print("\nğŸ”„ åŠ è½½é¢„è®­ç»ƒMAEç¼–ç å™¨...")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print(f"âœ… ä½¿ç”¨ Apple Silicon MPS")
    else:
        device = torch.device('cpu')
        print(f"âœ… ä½¿ç”¨ CPU")
    
    # æ¨¡å‹è·¯å¾„
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    
    if not os.path.exists(pretrain_path):
        print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {pretrain_path}")
        return None, None
    
    try:
        # åˆ›å»ºå®Œæ•´æ¨¡å‹
        model = models_mae.mae_vit_base_patch16()
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        print(f"ğŸ“¥ ä» {pretrain_path} åŠ è½½æƒé‡...")
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        
        # åªåŠ è½½ç¼–ç å™¨éƒ¨åˆ†çš„æƒé‡
        encoder_state_dict = {}
        for key, value in checkpoint['model'].items():
            if not key.startswith('decoder') and key != 'mask_token':
                encoder_state_dict[key] = value
        
        # ä½¿ç”¨strict=Falseæ¥å¿½ç•¥è§£ç å™¨æƒé‡
        model.load_state_dict(encoder_state_dict, strict=False)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(device)
        model.eval()
        
        print("âœ… é¢„è®­ç»ƒç¼–ç å™¨åŠ è½½æˆåŠŸï¼")
        return model, device
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def create_test_images():
    """åˆ›å»ºå¤šä¸ªæµ‹è¯•å›¾åƒ"""
    print("\nğŸ¨ åˆ›å»ºæµ‹è¯•å›¾åƒ...")
    
    images = []
    
    # å›¾åƒ1ï¼šå‡ ä½•å›¾æ¡ˆ
    img1 = np.zeros((224, 224, 3))
    # çº¢è‰²çŸ©å½¢
    img1[50:100, 50:150, 0] = 1.0
    # ç»¿è‰²åœ†å½¢
    y, x = np.ogrid[:224, :224]
    center_y, center_x = 112, 112
    mask = (x - center_x)**2 + (y - center_y)**2 <= 30**2
    img1[mask, 1] = 1.0
    images.append(("å‡ ä½•å›¾æ¡ˆ", img1))
    
    # å›¾åƒ2ï¼šæ¸å˜
    img2 = np.zeros((224, 224, 3))
    for i in range(224):
        img2[i, :, 0] = i / 224  # çº¢è‰²æ¸å˜
        img2[:, i, 1] = i / 224  # ç»¿è‰²æ¸å˜
    images.append(("å½©è‰²æ¸å˜", img2))
    
    # å›¾åƒ3ï¼šæ£‹ç›˜æ ¼
    img3 = np.zeros((224, 224, 3))
    for i in range(0, 224, 32):
        for j in range(0, 224, 32):
            if (i//32 + j//32) % 2 == 0:
                img3[i:i+32, j:j+32] = 1.0
    images.append(("æ£‹ç›˜æ ¼", img3))
    
    return images

def extract_features(model, device, images):
    """æå–å›¾åƒç‰¹å¾"""
    print("\nğŸ” æå–å›¾åƒç‰¹å¾...")
    
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    features_list = []
    
    for name, img in images:
        print(f"  å¤„ç†å›¾åƒ: {name}")
        
        # è½¬æ¢ä¸ºtensor
        img_tensor = transform(img.astype(np.float32)).unsqueeze(0).to(device)
        
        with torch.no_grad():
            # åªä½¿ç”¨ç¼–ç å™¨éƒ¨åˆ†
            # å›¾åƒåˆ†å—
            x = model.patch_embed(img_tensor)
            
            # æ·»åŠ ä½ç½®ç¼–ç 
            x = x + model.pos_embed[:, 1:, :]
            
            # æ·»åŠ cls token
            cls_token = model.cls_token + model.pos_embed[:, :1, :]
            cls_tokens = cls_token.expand(x.shape[0], -1, -1)
            x = torch.cat((cls_tokens, x), dim=1)
            
            # é€šè¿‡ç¼–ç å™¨
            for blk in model.blocks:
                x = blk(x)
            x = model.norm(x)
            
            # æå–ç‰¹å¾
            cls_feature = x[:, 0]  # CLS tokenç‰¹å¾
            patch_features = x[:, 1:]  # patchç‰¹å¾
            
            features_list.append({
                'name': name,
                'cls_feature': cls_feature.cpu().numpy(),
                'patch_features': patch_features.cpu().numpy(),
                'original_image': img
            })
            
            print(f"    CLSç‰¹å¾ç»´åº¦: {cls_feature.shape}")
            print(f"    Patchç‰¹å¾ç»´åº¦: {patch_features.shape}")
    
    return features_list

def visualize_features(features_list):
    """å¯è§†åŒ–ç‰¹å¾"""
    print("\nğŸ“Š å¯è§†åŒ–ç‰¹å¾...")
    
    num_images = len(features_list)
    fig, axes = plt.subplots(3, num_images, figsize=(num_images*4, 12))
    
    if num_images == 1:
        axes = axes.reshape(-1, 1)
    
    for i, features in enumerate(features_list):
        # åŸå§‹å›¾åƒ
        axes[0, i].imshow(features['original_image'])
        axes[0, i].set_title(f"{features['name']}\nåŸå§‹å›¾åƒ")
        axes[0, i].axis('off')
        
        # Patchç‰¹å¾çš„å¹³å‡å€¼å¯è§†åŒ–
        patch_features = features['patch_features'][0]  # (196, 768)
        patch_mean = patch_features.mean(axis=1)  # æ¯ä¸ªpatchçš„å¹³å‡ç‰¹å¾å€¼
        patch_2d = patch_mean.reshape(14, 14)  # é‡å¡‘ä¸º14x14çš„ç‰¹å¾å›¾
        
        im1 = axes[1, i].imshow(patch_2d, cmap='viridis')
        axes[1, i].set_title('Patchç‰¹å¾å‡å€¼')
        axes[1, i].axis('off')
        plt.colorbar(im1, ax=axes[1, i], fraction=0.046, pad=0.04)
        
        # CLSç‰¹å¾çš„å‰64ç»´å¯è§†åŒ–
        cls_feature = features['cls_feature'][0][:64]  # å–å‰64ç»´
        cls_2d = cls_feature.reshape(8, 8)  # é‡å¡‘ä¸º8x8æ˜¾ç¤º
        
        im2 = axes[2, i].imshow(cls_2d, cmap='coolwarm')
        axes[2, i].set_title('CLSç‰¹å¾ (å‰64ç»´)')
        axes[2, i].axis('off')
        plt.colorbar(im2, ax=axes[2, i], fraction=0.046, pad=0.04)
    
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    output_path = 'mae_encoder_features_demo.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"âœ… ç‰¹å¾å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
    
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return output_path

def analyze_features(features_list):
    """åˆ†æç‰¹å¾ç»Ÿè®¡"""
    print("\nğŸ“ˆ ç‰¹å¾åˆ†æ:")
    
    for features in features_list:
        name = features['name']
        cls_feat = features['cls_feature'][0]
        patch_feat = features['patch_features'][0]
        
        print(f"\n  {name}:")
        print(f"    CLSç‰¹å¾ç»Ÿè®¡:")
        print(f"      å‡å€¼: {cls_feat.mean():.4f}")
        print(f"      æ ‡å‡†å·®: {cls_feat.std():.4f}")
        print(f"      æœ€å¤§å€¼: {cls_feat.max():.4f}")
        print(f"      æœ€å°å€¼: {cls_feat.min():.4f}")
        
        print(f"    Patchç‰¹å¾ç»Ÿè®¡:")
        print(f"      å‡å€¼: {patch_feat.mean():.4f}")
        print(f"      æ ‡å‡†å·®: {patch_feat.std():.4f}")
        print(f"      ç‰¹å¾å¤šæ ·æ€§: {patch_feat.std(axis=0).mean():.4f}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ MAE é¢„è®­ç»ƒç¼–ç å™¨æ¼”ç¤º")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    # åŠ è½½ç¼–ç å™¨
    model, device = load_encoder_only()
    if model is None:
        return
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_images = create_test_images()
    
    # æå–ç‰¹å¾
    features = extract_features(model, device, test_images)
    
    # å¯è§†åŒ–ç‰¹å¾
    output_path = visualize_features(features)
    
    # åˆ†æç‰¹å¾
    analyze_features(features)
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœå›¾åƒ: {output_path}")
    print("\nğŸ’¡ è¯´æ˜:")
    print("1. è¿™å±•ç¤ºäº†é¢„è®­ç»ƒMAEç¼–ç å™¨çš„ç‰¹å¾æå–èƒ½åŠ›")
    print("2. CLS tokenåŒ…å«äº†æ•´ä¸ªå›¾åƒçš„å…¨å±€ç‰¹å¾")
    print("3. Patchç‰¹å¾æ˜¾ç¤ºäº†æ¯ä¸ª16x16åŒºåŸŸçš„å±€éƒ¨ç‰¹å¾")
    print("4. è¿™äº›ç‰¹å¾å¯ä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡å¦‚åˆ†ç±»ã€æ£€æµ‹ç­‰")

if __name__ == "__main__":
    main()
