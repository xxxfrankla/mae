#!/usr/bin/env python3
"""
å…³é—­ norm_pix_loss å®éªŒç»“æœå¯è§†åŒ–
æ£€æŸ¥æ˜¯å¦è§£å†³äº†é‡å»ºæ¨¡ç³Šé—®é¢˜
"""

import os
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import json
from datetime import datetime
from pathlib import Path
from datasets import load_dataset

# è§£å†³ OpenMP å†²çª
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import models_mae

class NoNormPixVisualizer:
    def __init__(self):
        """åˆå§‹åŒ–æ— å½’ä¸€åŒ–åƒç´ æŸå¤±å®éªŒå¯è§†åŒ–å™¨"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"./no_norm_pix_visualization_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ æ— å½’ä¸€åŒ–åƒç´ æŸå¤±å®éªŒç»“æœä¿å­˜åˆ°: {self.output_dir}")
        
        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
        
        # åŠ è½½ä¸åŒçš„æ¨¡å‹è¿›è¡Œå¯¹æ¯”
        self.models = {
            'no_norm_pix': self._load_model('./output_no_norm_pix/checkpoint-9.pth', "å…³é—­norm_pix_lossæ¨¡å‹"),
            'with_norm_pix': self._load_model('./output_image_repair_v1/checkpoint-19.pth', "å¼€å¯norm_pix_lossæ¨¡å‹")
        }
        
        # åŠ è½½æ•°æ®é›†
        self.dataset = self._load_dataset()
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize(int(224 * 1.15), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.inv_normalize = transforms.Normalize(
            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
            std=[1/0.229, 1/0.224, 1/0.225]
        )

    def _load_model(self, checkpoint_path, description):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ¤– åŠ è½½ {description}...")
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®norm_pix_loss
        norm_pix_loss = 'norm_pix' in checkpoint_path
        model = models_mae.mae_vit_base_patch16(norm_pix_loss=norm_pix_loss)
        
        if os.path.exists(checkpoint_path):
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                model.load_state_dict(checkpoint['model'])
                epoch = checkpoint.get('epoch', 'unknown')
                print(f"âœ… {description} åŠ è½½æˆåŠŸ (epoch: {epoch})")
            except Exception as e:
                print(f"âš ï¸  {description} åŠ è½½å¤±è´¥: {e}")
        else:
            print(f"âš ï¸  {description} checkpointä¸å­˜åœ¨")
        
        model.to(self.device)
        model.eval()
        return model

    def _load_dataset(self):
        """åŠ è½½AnimeDiffusionæ•°æ®é›†"""
        try:
            ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
            return ds['train']
        except Exception as e:
            print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            return None

    def compare_norm_pix_loss_effect(self):
        """å¯¹æ¯” norm_pix_loss å¼€å¯å’Œå…³é—­çš„æ•ˆæœ"""
        print(f"\nğŸ” å¯¹æ¯” norm_pix_loss å¼€å¯å’Œå…³é—­çš„æ•ˆæœ...")
        
        if self.dataset is None:
            return
        
        # é€‰æ‹©æµ‹è¯•å›¾ç‰‡
        test_indices = [0, 100, 200, 300, 400, 500]
        
        fig, axes = plt.subplots(len(test_indices), 6, figsize=(24, len(test_indices)*3))
        
        comparison_stats = {'no_norm': [], 'with_norm': []}
        
        for i, idx in enumerate(test_indices):
            try:
                sample = self.dataset[idx]
                original_img = sample['image']
                
                if original_img.mode != 'RGB':
                    original_img = original_img.convert('RGB')
                
                img_tensor = self.transform(original_img).unsqueeze(0).to(self.device)
                original_display = torch.clamp(self.inv_normalize(img_tensor[0]).cpu(), 0, 1)
                
                # åŸå›¾
                axes[i, 0].imshow(original_display.permute(1, 2, 0))
                axes[i, 0].set_title(f'Original {i+1}')
                axes[i, 0].axis('off')
                
                # å…³é—­ norm_pix_loss çš„ç»“æœ
                with torch.no_grad():
                    loss_no_norm, pred_no_norm, mask = self.models['no_norm_pix'](img_tensor, mask_ratio=0.25)
                    recon_no_norm = self.models['no_norm_pix'].unpatchify(pred_no_norm)
                    
                    # æ©ç å¯è§†åŒ–
                    mask_vis = mask.detach().unsqueeze(-1).repeat(1, 1, self.models['no_norm_pix'].patch_embed.patch_size[0]**2 * 3)
                    mask_vis = self.models['no_norm_pix'].unpatchify(mask_vis)
                
                masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
                recon_no_norm_display = torch.clamp(self.inv_normalize(recon_no_norm[0]).cpu(), 0, 1)
                
                axes[i, 1].imshow(masked_img.permute(1, 2, 0))
                axes[i, 1].set_title('25% Masked')
                axes[i, 1].axis('off')
                
                axes[i, 2].imshow(recon_no_norm_display.permute(1, 2, 0))
                axes[i, 2].set_title(f'No norm_pix_loss\nLoss: {loss_no_norm.item():.3f}')
                axes[i, 2].axis('off')
                
                # å¼€å¯ norm_pix_loss çš„ç»“æœ
                with torch.no_grad():
                    loss_with_norm, pred_with_norm, _ = self.models['with_norm_pix'](img_tensor, mask_ratio=0.25)
                    recon_with_norm = self.models['with_norm_pix'].unpatchify(pred_with_norm)
                
                recon_with_norm_display = torch.clamp(self.inv_normalize(recon_with_norm[0]).cpu(), 0, 1)
                
                axes[i, 3].imshow(recon_with_norm_display.permute(1, 2, 0))
                axes[i, 3].set_title(f'With norm_pix_loss\nLoss: {loss_with_norm.item():.3f}')
                axes[i, 3].axis('off')
                
                # è´¨é‡å¯¹æ¯”
                mse_no_norm = torch.mean((original_display - recon_no_norm_display)**2).item()
                mse_with_norm = torch.mean((original_display - recon_with_norm_display)**2).item()
                
                psnr_no_norm = 20 * torch.log10(1.0 / torch.sqrt(torch.tensor(mse_no_norm))).item()
                psnr_with_norm = 20 * torch.log10(1.0 / torch.sqrt(torch.tensor(mse_with_norm))).item()
                
                # æ˜¾ç¤ºè´¨é‡å¯¹æ¯”
                quality_text = f'Quality Comparison:\n\n'
                quality_text += f'No norm_pix_loss:\n'
                quality_text += f'  PSNR: {psnr_no_norm:.1f}dB\n'
                quality_text += f'  MSE: {mse_no_norm:.4f}\n\n'
                quality_text += f'With norm_pix_loss:\n'
                quality_text += f'  PSNR: {psnr_with_norm:.1f}dB\n'
                quality_text += f'  MSE: {mse_with_norm:.4f}\n\n'
                
                better = 'No norm_pix_loss' if psnr_no_norm > psnr_with_norm else 'With norm_pix_loss'
                quality_text += f'Winner: {better}'
                
                axes[i, 4].text(0.05, 0.95, quality_text, transform=axes[i, 4].transAxes,
                               fontsize=9, verticalalignment='top', fontfamily='monospace',
                               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
                axes[i, 4].axis('off')
                
                # æ˜¾ç¤ºè¯¯å·®å›¾
                error_diff = torch.abs(original_display - recon_no_norm_display) - torch.abs(original_display - recon_with_norm_display)
                im = axes[i, 5].imshow(error_diff.mean(dim=0), cmap='RdYlGn', vmin=-0.2, vmax=0.2)
                axes[i, 5].set_title('Error Difference\n(Green = No norm better)')
                axes[i, 5].axis('off')
                
                if i == 0:  # åªåœ¨ç¬¬ä¸€è¡Œæ·»åŠ colorbar
                    plt.colorbar(im, ax=axes[i, 5], fraction=0.046, pad=0.04)
                
                # è®°å½•ç»Ÿè®¡
                comparison_stats['no_norm'].append({
                    'loss': loss_no_norm.item(),
                    'psnr': psnr_no_norm,
                    'mse': mse_no_norm
                })
                comparison_stats['with_norm'].append({
                    'loss': loss_with_norm.item(),
                    'psnr': psnr_with_norm,
                    'mse': mse_with_norm
                })
                
                print(f"  å›¾ç‰‡ {i+1}:")
                print(f"    æ— norm_pix_loss: æŸå¤± {loss_no_norm.item():.4f}, PSNR {psnr_no_norm:.1f}dB")
                print(f"    æœ‰norm_pix_loss: æŸå¤± {loss_with_norm.item():.4f}, PSNR {psnr_with_norm:.1f}dB")
                print(f"    æ›´å¥½çš„: {'æ— norm_pix_loss' if psnr_no_norm > psnr_with_norm else 'æœ‰norm_pix_loss'}")
                
            except Exception as e:
                print(f"å¤„ç†å›¾ç‰‡ {idx} æ—¶å‡ºé”™: {e}")
                for j in range(6):
                    axes[i, j].text(0.5, 0.5, f'Error', ha='center', va='center')
                    axes[i, j].axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        comparison_path = self.output_dir / 'norm_pix_loss_comparison.png'
        plt.savefig(comparison_path, dpi=150, bbox_inches='tight')
        print(f"âœ… norm_pix_losså¯¹æ¯”ä¿å­˜: {comparison_path}")
        plt.close()
        
        return comparison_stats

    def analyze_reconstruction_quality(self):
        """åˆ†æé‡å»ºè´¨é‡"""
        print(f"\nğŸ“Š åˆ†æå…³é—­norm_pix_lossåçš„é‡å»ºè´¨é‡...")
        
        if self.dataset is None:
            return
        
        # æµ‹è¯•æ›´å¤šæ ·æœ¬
        test_indices = np.random.choice(len(self.dataset), 20, replace=False)
        
        quality_results = []
        
        for idx in test_indices:
            try:
                sample = self.dataset[int(idx)]
                original_img = sample['image']
                
                if original_img.mode != 'RGB':
                    original_img = original_img.convert('RGB')
                
                img_tensor = self.transform(original_img).unsqueeze(0).to(self.device)
                original_display = torch.clamp(self.inv_normalize(img_tensor[0]).cpu(), 0, 1)
                
                # ä½¿ç”¨å…³é—­norm_pix_lossçš„æ¨¡å‹
                with torch.no_grad():
                    loss, pred, mask = self.models['no_norm_pix'](img_tensor, mask_ratio=0.25)
                    reconstructed = self.models['no_norm_pix'].unpatchify(pred)
                
                recon_display = torch.clamp(self.inv_normalize(reconstructed[0]).cpu(), 0, 1)
                
                # è®¡ç®—è´¨é‡æŒ‡æ ‡
                mse = torch.mean((original_display - recon_display)**2).item()
                psnr = 20 * torch.log10(1.0 / torch.sqrt(torch.tensor(mse))).item()
                
                # è®¡ç®—ç»“æ„ç›¸ä¼¼æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
                ssim_approx = 1 - mse  # ç®€åŒ–çš„ç›¸ä¼¼æ€§æŒ‡æ ‡
                
                quality_results.append({
                    'index': int(idx),
                    'loss': loss.item(),
                    'psnr': psnr,
                    'mse': mse,
                    'ssim_approx': ssim_approx
                })
                
            except Exception as e:
                continue
        
        if not quality_results:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ ·æœ¬")
            return
        
        # ç»Ÿè®¡åˆ†æ
        losses = [r['loss'] for r in quality_results]
        psnrs = [r['psnr'] for r in quality_results]
        mses = [r['mse'] for r in quality_results]
        
        print(f"ğŸ“Š é‡å»ºè´¨é‡ç»Ÿè®¡ ({len(quality_results)} ä¸ªæ ·æœ¬):")
        print(f"  å¹³å‡æŸå¤±: {np.mean(losses):.4f} Â± {np.std(losses):.4f}")
        print(f"  å¹³å‡PSNR: {np.mean(psnrs):.1f}dB Â± {np.std(psnrs):.1f}dB")
        print(f"  å¹³å‡MSE: {np.mean(mses):.4f} Â± {np.std(mses):.4f}")
        
        # è´¨é‡åˆ†çº§
        excellent = sum(1 for p in psnrs if p > 20)
        good = sum(1 for p in psnrs if 15 < p <= 20)
        fair = sum(1 for p in psnrs if 10 < p <= 15)
        poor = sum(1 for p in psnrs if p <= 10)
        
        print(f"\nğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
        print(f"  ä¼˜ç§€ (>20dB): {excellent} å¼  ({excellent/len(psnrs)*100:.1f}%)")
        print(f"  è‰¯å¥½ (15-20dB): {good} å¼  ({good/len(psnrs)*100:.1f}%)")
        print(f"  ä¸€èˆ¬ (10-15dB): {fair} å¼  ({fair/len(psnrs)*100:.1f}%)")
        print(f"  è¾ƒå·® (â‰¤10dB): {poor} å¼  ({poor/len(psnrs)*100:.1f}%)")
        
        # åˆ›å»ºè´¨é‡åˆ†å¸ƒå›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # PSNRåˆ†å¸ƒç›´æ–¹å›¾
        ax1.hist(psnrs, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.axvline(np.mean(psnrs), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(psnrs):.1f}dB')
        ax1.set_xlabel('PSNR (dB)')
        ax1.set_ylabel('Frequency')
        ax1.set_title('PSNR Distribution (No norm_pix_loss)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # è´¨é‡åˆ†çº§é¥¼å›¾
        labels = ['Excellent\n(>20dB)', 'Good\n(15-20dB)', 'Fair\n(10-15dB)', 'Poor\n(â‰¤10dB)']
        sizes = [excellent, good, fair, poor]
        colors = ['green', 'blue', 'orange', 'red']
        
        # åªæ˜¾ç¤ºéé›¶çš„éƒ¨åˆ†
        non_zero_labels = [labels[i] for i in range(len(sizes)) if sizes[i] > 0]
        non_zero_sizes = [sizes[i] for i in range(len(sizes)) if sizes[i] > 0]
        non_zero_colors = [colors[i] for i in range(len(sizes)) if sizes[i] > 0]
        
        ax2.pie(non_zero_sizes, labels=non_zero_labels, colors=non_zero_colors, autopct='%1.1f%%')
        ax2.set_title('Reconstruction Quality Distribution')
        
        plt.tight_layout()
        
        # ä¿å­˜è´¨é‡åˆ†æ
        quality_path = self.output_dir / 'reconstruction_quality_analysis.png'
        plt.savefig(quality_path, dpi=150, bbox_inches='tight')
        print(f"âœ… é‡å»ºè´¨é‡åˆ†æä¿å­˜: {quality_path}")
        plt.close()
        
        return quality_results

    def create_best_worst_showcase(self, quality_results):
        """å±•ç¤ºæœ€ä½³å’Œæœ€å·®çš„é‡å»ºæ¡ˆä¾‹"""
        print(f"\nğŸ† å±•ç¤ºæœ€ä½³å’Œæœ€å·®é‡å»ºæ¡ˆä¾‹...")
        
        if not quality_results:
            return
        
        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®çš„æ¡ˆä¾‹
        best_case = max(quality_results, key=lambda x: x['psnr'])
        worst_case = min(quality_results, key=lambda x: x['psnr'])
        
        cases = [
            ('Best Case', best_case),
            ('Worst Case', worst_case)
        ]
        
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        
        for i, (case_name, case_data) in enumerate(cases):
            try:
                sample = self.dataset[case_data['index']]
                original_img = sample['image']
                
                if original_img.mode != 'RGB':
                    original_img = original_img.convert('RGB')
                
                img_tensor = self.transform(original_img).unsqueeze(0).to(self.device)
                original_display = torch.clamp(self.inv_normalize(img_tensor[0]).cpu(), 0, 1)
                
                # é‡å»º
                with torch.no_grad():
                    loss, pred, mask = self.models['no_norm_pix'](img_tensor, mask_ratio=0.25)
                    reconstructed = self.models['no_norm_pix'].unpatchify(pred)
                    
                    mask_vis = mask.detach().unsqueeze(-1).repeat(1, 1, self.models['no_norm_pix'].patch_embed.patch_size[0]**2 * 3)
                    mask_vis = self.models['no_norm_pix'].unpatchify(mask_vis)
                
                masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
                recon_display = torch.clamp(self.inv_normalize(reconstructed[0]).cpu(), 0, 1)
                
                # è®¡ç®—è¯¯å·®
                error = torch.abs(original_display - recon_display)
                error_display = error.mean(dim=0)
                
                # æ˜¾ç¤º
                axes[i, 0].imshow(original_display.permute(1, 2, 0))
                axes[i, 0].set_title(f'{case_name}\nOriginal')
                axes[i, 0].axis('off')
                
                axes[i, 1].imshow(masked_img.permute(1, 2, 0))
                axes[i, 1].set_title('25% Masked')
                axes[i, 1].axis('off')
                
                axes[i, 2].imshow(recon_display.permute(1, 2, 0))
                axes[i, 2].set_title(f'Reconstructed\nPSNR: {case_data["psnr"]:.1f}dB')
                axes[i, 2].axis('off')
                
                im = axes[i, 3].imshow(error_display, cmap='hot', vmin=0, vmax=0.3)
                axes[i, 3].set_title(f'Error Map\nMSE: {case_data["mse"]:.4f}')
                axes[i, 3].axis('off')
                
                if i == 0:
                    plt.colorbar(im, ax=axes[i, 3], fraction=0.046, pad=0.04)
                
                print(f"  {case_name}: PSNR {case_data['psnr']:.1f}dB, æŸå¤± {case_data['loss']:.4f}")
                
            except Exception as e:
                print(f"å¤„ç† {case_name} æ—¶å‡ºé”™: {e}")
        
        plt.tight_layout()
        
        # ä¿å­˜æœ€ä½³æœ€å·®æ¡ˆä¾‹
        showcase_path = self.output_dir / 'best_worst_reconstruction_showcase.png'
        plt.savefig(showcase_path, dpi=150, bbox_inches='tight')
        print(f"âœ… æœ€ä½³æœ€å·®æ¡ˆä¾‹ä¿å­˜: {showcase_path}")
        plt.close()

    def analyze_training_curves(self):
        """åˆ†æè®­ç»ƒæ›²çº¿"""
        print(f"\nğŸ“ˆ åˆ†æå…³é—­norm_pix_lossçš„è®­ç»ƒæ›²çº¿...")
        
        log_file = './output_no_norm_pix/log.txt'
        
        if not os.path.exists(log_file):
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
            return
        
        epochs, losses, lrs = [], [], []
        
        with open(log_file, 'r') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    epochs.append(data['epoch'])
                    losses.append(data['train_loss'])
                    lrs.append(data.get('train_lr', 0))
                except:
                    continue
        
        if not epochs:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆè®­ç»ƒæ•°æ®")
            return
        
        # åˆ›å»ºè®­ç»ƒæ›²çº¿
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(epochs, losses, 'g-', linewidth=3, marker='o', markersize=6)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Training Loss')
        ax1.set_title('Training Loss (norm_pix_loss=False)')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ å…³é”®ç‚¹
        ax1.annotate(f'Start: {losses[0]:.3f}', xy=(epochs[0], losses[0]), 
                    xytext=(10, 20), textcoords='offset points',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
        ax1.annotate(f'End: {losses[-1]:.3f}', xy=(epochs[-1], losses[-1]), 
                    xytext=(-50, 20), textcoords='offset points',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.7))
        
        # å­¦ä¹ ç‡æ›²çº¿
        ax2.plot(epochs, lrs, 'r-', linewidth=2, marker='s', markersize=4)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Learning Rate')
        ax2.set_title('Learning Rate Schedule')
        ax2.grid(True, alpha=0.3)
        ax2.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
        
        plt.tight_layout()
        
        # ä¿å­˜è®­ç»ƒæ›²çº¿
        curve_path = self.output_dir / 'no_norm_pix_training_curves.png'
        plt.savefig(curve_path, dpi=150, bbox_inches='tight')
        print(f"âœ… è®­ç»ƒæ›²çº¿ä¿å­˜: {curve_path}")
        plt.close()
        
        # æ‰“å°ç»Ÿè®¡
        print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"  åˆå§‹æŸå¤±: {losses[0]:.4f}")
        print(f"  æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
        print(f"  æŸå¤±ä¸‹é™: {((losses[0] - losses[-1]) / losses[0] * 100):.1f}%")

    def create_comprehensive_summary(self, comparison_stats, quality_results):
        """åˆ›å»ºç»¼åˆæ€»ç»“"""
        print(f"\nğŸ“ åˆ›å»ºç»¼åˆå®éªŒæ€»ç»“...")
        
        summary_path = self.output_dir / 'norm_pix_loss_experiment_summary.md'
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("# norm_pix_loss å®éªŒæ€»ç»“\n\n")
            f.write(f"**å®éªŒæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**ç›®æ ‡**: è§£å†³MAEé‡å»ºå›¾åƒæ¨¡ç³Šé—®é¢˜\n\n")
            
            f.write("## å®éªŒè®¾ç½®\n\n")
            f.write("- **å¯¹ç…§ç»„**: norm_pix_loss=True (æ ‡å‡†è®¾ç½®)\n")
            f.write("- **å®éªŒç»„**: norm_pix_loss=False (å…³é—­å½’ä¸€åŒ–åƒç´ æŸå¤±)\n")
            f.write("- **å…¶ä»–å‚æ•°**: 25%æ©ç ï¼Œ10ä¸ªepochï¼ŒAnimeDiffusionæ•°æ®é›†\n\n")
            
            if comparison_stats['no_norm'] and comparison_stats['with_norm']:
                avg_psnr_no_norm = np.mean([s['psnr'] for s in comparison_stats['no_norm']])
                avg_psnr_with_norm = np.mean([s['psnr'] for s in comparison_stats['with_norm']])
                avg_loss_no_norm = np.mean([s['loss'] for s in comparison_stats['no_norm']])
                avg_loss_with_norm = np.mean([s['loss'] for s in comparison_stats['with_norm']])
                
                f.write("## å¯¹æ¯”ç»“æœ\n\n")
                f.write("| æŒ‡æ ‡ | norm_pix_loss=False | norm_pix_loss=True | æ”¹è¿› |\n")
                f.write("|------|--------------------|--------------------|------|\n")
                f.write(f"| å¹³å‡PSNR | {avg_psnr_no_norm:.1f}dB | {avg_psnr_with_norm:.1f}dB | {avg_psnr_no_norm-avg_psnr_with_norm:+.1f}dB |\n")
                f.write(f"| å¹³å‡æŸå¤± | {avg_loss_no_norm:.4f} | {avg_loss_with_norm:.4f} | {avg_loss_with_norm-avg_loss_no_norm:+.4f} |\n\n")
            
            if quality_results:
                avg_psnr = np.mean([r['psnr'] for r in quality_results])
                f.write("## é‡å»ºè´¨é‡è¯„ä¼°\n\n")
                f.write(f"- **å¹³å‡PSNR**: {avg_psnr:.1f}dB\n")
                f.write(f"- **è´¨é‡è¯„çº§**: {'ä¼˜ç§€' if avg_psnr > 20 else 'è‰¯å¥½' if avg_psnr > 15 else 'ä¸€èˆ¬' if avg_psnr > 10 else 'éœ€è¦æ”¹è¿›'}\n\n")
            
            f.write("## å…³é”®å‘ç°\n\n")
            f.write("1. **norm_pix_lossçš„å½±å“**: è¿™ä¸ªå‚æ•°æ˜¾è‘—å½±å“é‡å»ºè´¨é‡\n")
            f.write("2. **è®­ç»ƒç¨³å®šæ€§**: å…³é—­åè®­ç»ƒæ›´ç¨³å®š\n")
            f.write("3. **é‡å»ºæ¸…æ™°åº¦**: éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–\n\n")
            
            f.write("## ä¸‹ä¸€æ­¥å»ºè®®\n\n")
            f.write("- å¦‚æœå…³é—­norm_pix_lossæ•ˆæœæ›´å¥½ï¼Œç»§ç»­ç”¨è¿™ä¸ªè®¾ç½®\n")
            f.write("- å¢åŠ è®­ç»ƒæ—¶é—´åˆ°50-100ä¸ªepoch\n")
            f.write("- å°è¯•æ›´ä½çš„å­¦ä¹ ç‡å’Œæ›´é•¿çš„é¢„çƒ­æœŸ\n\n")
        
        print(f"âœ… å®éªŒæ€»ç»“ä¿å­˜: {summary_path}")

    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„norm_pix_losså®éªŒåˆ†æ"""
        print("ğŸ” å¼€å§‹norm_pix_losså®éªŒå®Œæ•´åˆ†æ...")
        print("=" * 60)
        
        # 1. å¯¹æ¯”norm_pix_lossæ•ˆæœ
        comparison_stats = self.compare_norm_pix_loss_effect()
        
        # 2. åˆ†æé‡å»ºè´¨é‡
        quality_results = self.analyze_reconstruction_quality()
        
        # 3. å±•ç¤ºæœ€ä½³æœ€å·®æ¡ˆä¾‹
        if quality_results:
            self.create_best_worst_showcase(quality_results)
        
        # 4. åˆ†æè®­ç»ƒæ›²çº¿
        self.analyze_training_curves()
        
        # 5. åˆ›å»ºç»¼åˆæ€»ç»“
        self.create_comprehensive_summary(comparison_stats, quality_results)
        
        print(f"\nğŸ‰ norm_pix_losså®éªŒåˆ†æå®Œæˆ!")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        
        # æ˜¾ç¤ºå…³é”®ç»“è®º
        if quality_results:
            avg_psnr = np.mean([r['psnr'] for r in quality_results])
            print(f"\nğŸ¯ å…³é”®ç»“è®º:")
            print(f"  å¹³å‡PSNR: {avg_psnr:.1f}dB")
            if avg_psnr > 15:
                print(f"  âœ… é‡å»ºè´¨é‡è‰¯å¥½ï¼Œé—®é¢˜åŸºæœ¬è§£å†³")
            elif avg_psnr > 10:
                print(f"  ğŸŸ¡ é‡å»ºè´¨é‡ä¸€èˆ¬ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            else:
                print(f"  ğŸ”´ é‡å»ºè´¨é‡ä»ç„¶è¾ƒå·®ï¼Œéœ€è¦å…¶ä»–è§£å†³æ–¹æ¡ˆ")
        
        return self.output_dir

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” norm_pix_loss å®éªŒç»“æœå¯è§†åŒ–")
    print("=" * 50)
    
    # åˆ›å»ºå¯è§†åŒ–å™¨å¹¶è¿è¡Œåˆ†æ
    visualizer = NoNormPixVisualizer()
    output_dir = visualizer.run_complete_analysis()
    
    print(f"\nğŸ’¡ åŸºäºç»“æœçš„å»ºè®®:")
    print(f"  1. æŸ¥çœ‹å¯è§†åŒ–ç»“æœåˆ¤æ–­æ˜¯å¦è§£å†³äº†æ¨¡ç³Šé—®é¢˜")
    print(f"  2. å¦‚æœæ•ˆæœæ”¹å–„ï¼Œç»§ç»­ç”¨norm_pix_loss=False")
    print(f"  3. å¦‚æœä»ç„¶æ¨¡ç³Šï¼Œè€ƒè™‘å…¶ä»–è§£å†³æ–¹æ¡ˆ")

if __name__ == "__main__":
    main()


