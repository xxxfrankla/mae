#!/usr/bin/env python3
"""
è°ƒè¯•MAEé‡å»ºè´¨é‡é—®é¢˜
åˆ†æä¸ºä»€ä¹ˆé‡å»ºå›¾åƒæ¨¡ç³Šï¼Œå¹¶æä¾›è§£å†³æ–¹æ¡ˆ
"""

import os
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from datasets import load_dataset

# è§£å†³ OpenMP å†²çª
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import models_mae

def analyze_reconstruction_issues():
    """åˆ†æé‡å»ºè´¨é‡é—®é¢˜"""
    print("ğŸ” åˆ†æMAEé‡å»ºè´¨é‡é—®é¢˜...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # åŠ è½½ä¸åŒçš„æ¨¡å‹è¿›è¡Œå¯¹æ¯”
    models_to_test = [
        {
            'name': '25%æ©ç è®­ç»ƒæ¨¡å‹',
            'path': './output_animediffusion_mask25/checkpoint-9.pth',
            'mask_ratio': 0.25
        },
        {
            'name': '75%æ©ç è®­ç»ƒæ¨¡å‹', 
            'path': './output_animediffusion/checkpoint-4.pth',
            'mask_ratio': 0.75
        },
        {
            'name': 'éšæœºåˆå§‹åŒ–æ¨¡å‹',
            'path': None,
            'mask_ratio': 0.25
        }
    ]
    
    # åŠ è½½æµ‹è¯•å›¾ç‰‡
    try:
        ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
        test_sample = ds['train'][0]
        original_img = test_sample['image']
        
        if original_img.mode != 'RGB':
            original_img = original_img.convert('RGB')
        
        print(f"âœ… æµ‹è¯•å›¾ç‰‡åŠ è½½æˆåŠŸ: {original_img.size}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å›¾ç‰‡åŠ è½½å¤±è´¥: {e}")
        return
    
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize(int(224 * 1.15), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    img_tensor = transform(original_img).unsqueeze(0).to(device)
    original_display = torch.clamp(inv_normalize(img_tensor[0]).cpu(), 0, 1)
    
    fig, axes = plt.subplots(len(models_to_test), 4, figsize=(16, len(models_to_test)*3))
    
    for i, model_info in enumerate(models_to_test):
        print(f"\nğŸ¤– æµ‹è¯• {model_info['name']}...")
        
        # åŠ è½½æ¨¡å‹
        model = models_mae.mae_vit_base_patch16(norm_pix_loss=True)
        
        if model_info['path'] and os.path.exists(model_info['path']):
            try:
                checkpoint = torch.load(model_info['path'], map_location='cpu', weights_only=False)
                model.load_state_dict(checkpoint['model'])
                print(f"  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"  âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        else:
            print(f"  âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
        
        model.to(device)
        model.eval()
        
        # è¿›è¡Œé‡å»º
        with torch.no_grad():
            loss, pred, mask = model(img_tensor, mask_ratio=model_info['mask_ratio'])
            reconstructed = model.unpatchify(pred)
            
            # åˆ›å»ºæ©ç å¯è§†åŒ–
            mask_vis = mask.detach().unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
            mask_vis = model.unpatchify(mask_vis)
        
        # æ˜¾ç¤ºç»“æœ
        masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
        recon_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
        
        # è®¡ç®—é‡å»ºè´¨é‡æŒ‡æ ‡
        mse = torch.mean((original_display - recon_display)**2).item()
        psnr = 20 * torch.log10(1.0 / torch.sqrt(torch.mean((original_display - recon_display)**2))).item()
        
        # æ˜¾ç¤º
        axes[i, 0].imshow(original_display.permute(1, 2, 0))
        axes[i, 0].set_title('Original')
        axes[i, 0].axis('off')
        
        axes[i, 1].imshow(masked_img.permute(1, 2, 0))
        axes[i, 1].set_title(f'{model_info["mask_ratio"]*100:.0f}% Masked')
        axes[i, 1].axis('off')
        
        axes[i, 2].imshow(recon_display.permute(1, 2, 0))
        axes[i, 2].set_title(f'{model_info["name"]}\nLoss: {loss.item():.3f}\nPSNR: {psnr:.1f}dB')
        axes[i, 2].axis('off')
        
        # æ˜¾ç¤ºè¯¯å·®å›¾
        error = torch.abs(original_display - recon_display).mean(dim=0)
        im = axes[i, 3].imshow(error, cmap='hot', vmin=0, vmax=0.3)
        axes[i, 3].set_title(f'Error Map\nMSE: {mse:.4f}')
        axes[i, 3].axis('off')
        plt.colorbar(im, ax=axes[i, 3], fraction=0.046, pad=0.04)
        
        print(f"  æŸå¤±: {loss.item():.4f}, PSNR: {psnr:.1f}dB, MSE: {mse:.4f}")
    
    plt.tight_layout()
    plt.savefig('reconstruction_quality_debug.png', dpi=150, bbox_inches='tight')
    print(f"\nâœ… é‡å»ºè´¨é‡è°ƒè¯•å›¾ä¿å­˜: reconstruction_quality_debug.png")
    plt.close()

def identify_blur_causes():
    """è¯†åˆ«æ¨¡ç³Šçš„åŸå› """
    print(f"\nğŸ” è¯†åˆ«é‡å»ºæ¨¡ç³Šçš„å¯èƒ½åŸå› :")
    
    causes_and_solutions = [
        {
            'cause': 'è®­ç»ƒä¸å……åˆ†',
            'description': 'æ¨¡å‹è¿˜æ²¡æœ‰å­¦ä¼šæœ‰æ•ˆçš„é‡å»º',
            'solution': 'å¢åŠ è®­ç»ƒè½®æ•°åˆ°50-100ä¸ªepoch',
            'priority': 'HIGH'
        },
        {
            'cause': 'å­¦ä¹ ç‡è¿‡é«˜',
            'description': 'ä¼˜åŒ–æ­¥é•¿å¤ªå¤§ï¼Œå¯¼è‡´ä¸ç¨³å®š',
            'solution': 'é™ä½å­¦ä¹ ç‡ (blr=1e-4 æˆ–æ›´ä½)',
            'priority': 'MEDIUM'
        },
        {
            'cause': 'æ©ç æ¯”ä¾‹ä¸å½“',
            'description': '25%å¯èƒ½å¤ªç®€å•ï¼Œ75%å¯èƒ½å¤ªéš¾',
            'solution': 'å°è¯•50%æ©ç æ¯”ä¾‹',
            'priority': 'MEDIUM'
        },
        {
            'cause': 'æ•°æ®é¢„å¤„ç†é—®é¢˜',
            'description': 'å›¾åƒç¼©æ”¾æˆ–å½’ä¸€åŒ–ä¸å½“',
            'solution': 'æ£€æŸ¥å›¾åƒé¢„å¤„ç†æµç¨‹',
            'priority': 'HIGH'
        },
        {
            'cause': 'æ¨¡å‹å®¹é‡ä¸è¶³',
            'description': 'ViT-Baseå¯èƒ½å¯¹é«˜è´¨é‡å›¾åƒä¸å¤Ÿ',
            'solution': 'å°è¯•ViT-Largeæ¨¡å‹',
            'priority': 'LOW'
        }
    ]
    
    for i, item in enumerate(causes_and_solutions, 1):
        priority_color = {'HIGH': 'ğŸ”´', 'MEDIUM': 'ğŸŸ¡', 'LOW': 'ğŸŸ¢'}
        print(f"\n{i}. {priority_color[item['priority']]} {item['cause']}")
        print(f"   åŸå› : {item['description']}")
        print(f"   è§£å†³æ–¹æ¡ˆ: {item['solution']}")

def create_optimized_training_configs():
    """åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
    print(f"\nğŸ¯ åˆ›å»ºä¼˜åŒ–çš„25%æ©ç è®­ç»ƒé…ç½®...")
    
    configs = {
        'improved_25_mask': {
            'mask_ratio': 0.25,
            'epochs': 50,
            'batch_size': 6,
            'blr': 1e-4,  # é™ä½å­¦ä¹ ç‡
            'warmup_epochs': 10,
            'max_samples': 1000,
            'description': 'æ”¹è¿›çš„25%æ©ç é…ç½®'
        },
        'balanced_50_mask': {
            'mask_ratio': 0.5,
            'epochs': 30,
            'batch_size': 6,
            'blr': 1.2e-4,
            'warmup_epochs': 8,
            'max_samples': 1000,
            'description': 'å¹³è¡¡çš„50%æ©ç é…ç½®'
        },
        'fine_tuned_25': {
            'mask_ratio': 0.25,
            'epochs': 100,
            'batch_size': 4,
            'blr': 8e-5,  # æ›´ä½çš„å­¦ä¹ ç‡
            'warmup_epochs': 15,
            'max_samples': 2000,
            'description': 'ç²¾ç»†è°ƒä¼˜çš„25%æ©ç é…ç½®'
        }
    }
    
    print(f"\nğŸ“‹ æ¨èçš„ä¼˜åŒ–é…ç½®:")
    print(f"{'é…ç½®å':<20} {'æ©ç ':<6} {'è½®æ•°':<6} {'æ‰¹æ¬¡':<6} {'å­¦ä¹ ç‡':<10} {'æè¿°'}")
    print("-" * 80)
    
    for name, config in configs.items():
        print(f"{name:<20} {config['mask_ratio']*100:.0f}%{'':<3} {config['epochs']:<6} {config['batch_size']:<6} {config['blr']:.1e}{'':<3} {config['description']}")
    
    # ç”Ÿæˆè®­ç»ƒå‘½ä»¤
    print(f"\nğŸš€ æ¨èçš„è®­ç»ƒå‘½ä»¤:")
    
    for name, config in configs.items():
        print(f"\n# {config['description']}")
        cmd = f"python main_pretrain_animediffusion.py \\\n"
        cmd += f"    --mask_ratio {config['mask_ratio']} \\\n"
        cmd += f"    --epochs {config['epochs']} \\\n"
        cmd += f"    --batch_size {config['batch_size']} \\\n"
        cmd += f"    --blr {config['blr']:.1e} \\\n"
        cmd += f"    --warmup_epochs {config['warmup_epochs']} \\\n"
        cmd += f"    --max_samples {config['max_samples']} \\\n"
        cmd += f"    --output_dir ./output_{name} \\\n"
        cmd += f"    --log_dir ./output_{name}"
        print(cmd)
    
    return configs

def test_different_normalizations():
    """æµ‹è¯•ä¸åŒçš„å½’ä¸€åŒ–æ–¹æ³•"""
    print(f"\nğŸ§ª æµ‹è¯•ä¸åŒçš„å›¾åƒå½’ä¸€åŒ–æ–¹æ³•...")
    
    try:
        ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
        original_img = ds['train'][0]['image']
        
        if original_img.mode != 'RGB':
            original_img = original_img.convert('RGB')
        
        # æµ‹è¯•ä¸åŒçš„å½’ä¸€åŒ–æ–¹æ³•
        normalizations = [
            {
                'name': 'ImageNetæ ‡å‡†',
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            },
            {
                'name': 'é›¶å‡å€¼å•ä½æ–¹å·®',
                'mean': [0.5, 0.5, 0.5],
                'std': [0.5, 0.5, 0.5]
            },
            {
                'name': 'æ— å½’ä¸€åŒ–',
                'mean': [0.0, 0.0, 0.0],
                'std': [1.0, 1.0, 1.0]
            }
        ]
        
        fig, axes = plt.subplots(1, len(normalizations), figsize=(len(normalizations)*4, 4))
        
        device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
        model = models_mae.mae_vit_base_patch16(norm_pix_loss=True)
        model.to(device)
        model.eval()
        
        for i, norm_config in enumerate(normalizations):
            # åˆ›å»ºå˜æ¢
            transform = transforms.Compose([
                transforms.Resize(int(224 * 1.15), interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(mean=norm_config['mean'], std=norm_config['std'])
            ])
            
            inv_normalize = transforms.Normalize(
                mean=[-m/s for m, s in zip(norm_config['mean'], norm_config['std'])],
                std=[1/s for s in norm_config['std']]
            )
            
            # å¤„ç†å›¾åƒ
            img_tensor = transform(original_img).unsqueeze(0).to(device)
            
            with torch.no_grad():
                loss, pred, mask = model(img_tensor, mask_ratio=0.25)
                reconstructed = model.unpatchify(pred)
            
            # åå½’ä¸€åŒ–æ˜¾ç¤º
            recon_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
            
            axes[i].imshow(recon_display.permute(1, 2, 0))
            axes[i].set_title(f'{norm_config["name"]}\nLoss: {loss.item():.3f}')
            axes[i].axis('off')
            
            print(f"  {norm_config['name']}: æŸå¤± {loss.item():.4f}")
        
        plt.tight_layout()
        plt.savefig('normalization_comparison.png', dpi=150, bbox_inches='tight')
        print(f"âœ… å½’ä¸€åŒ–å¯¹æ¯”ä¿å­˜: normalization_comparison.png")
        plt.close()
        
    except Exception as e:
        print(f"å½’ä¸€åŒ–æµ‹è¯•å¤±è´¥: {e}")

def create_image_repair_config():
    """åˆ›å»ºä¸“é—¨ç”¨äºå›¾åƒä¿®å¤çš„é…ç½®"""
    print(f"\nğŸ› ï¸ åˆ›å»ºå›¾åƒä¿®å¤ä¸“ç”¨é…ç½®...")
    
    repair_config = {
        # åŸºæœ¬å‚æ•°
        'mask_ratio': 0.25,  # è¾ƒä½çš„æ©ç æ¯”ä¾‹ï¼Œé€‚åˆä¿®å¤
        'epochs': 100,       # å……åˆ†è®­ç»ƒ
        'batch_size': 4,     # è¾ƒå°æ‰¹æ¬¡ï¼Œæ›´ç¨³å®š
        'accum_iter': 16,    # ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
        
        # å­¦ä¹ ç‡ä¼˜åŒ–
        'blr': 5e-5,         # æ›´ä½çš„åŸºç¡€å­¦ä¹ ç‡
        'warmup_epochs': 20, # æ›´é•¿çš„é¢„çƒ­
        'min_lr': 1e-6,      # è®¾ç½®æœ€å°å­¦ä¹ ç‡
        
        # æ•°æ®å¤„ç†
        'input_size': 224,   # æ ‡å‡†å°ºå¯¸
        'resize_strategy': 'smart_crop',
        'max_samples': 2000, # é€‚ä¸­çš„æ ·æœ¬æ•°
        
        # æ­£åˆ™åŒ–
        'weight_decay': 0.02, # é™ä½æƒé‡è¡°å‡
        'norm_pix_loss': True,
        
        # ä¿å­˜ç­–ç•¥
        'save_freq': 10,
        'output_dir': './output_image_repair',
        'log_dir': './output_image_repair'
    }
    
    print(f"ğŸ“‹ å›¾åƒä¿®å¤ä¸“ç”¨é…ç½®:")
    for key, value in repair_config.items():
        print(f"  {key}: {value}")
    
    # ç”Ÿæˆè®­ç»ƒå‘½ä»¤
    cmd = "# å›¾åƒä¿®å¤ä¸“ç”¨MAEè®­ç»ƒ\n"
    cmd += "python main_pretrain_animediffusion.py \\\n"
    for key, value in repair_config.items():
        if key not in ['output_dir', 'log_dir']:  # è¿™äº›åœ¨å‘½ä»¤æœ«å°¾
            cmd += f"    --{key} {value} \\\n"
    cmd += f"    --output_dir {repair_config['output_dir']} \\\n"
    cmd += f"    --log_dir {repair_config['log_dir']}"
    
    print(f"\nğŸš€ å›¾åƒä¿®å¤è®­ç»ƒå‘½ä»¤:")
    print(cmd)
    
    # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
    with open('image_repair_config.sh', 'w') as f:
        f.write("#!/bin/bash\n")
        f.write("# å›¾åƒä¿®å¤ä¸“ç”¨MAEè®­ç»ƒé…ç½®\n")
        f.write("export KMP_DUPLICATE_LIB_OK=TRUE\n\n")
        f.write(cmd)
    
    os.chmod('image_repair_config.sh', 0o755)
    print(f"âœ… é…ç½®è„šæœ¬ä¿å­˜: image_repair_config.sh")
    
    return repair_config

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” MAEé‡å»ºè´¨é‡è°ƒè¯•å·¥å…·")
    print("=" * 50)
    
    # 1. åˆ†æé‡å»ºé—®é¢˜
    analyze_reconstruction_issues()
    
    # 2. è¯†åˆ«æ¨¡ç³ŠåŸå› 
    identify_blur_causes()
    
    # 3. æµ‹è¯•ä¸åŒå½’ä¸€åŒ–
    test_different_normalizations()
    
    # 4. åˆ›å»ºå›¾åƒä¿®å¤é…ç½®
    repair_config = create_image_repair_config()
    
    print(f"\nğŸ’¡ è§£å†³æ¨¡ç³Šé—®é¢˜çš„å»ºè®®:")
    print(f"  1ï¸âƒ£ ğŸ”´ å¢åŠ è®­ç»ƒè½®æ•° (å½“å‰10 â†’ å»ºè®®50-100)")
    print(f"  2ï¸âƒ£ ğŸŸ¡ é™ä½å­¦ä¹ ç‡ (å½“å‰1.5e-4 â†’ å»ºè®®5e-5)")
    print(f"  3ï¸âƒ£ ğŸŸ¡ å»¶é•¿é¢„çƒ­æœŸ (å½“å‰5 â†’ å»ºè®®20 epochs)")
    print(f"  4ï¸âƒ£ ğŸŸ¢ ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®")
    
    print(f"\nğŸ¯ æ¨èç«‹å³å°è¯•:")
    print(f"  bash image_repair_config.sh")

if __name__ == "__main__":
    main()


