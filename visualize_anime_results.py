#!/usr/bin/env python3
"""
åŠ¨æ¼«æ•°æ®é›† MAE è®­ç»ƒç»“æœå¯è§†åŒ–
å±•ç¤ºåœ¨çœŸå®åŠ¨æ¼«å›¾ç‰‡ä¸Šçš„é‡å»ºæ•ˆæœ
"""

import os
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import json
from datetime import datetime
from pathlib import Path
from datasets import load_dataset

# è§£å†³ OpenMP å†²çª
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import models_mae

class AnimeMAEVisualizer:
    def __init__(self, checkpoint_path=None, output_dir=None):
        """åˆå§‹åŒ–åŠ¨æ¼«MAEå¯è§†åŒ–å™¨"""
        
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.output_dir = Path(f"./anime_visualization_{timestamp}")
        else:
            self.output_dir = Path(output_dir)
        
        self.output_dir.mkdir(exist_ok=True)
        print(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {self.output_dir}")
        
        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model(checkpoint_path)
        
        # åŠ è½½æ•°æ®é›†
        self.dataset = self._load_anime_dataset()
        
        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.inv_normalize = transforms.Normalize(
            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
            std=[1/0.229, 1/0.224, 1/0.225]
        )

    def _load_model(self, checkpoint_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("ğŸ¤– åŠ è½½MAEæ¨¡å‹...")
        
        model = models_mae.mae_vit_base_patch16(norm_pix_loss=True)
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
                model.load_state_dict(checkpoint['model'])
                epoch = checkpoint.get('epoch', 'unknown')
                print(f"âœ… åŠ è½½è®­ç»ƒæ¨¡å‹æˆåŠŸ (epoch: {epoch})")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½checkpointå¤±è´¥: {e}")
                print("ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        else:
            print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        
        model.to(self.device)
        model.eval()
        return model

    def _load_anime_dataset(self):
        """åŠ è½½åŠ¨æ¼«æ•°æ®é›†"""
        print("ğŸŒ åŠ è½½åŠ¨æ¼«æ•°æ®é›†...")
        try:
            ds = load_dataset("none-yet/anime-captions")
            print("âœ… åŠ¨æ¼«æ•°æ®é›†åŠ è½½æˆåŠŸ")
            return ds['train']
        except Exception as e:
            print(f"âŒ åŠ¨æ¼«æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            return None

    def visualize_anime_reconstruction(self, num_samples=8, mask_ratios=[0.5, 0.75, 0.9]):
        """å¯è§†åŒ–åŠ¨æ¼«å›¾ç‰‡çš„MAEé‡å»ºæ•ˆæœ"""
        print(f"ğŸ¨ å¯è§†åŒ– {num_samples} å¼ åŠ¨æ¼«å›¾ç‰‡çš„é‡å»ºæ•ˆæœ...")
        
        if self.dataset is None:
            print("âŒ æ•°æ®é›†æœªåŠ è½½")
            return
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        sample_indices = np.random.choice(len(self.dataset), num_samples, replace=False)
        
        for mask_ratio in mask_ratios:
            fig, axes = plt.subplots(3, num_samples, figsize=(num_samples*3, 9))
            
            print(f"\nğŸ­ æ©ç æ¯”ä¾‹: {mask_ratio*100:.0f}%")
            
            for i, idx in enumerate(sample_indices):
                try:
                    # è·å–åŸå§‹å›¾ç‰‡
                    sample = self.dataset[int(idx)]
                    original_img = sample['image']
                    caption = sample['text'][:50] + "..." if len(sample['text']) > 50 else sample['text']
                    
                    # ç¡®ä¿æ˜¯RGBæ¨¡å¼
                    if original_img.mode != 'RGB':
                        original_img = original_img.convert('RGB')
                    
                    # é¢„å¤„ç†
                    img_tensor = self.transform(original_img).unsqueeze(0).to(self.device)
                    
                    # MAE å‰å‘ä¼ æ’­
                    with torch.no_grad():
                        loss, pred, mask = self.model(img_tensor, mask_ratio=mask_ratio)
                        reconstructed = self.model.unpatchify(pred)
                        
                        # åˆ›å»ºæ©ç å¯è§†åŒ–
                        mask_vis = mask.detach()
                        mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, self.model.patch_embed.patch_size[0]**2 * 3)
                        mask_vis = self.model.unpatchify(mask_vis)
                    
                    # è½¬æ¢ä¸ºæ˜¾ç¤ºæ ¼å¼
                    original_display = torch.clamp(self.inv_normalize(img_tensor[0]).cpu(), 0, 1)
                    reconstructed_display = torch.clamp(self.inv_normalize(reconstructed[0]).cpu(), 0, 1)
                    masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
                    
                    # æ˜¾ç¤ºç»“æœ
                    axes[0, i].imshow(original_display.permute(1, 2, 0))
                    axes[0, i].set_title(f'Original {i+1}', fontsize=10)
                    axes[0, i].axis('off')
                    
                    axes[1, i].imshow(masked_img.permute(1, 2, 0))
                    axes[1, i].set_title(f'Masked {i+1}', fontsize=10)
                    axes[1, i].axis('off')
                    
                    axes[2, i].imshow(reconstructed_display.permute(1, 2, 0))
                    axes[2, i].set_title(f'Reconstructed {i+1}\nLoss: {loss.item():.3f}', fontsize=10)
                    axes[2, i].axis('off')
                    
                    print(f"  æ ·æœ¬ {i+1}: æŸå¤± {loss.item():.4f}")
                    
                except Exception as e:
                    print(f"å¤„ç†æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}")
                    for row in range(3):
                        axes[row, i].text(0.5, 0.5, f'Error\n{str(e)[:20]}', 
                                        ha='center', va='center')
                        axes[row, i].axis('off')
            
            plt.tight_layout()
            
            # ä¿å­˜ç»“æœ
            result_path = self.output_dir / f'anime_reconstruction_mask_{mask_ratio*100:.0f}percent.png'
            plt.savefig(result_path, dpi=150, bbox_inches='tight')
            print(f"âœ… ä¿å­˜: {result_path}")
            plt.close()

    def compare_with_synthetic(self):
        """å¯¹æ¯”åŠ¨æ¼«æ•°æ®é›†å’Œåˆæˆæ•°æ®é›†çš„è®­ç»ƒæ•ˆæœ"""
        print(f"\nğŸ“Š å¯¹æ¯”åŠ¨æ¼«æ•°æ®é›†å’Œåˆæˆæ•°æ®é›†çš„è®­ç»ƒæ•ˆæœ...")
        
        # è¯»å–ä¸¤ä¸ªå®éªŒçš„æ—¥å¿—
        anime_log = './output_anime/log.txt'
        synthetic_log = './output_m4/log.txt'
        
        def read_log(log_file):
            if not os.path.exists(log_file):
                return None
            
            epochs, losses = [], []
            with open(log_file, 'r') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        epochs.append(data['epoch'])
                        losses.append(data['train_loss'])
                    except:
                        continue
            return epochs, losses
        
        anime_data = read_log(anime_log)
        synthetic_data = read_log(synthetic_log)
        
        if anime_data is None and synthetic_data is None:
            print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ—¥å¿—")
            return
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æŸå¤±å¯¹æ¯”
        if anime_data:
            epochs_a, losses_a = anime_data
            ax1.plot(epochs_a, losses_a, 'b-', linewidth=2, marker='o', 
                    label=f'Anime Dataset (final: {losses_a[-1]:.3f})', markersize=6)
        
        if synthetic_data:
            epochs_s, losses_s = synthetic_data
            ax1.plot(epochs_s, losses_s, 'r-', linewidth=2, marker='s', 
                    label=f'Synthetic Dataset (final: {losses_s[-1]:.3f})', markersize=6)
        
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Training Loss')
        ax1.set_title('Training Loss Comparison')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ•°æ®é›†å¯¹æ¯”ä¿¡æ¯
        info_text = []
        if anime_data:
            info_text.append(f"ğŸŒ Anime Dataset:")
            info_text.append(f"  â€¢ 337K real anime images")
            info_text.append(f"  â€¢ 512x512 â†’ 224x224")
            info_text.append(f"  â€¢ Final loss: {losses_a[-1]:.4f}")
            info_text.append(f"  â€¢ Epochs: {len(epochs_a)}")
        
        if synthetic_data:
            info_text.append(f"\nğŸ¨ Synthetic Dataset:")
            info_text.append(f"  â€¢ 250 synthetic images")
            info_text.append(f"  â€¢ 224x224 geometric patterns")
            info_text.append(f"  â€¢ Final loss: {losses_s[-1]:.4f}")
            info_text.append(f"  â€¢ Epochs: {len(epochs_s)}")
        
        ax2.text(0.05, 0.95, '\n'.join(info_text), transform=ax2.transAxes, 
                fontsize=11, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        ax2.axis('off')
        ax2.set_title('Dataset Comparison')
        
        plt.tight_layout()
        
        # ä¿å­˜å¯¹æ¯”å›¾
        comparison_path = self.output_dir / 'dataset_comparison.png'
        plt.savefig(comparison_path, dpi=150, bbox_inches='tight')
        print(f"âœ… å¯¹æ¯”å›¾ä¿å­˜: {comparison_path}")
        plt.close()

    def create_anime_mae_demo(self):
        """åˆ›å»ºåŠ¨æ¼«MAEæ¼”ç¤º"""
        print(f"\nğŸ­ åˆ›å»ºåŠ¨æ¼«MAEæ¼”ç¤º...")
        
        if self.dataset is None:
            return
        
        # é€‰æ‹©å‡ å¼ æœ‰ä»£è¡¨æ€§çš„åŠ¨æ¼«å›¾ç‰‡
        demo_indices = [0, 100, 200, 500, 1000, 1500]  # ä¸åŒé£æ ¼çš„å›¾ç‰‡
        
        fig, axes = plt.subplots(len(demo_indices), 4, figsize=(16, len(demo_indices)*4))
        
        for i, idx in enumerate(demo_indices):
            try:
                # è·å–å›¾ç‰‡
                sample = self.dataset[idx]
                original_img = sample['image']
                caption = sample['text']
                
                if original_img.mode != 'RGB':
                    original_img = original_img.convert('RGB')
                
                # é¢„å¤„ç†
                img_tensor = self.transform(original_img).unsqueeze(0).to(self.device)
                
                # MAEå¤„ç†
                with torch.no_grad():
                    loss, pred, mask = self.model(img_tensor, mask_ratio=0.75)
                    reconstructed = self.model.unpatchify(pred)
                    
                    # æ©ç å¯è§†åŒ–
                    mask_vis = mask.detach()
                    mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, self.model.patch_embed.patch_size[0]**2 * 3)
                    mask_vis = self.model.unpatchify(mask_vis)
                
                # æ˜¾ç¤ºæ ¼å¼è½¬æ¢
                original_display = torch.clamp(self.inv_normalize(img_tensor[0]).cpu(), 0, 1)
                reconstructed_display = torch.clamp(self.inv_normalize(reconstructed[0]).cpu(), 0, 1)
                masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
                
                # è®¡ç®—é‡å»ºè¯¯å·®
                error = torch.abs(original_display - reconstructed_display)
                error_display = error.mean(dim=0)
                
                # æ˜¾ç¤º
                axes[i, 0].imshow(original_display.permute(1, 2, 0))
                axes[i, 0].set_title(f'Original Anime {i+1}')
                axes[i, 0].axis('off')
                
                axes[i, 1].imshow(masked_img.permute(1, 2, 0))
                axes[i, 1].set_title('75% Masked')
                axes[i, 1].axis('off')
                
                axes[i, 2].imshow(reconstructed_display.permute(1, 2, 0))
                axes[i, 2].set_title(f'Reconstructed\nLoss: {loss.item():.3f}')
                axes[i, 2].axis('off')
                
                im = axes[i, 3].imshow(error_display, cmap='hot')
                axes[i, 3].set_title('Error Map')
                axes[i, 3].axis('off')
                plt.colorbar(im, ax=axes[i, 3], fraction=0.046, pad=0.04)
                
                print(f"  åŠ¨æ¼«å›¾ç‰‡ {i+1}: æŸå¤± {loss.item():.4f}")
                
            except Exception as e:
                print(f"å¤„ç†åŠ¨æ¼«å›¾ç‰‡ {idx} æ—¶å‡ºé”™: {e}")
                for j in range(4):
                    axes[i, j].text(0.5, 0.5, f'Error: {str(e)[:30]}', 
                                  ha='center', va='center')
                    axes[i, j].axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜æ¼”ç¤º
        demo_path = self.output_dir / 'anime_mae_demo.png'
        plt.savefig(demo_path, dpi=150, bbox_inches='tight')
        print(f"âœ… åŠ¨æ¼«MAEæ¼”ç¤ºä¿å­˜: {demo_path}")
        plt.close()

    def analyze_anime_reconstruction_quality(self):
        """åˆ†æåŠ¨æ¼«å›¾ç‰‡é‡å»ºè´¨é‡"""
        print(f"\nğŸ” åˆ†æåŠ¨æ¼«å›¾ç‰‡é‡å»ºè´¨é‡...")
        
        if self.dataset is None:
            return
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„åŠ¨æ¼«å›¾ç‰‡
        test_indices = np.random.choice(len(self.dataset), 50, replace=False)
        
        reconstruction_stats = []
        
        for idx in test_indices:
            try:
                sample = self.dataset[int(idx)]
                img = sample['image']
                caption = sample['text']
                
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    loss, pred, mask = self.model(img_tensor, mask_ratio=0.75)
                
                reconstruction_stats.append({
                    'index': idx,
                    'loss': loss.item(),
                    'caption_length': len(caption),
                    'caption': caption[:100]
                })
                
            except Exception as e:
                continue
        
        if not reconstruction_stats:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ ·æœ¬")
            return
        
        # åˆ†æç»“æœ
        losses = [stat['loss'] for stat in reconstruction_stats]
        
        print(f"ğŸ“Š é‡å»ºè´¨é‡åˆ†æ ({len(reconstruction_stats)} ä¸ªæ ·æœ¬):")
        print(f"  å¹³å‡æŸå¤±: {np.mean(losses):.4f}")
        print(f"  æŸå¤±æ ‡å‡†å·®: {np.std(losses):.4f}")
        print(f"  æœ€å°æŸå¤±: {np.min(losses):.4f}")
        print(f"  æœ€å¤§æŸå¤±: {np.max(losses):.4f}")
        
        # æ‰¾å‡ºé‡å»ºæœ€å¥½å’Œæœ€å·®çš„å›¾ç‰‡
        best_idx = np.argmin(losses)
        worst_idx = np.argmax(losses)
        
        print(f"\nğŸ† é‡å»ºæœ€ä½³:")
        print(f"  æŸå¤±: {reconstruction_stats[best_idx]['loss']:.4f}")
        print(f"  æè¿°: {reconstruction_stats[best_idx]['caption']}")
        
        print(f"\nğŸ˜… é‡å»ºæœ€å·®:")
        print(f"  æŸå¤±: {reconstruction_stats[worst_idx]['loss']:.4f}")
        print(f"  æè¿°: {reconstruction_stats[worst_idx]['caption']}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = self.output_dir / 'anime_reconstruction_analysis.json'
        with open(stats_path, 'w') as f:
            json.dump({
                'summary': {
                    'num_samples': len(reconstruction_stats),
                    'mean_loss': float(np.mean(losses)),
                    'std_loss': float(np.std(losses)),
                    'min_loss': float(np.min(losses)),
                    'max_loss': float(np.max(losses))
                },
                'best_sample': reconstruction_stats[best_idx],
                'worst_sample': reconstruction_stats[worst_idx],
                'all_samples': reconstruction_stats
            }, f, indent=2)
        
        print(f"âœ… åˆ†æç»“æœä¿å­˜: {stats_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ åŠ¨æ¼«æ•°æ®é›† MAE ç»“æœå¯è§†åŒ–")
    print("=" * 60)
    
    # æŸ¥æ‰¾æœ€æ–°çš„checkpoint
    checkpoint_path = './output_anime/checkpoint-2.pth'  # æœ€åä¸€ä¸ªepoch
    
    if not os.path.exists(checkpoint_path):
        print(f"âš ï¸  æœªæ‰¾åˆ°checkpoint: {checkpoint_path}")
        print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è¿›è¡Œæ¼”ç¤º")
        checkpoint_path = None
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = AnimeMAEVisualizer(checkpoint_path=checkpoint_path)
    
    # 1. å¯è§†åŒ–é‡å»ºæ•ˆæœ
    visualizer.visualize_anime_reconstruction(num_samples=6, mask_ratios=[0.75])
    
    # 2. åˆ›å»ºæ¼”ç¤º
    visualizer.create_anime_mae_demo()
    
    # 3. åˆ†æé‡å»ºè´¨é‡
    visualizer.analyze_anime_reconstruction_quality()
    
    # 4. å¯¹æ¯”ä¸åŒæ•°æ®é›†
    visualizer.compare_with_synthetic()
    
    print(f"\nğŸ‰ åŠ¨æ¼«æ•°æ®é›†å¯è§†åŒ–å®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {visualizer.output_dir}")

if __name__ == "__main__":
    main()


