#!/usr/bin/env python3
"""
ä½¿ç”¨AnimeDiffusionæ•°æ®é›†çš„æœ€å®Œæ•´MAEæ¼”ç¤º
å±•ç¤ºç¼–ç å™¨vsè§£ç å™¨ï¼Œå›ç­”ï¼šåªç”¨ç¼–ç å™¨èƒ½å¦é‡å»ºå›¾åƒï¼Ÿ
"""

import os
import sys
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torchvision.transforms as transforms
import models_mae
from animediffusion_dataset_loader import create_animediffusion_dataloader
import random

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_mae_model():
    """åŠ è½½MAEæ¨¡å‹"""
    print("\nğŸ¤– åŠ è½½MAEæ¨¡å‹...")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print(f"âœ… ä½¿ç”¨ Apple Silicon MPS")
    else:
        device = torch.device('cpu')
        print(f"âœ… ä½¿ç”¨ CPU")
    
    # åˆ›å»ºæ¨¡å‹
    model = models_mae.mae_vit_base_patch16()
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆåªæœ‰ç¼–ç å™¨éƒ¨åˆ†ï¼‰
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    if os.path.exists(pretrain_path):
        print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrain_path}")
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        
        # åªåŠ è½½ç¼–ç å™¨æƒé‡
        encoder_state_dict = {}
        decoder_missing_keys = []
        
        for key, value in checkpoint['model'].items():
            if not key.startswith('decoder') and key != 'mask_token':
                encoder_state_dict[key] = value
            else:
                decoder_missing_keys.append(key)
        
        model.load_state_dict(encoder_state_dict, strict=False)
        print("âœ… ç¼–ç å™¨æƒé‡åŠ è½½æˆåŠŸ")
        print(f"âš ï¸  è§£ç å™¨æƒé‡ç¼ºå¤±: {len(decoder_missing_keys)} ä¸ªå‚æ•°")
    else:
        print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
    
    model = model.to(device)
    model.eval()
    
    return model, device

def load_animediffusion_samples():
    """ä½¿ç”¨AnimeDiffusionæ•°æ®é›†åŠ è½½å™¨è·å–æ ·æœ¬"""
    print("\nğŸŒ ä½¿ç”¨AnimeDiffusionæ•°æ®é›†åŠ è½½å™¨...")
    
    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ŒåªåŠ è½½å°‘é‡æ ·æœ¬ç”¨äºæ¼”ç¤º
        dataloader, dataset = create_animediffusion_dataloader(
            batch_size=6,  # ä¸€æ¬¡åŠ è½½6å¼ å›¾ç‰‡
            max_samples=50,  # åªä»å‰50å¼ ä¸­é€‰æ‹©
            input_size=224,
            num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        if dataloader is None:
            print("âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥")
            return None
        
        # è·å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        for images, _ in dataloader:
            print(f"âœ… æˆåŠŸåŠ è½½ {images.shape[0]} å¼ AnimeDiffusionå›¾ç‰‡")
            print(f"   å›¾åƒå½¢çŠ¶: {images.shape}")
            print(f"   æ•°æ®èŒƒå›´: [{images.min():.3f}, {images.max():.3f}]")
            return images[:3]  # åªå–å‰3å¼ ç”¨äºæ¼”ç¤º
            
    except Exception as e:
        print(f"âŒ AnimeDiffusionæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        print("   å¯èƒ½éœ€è¦ç½‘ç»œè¿æ¥æˆ–HuggingFaceè´¦æˆ·")
        return None

def create_fallback_anime_samples():
    """åˆ›å»ºå¤‡ç”¨çš„åŠ¨æ¼«é£æ ¼æ ·æœ¬"""
    print("ğŸ¨ åˆ›å»ºå¤‡ç”¨åŠ¨æ¼«é£æ ¼æ ·æœ¬...")
    
    # åˆ›å»ºæ ‡å‡†åŒ–çš„tensor
    samples = []
    
    # æ ·æœ¬1ï¼šåŠ¨æ¼«äººç‰©è„¸éƒ¨
    img1 = torch.zeros(3, 224, 224)
    
    # è„¸éƒ¨è½®å»“
    y, x = torch.meshgrid(torch.arange(224), torch.arange(224), indexing='ij')
    face_mask = ((x - 112)/60)**2 + ((y - 112)/80)**2 <= 1
    
    # è‚¤è‰² (å½’ä¸€åŒ–åçš„å€¼)
    img1[0][face_mask] = (1.0 - 0.485) / 0.229  # R
    img1[1][face_mask] = (0.9 - 0.456) / 0.224  # G  
    img1[2][face_mask] = (0.8 - 0.406) / 0.225  # B
    
    # çœ¼ç›
    eye1_mask = ((x - 90)/8)**2 + ((y - 90)/12)**2 <= 1
    eye2_mask = ((x - 134)/8)**2 + ((y - 90)/12)**2 <= 1
    for c in range(3):
        img1[c][eye1_mask] = (0.1 - [0.485, 0.456, 0.406][c]) / [0.229, 0.224, 0.225][c]
        img1[c][eye2_mask] = (0.1 - [0.485, 0.456, 0.406][c]) / [0.229, 0.224, 0.225][c]
    
    samples.append(img1.unsqueeze(0))
    
    # æ ·æœ¬2ï¼šå½©è‰²æ¸å˜
    img2 = torch.zeros(3, 224, 224)
    for i in range(224):
        for j in range(224):
            r = (i / 224 - 0.485) / 0.229
            g = (j / 224 - 0.456) / 0.224  
            b = (0.8 - 0.406) / 0.225
            img2[0, i, j] = r
            img2[1, i, j] = g
            img2[2, i, j] = b
    
    samples.append(img2.unsqueeze(0))
    
    # æ ·æœ¬3ï¼šå‡ ä½•å›¾æ¡ˆ
    img3 = torch.zeros(3, 224, 224)
    for i in range(0, 224, 28):
        for j in range(0, 224, 28):
            color_r = (i/224 - 0.485) / 0.229
            color_g = (j/224 - 0.456) / 0.224
            color_b = (0.7 - 0.406) / 0.225
            img3[0, i:i+28, j:j+28] = color_r
            img3[1, i:i+28, j:j+28] = color_g  
            img3[2, i:i+28, j:j+28] = color_b
    
    samples.append(img3.unsqueeze(0))
    
    # åˆå¹¶æ‰€æœ‰æ ·æœ¬
    return torch.cat(samples, dim=0)

def demonstrate_complete_mae_analysis(model, device, images):
    """å®Œæ•´çš„MAEåˆ†ææ¼”ç¤º"""
    print("\nğŸ” å®Œæ•´MAEåˆ†æï¼šç¼–ç å™¨ vs è§£ç å™¨")
    
    # åå½’ä¸€åŒ–ç”¨äºæ˜¾ç¤º
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    num_images = min(3, images.shape[0])
    fig, axes = plt.subplots(6, num_images, figsize=(num_images*4, 24))
    
    if num_images == 1:
        axes = axes.reshape(-1, 1)
    
    images = images[:num_images].to(device)
    
    for i in range(num_images):
        img_tensor = images[i:i+1]
        
        print(f"\n  åˆ†æå›¾åƒ {i+1}...")
        
        with torch.no_grad():
            # 1. åŸå§‹å›¾åƒæ˜¾ç¤º
            original_display = torch.clamp(inv_normalize(img_tensor[0]).cpu(), 0, 1)
            axes[0, i].imshow(original_display.permute(1, 2, 0))
            axes[0, i].set_title(f'AnimeDiffusionå›¾åƒ {i+1}\nåŸå§‹é«˜è´¨é‡åŠ¨æ¼«å›¾ç‰‡')
            axes[0, i].axis('off')
            
            # 2. å›¾åƒåˆ†å—è¿‡ç¨‹
            print("    ğŸ“¦ å›¾åƒåˆ†å—...")
            patches = model.patchify(img_tensor)  # (1, 196, 768)
            
            # å¯è§†åŒ–å‰16ä¸ªpatches
            patch_display = torch.zeros(4*16, 4*16, 3)
            for p in range(16):
                if p < patches.shape[1]:
                    patch_data = patches[0, p].cpu().reshape(16, 16, 3)
                    # åå½’ä¸€åŒ–
                    patch_data = patch_data * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])
                    patch_data = torch.clamp(patch_data, 0, 1)
                    
                    row, col = p // 4, p % 4
                    patch_display[row*16:(row+1)*16, col*16:(col+1)*16] = patch_data
            
            axes[1, i].imshow(patch_display)
            axes[1, i].set_title(f'å›¾åƒåˆ†å—\nå‰16ä¸ª16Ã—16 patches')
            axes[1, i].axis('off')
            
            # 3. ç¼–ç å™¨ç‰¹å¾æå–
            print("    ğŸ§  ç¼–ç å™¨ç‰¹å¾æå–...")
            x = model.patch_embed(img_tensor)
            x = x + model.pos_embed[:, 1:, :]
            
            # æ·»åŠ cls token
            cls_token = model.cls_token + model.pos_embed[:, :1, :]
            cls_tokens = cls_token.expand(x.shape[0], -1, -1)
            x = torch.cat((cls_tokens, x), dim=1)
            
            # é€šè¿‡ç¼–ç å™¨
            for blk in model.blocks:
                x = blk(x)
            encoded_features = model.norm(x)
            
            # å¯è§†åŒ–ç¼–ç å™¨ç‰¹å¾
            patch_features = encoded_features[:, 1:, :].cpu().numpy()
            feature_map = patch_features[0].mean(axis=1).reshape(14, 14)
            
            im1 = axes[2, i].imshow(feature_map, cmap='viridis')
            axes[2, i].set_title('ç¼–ç å™¨è¾“å‡ºç‰¹å¾\n768ç»´æŠ½è±¡è¯­ä¹‰è¡¨ç¤º')
            axes[2, i].axis('off')
            plt.colorbar(im1, ax=axes[2, i], fraction=0.046, pad=0.04)
            
            # 4. æ©ç æ¨¡æ‹Ÿ
            print("    ğŸ­ æ¨¡æ‹ŸMAEæ©ç è¿‡ç¨‹...")
            mask_ratio = 0.75
            N, L, D = encoded_features.shape
            len_keep = int(L * (1 - mask_ratio))
            
            # åˆ›å»ºéšæœºæ©ç 
            noise = torch.rand(N, L-1, device=device)  # ä¸åŒ…æ‹¬cls token
            ids_shuffle = torch.argsort(noise, dim=1)
            ids_restore = torch.argsort(ids_shuffle, dim=1)
            
            # åˆ›å»ºæ©ç 
            mask = torch.ones([N, L-1], device=device)
            mask[:, :len_keep] = 0
            mask = torch.gather(mask, dim=1, index=ids_restore)
            
            # å¯è§†åŒ–æ©ç 
            mask_vis = mask[0].cpu().numpy().reshape(14, 14)
            
            axes[3, i].imshow(mask_vis, cmap='RdYlBu_r', vmin=0, vmax=1)
            axes[3, i].set_title(f'MAEæ©ç æ¨¡å¼\n{mask_ratio*100:.0f}% patchesè¢«æ©ç›–')
            axes[3, i].axis('off')
            
            # 5. ç¼–ç å™¨é™åˆ¶è¯´æ˜
            axes[4, i].text(0.5, 0.8, 'âŒ ç¼–ç å™¨æ— æ³•é‡å»º', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=14, color='red', weight='bold')
            axes[4, i].text(0.5, 0.65, 'ç¼–ç å™¨è¾“å‡º:', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=12)
            axes[4, i].text(0.5, 0.55, '768ç»´è¯­ä¹‰ç‰¹å¾', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=11)
            axes[4, i].text(0.5, 0.4, 'â‰ ', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=20, color='red')
            axes[4, i].text(0.5, 0.25, 'éœ€è¦çš„è¾“å‡º:', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=12)
            axes[4, i].text(0.5, 0.15, '768ç»´åƒç´ å€¼', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=11)
            axes[4, i].set_xlim(0, 1)
            axes[4, i].set_ylim(0, 1)
            axes[4, i].axis('off')
            axes[4, i].set_title('ç¼–ç å™¨çš„é™åˆ¶')
            
            # 6. å®Œæ•´MAEéœ€è¦è§£ç å™¨
            axes[5, i].text(0.5, 0.9, 'âœ… å®Œæ•´MAEæ¶æ„', ha='center', va='center',
                           transform=axes[5, i].transAxes, fontsize=14, color='green', weight='bold')
            axes[5, i].text(0.5, 0.75, 'ç¼–ç å™¨: ç†è§£å›¾åƒ', ha='center', va='center',
                           transform=axes[5, i].transAxes, fontsize=11)
            axes[5, i].text(0.5, 0.65, 'â†“', ha='center', va='center',
                           transform=axes[5, i].transAxes, fontsize=16)
            axes[5, i].text(0.5, 0.55, 'æŠ½è±¡ç‰¹å¾', ha='center', va='center',
                           transform=axes[5, i].transAxes, fontsize=11)
            axes[5, i].text(0.5, 0.45, 'â†“', ha='center', va='center',
                           transform=axes[5, i].transAxes, fontsize=16)
            axes[5, i].text(0.5, 0.35, 'è§£ç å™¨: ç”Ÿæˆåƒç´ ', ha='center', va='center',
                           transform=axes[5, i].transAxes, fontsize=11)
            axes[5, i].text(0.5, 0.25, 'â†“', ha='center', va='center',
                           transform=axes[5, i].transAxes, fontsize=16)
            axes[5, i].text(0.5, 0.15, 'é‡å»ºå›¾åƒ', ha='center', va='center',
                           transform=axes[5, i].transAxes, fontsize=11, color='green')
            axes[5, i].set_xlim(0, 1)
            axes[5, i].set_ylim(0, 1)
            axes[5, i].axis('off')
            axes[5, i].set_title('éœ€è¦å®Œæ•´æ¶æ„')
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print(f"      ç¼–ç å™¨ç‰¹å¾ç»Ÿè®¡: å‡å€¼={encoded_features.mean().item():.4f}, æ ‡å‡†å·®={encoded_features.std().item():.4f}")
            print(f"      ç‰¹å¾ç»´åº¦: {encoded_features.shape}")
    
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    output_path = 'complete_animediffusion_mae_analysis.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… å®Œæ•´åˆ†æç»“æœå·²ä¿å­˜: {output_path}")
    
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return output_path

def explain_complete_mae():
    """å®Œæ•´è§£é‡ŠMAEåŸç†"""
    print("\nğŸ“š MAEå®Œæ•´åŸç†è§£æ:")
    print("=" * 60)
    
    print("ğŸŒ AnimeDiffusionæ•°æ®é›†ç‰¹ç‚¹:")
    print("   â€¢ é«˜è´¨é‡åŠ¨æ¼«å›¾ç‰‡ (1920Ã—1080)")
    print("   â€¢ ä¸°å¯Œçš„è§†è§‰ç»†èŠ‚å’Œè‰²å½©")
    print("   â€¢ å¤æ‚çš„çº¹ç†å’Œç»“æ„")
    print("   â€¢ é€‚åˆæµ‹è¯•é‡å»ºèƒ½åŠ›")
    print()
    
    print("ğŸ” MAEç¼–ç å™¨åˆ†æ:")
    print("   è¾“å…¥: 25%å¯è§patches (49/196ä¸ª)")
    print("   å¤„ç†: 12å±‚Transformerç¼–ç å™¨")
    print("   è¾“å‡º: 768ç»´æŠ½è±¡ç‰¹å¾å‘é‡")
    print("   åŠŸèƒ½: ç†è§£å›¾åƒè¯­ä¹‰å†…å®¹")
    print("   âŒ æ— æ³•ç›´æ¥ç”Ÿæˆåƒç´ !")
    print()
    
    print("ğŸ¨ MAEè§£ç å™¨ä½œç”¨:")
    print("   è¾“å…¥: ç¼–ç å™¨ç‰¹å¾ + 147ä¸ªmask tokens")
    print("   å¤„ç†: 8å±‚è½»é‡çº§Transformer")
    print("   è¾“å‡º: 196ä¸ª768ç»´åƒç´ é¢„æµ‹")
    print("   åŠŸèƒ½: ä»ç‰¹å¾é‡å»ºå…·ä½“åƒç´ ")
    print("   âœ… è´Ÿè´£åƒç´ çº§é‡å»º!")
    print()
    
    print("ğŸ¯ å…³é”®æŠ€æœ¯æ´å¯Ÿ:")
    print("   1. ç¼–ç å™¨å­¦ä¹ è¯­ä¹‰è¡¨ç¤º (what)")
    print("   2. è§£ç å™¨å­¦ä¹ åƒç´ ç”Ÿæˆ (how)")
    print("   3. 75%æ©ç ç‡å¼ºè¿«æ¨¡å‹ç†è§£å…¨å±€ç»“æ„")
    print("   4. é¢„è®­ç»ƒå­¦åˆ°çš„ç‰¹å¾å¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡")
    print()
    
    print("ğŸ’¡ å®é™…åº”ç”¨ä»·å€¼:")
    print("   â€¢ ç¼–ç å™¨: ç‰¹å¾æå–ã€åˆ†ç±»ã€æ£€ç´¢")
    print("   â€¢ å®Œæ•´MAE: å›¾åƒä¿®å¤ã€å»å™ªã€ç¼–è¾‘")
    print("   â€¢ é¢„è®­ç»ƒæƒé‡: æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ AnimeDiffusionæ•°æ®é›† - æœ€å®Œæ•´MAEæ¼”ç¤º")
    print("=" * 60)
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    random.seed(42)
    
    # åŠ è½½MAEæ¨¡å‹
    model, device = load_mae_model()
    
    # å°è¯•åŠ è½½AnimeDiffusionæ•°æ®é›†
    print("\nğŸ¯ å°è¯•åŠ è½½çœŸå®AnimeDiffusionæ•°æ®é›†...")
    images = load_animediffusion_samples()
    
    if images is None:
        print("\nğŸ¨ ä½¿ç”¨å¤‡ç”¨åŠ¨æ¼«é£æ ¼æ ·æœ¬...")
        images = create_fallback_anime_samples()
    
    # å®Œæ•´æ¼”ç¤ºåˆ†æ
    output_path = demonstrate_complete_mae_analysis(model, device, images)
    
    # å®Œæ•´åŸç†è§£é‡Š
    explain_complete_mae()
    
    print(f"\nğŸ‰ æœ€å®Œæ•´æ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ ç»“æœå›¾åƒ: {output_path}")
    print("\nğŸ¯ æœ€ç»ˆç»“è®º:")
    print("âŒ ç¼–ç å™¨åªèƒ½æå–æŠ½è±¡è¯­ä¹‰ç‰¹å¾")
    print("ğŸ¨ è§£ç å™¨æ‰èƒ½ä»ç‰¹å¾ç”Ÿæˆåƒç´ ")
    print("ğŸ”„ å®Œæ•´MAE = ç¼–ç å™¨(ç†è§£) + è§£ç å™¨(é‡å»º)")
    print("ğŸ’¡ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåªæœ‰ç¼–ç å™¨æ— æ³•é‡å»ºAnimeDiffusionå›¾åƒ!")

if __name__ == "__main__":
    main()

