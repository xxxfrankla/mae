#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„ AnimeDiffusion æ•°æ®é›†åŠ è½½å™¨
å°†é«˜åˆ†è¾¨ç‡å›¾åƒé¢„å¤„ç†åˆ°åˆé€‚å°ºå¯¸
"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from datasets import load_dataset
from PIL import Image
import numpy as np

class OptimizedAnimeDiffusionDataset(Dataset):
    """ä¼˜åŒ–çš„ AnimeDiffusion æ•°æ®é›†åŒ…è£…å™¨"""
    
    def __init__(self, hf_dataset, transform=None, max_samples=None, target_size=224):
        """
        Args:
            hf_dataset: HuggingFace æ•°æ®é›†
            transform: å›¾åƒå˜æ¢
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸
        """
        self.dataset = hf_dataset
        self.transform = transform
        self.target_size = target_size
        
        if max_samples is not None:
            self.length = min(max_samples, len(hf_dataset))
        else:
            self.length = len(hf_dataset)
        
        print(f"ğŸ“Š ä¼˜åŒ–çš„ AnimeDiffusion æ•°æ®é›†:")
        print(f"  åŸå§‹åˆ†è¾¨ç‡: 1920Ã—1080")
        print(f"  ç›®æ ‡åˆ†è¾¨ç‡: {target_size}Ã—{target_size}")
        print(f"  æ ·æœ¬æ•°é‡: {self.length}")
        print(f"  åˆ†è¾¨ç‡ç¼©æ”¾: {1920/target_size:.1f}x ç¼©å°")

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        try:
            sample = self.dataset[idx]
            
            # è·å–å›¾åƒ
            image = sample['image']
            
            # ç¡®ä¿æ˜¯PILå›¾åƒ
            if not isinstance(image, Image.Image):
                if isinstance(image, np.ndarray):
                    image = Image.fromarray(image)
                else:
                    image = Image.new('RGB', (1920, 1080), color='black')
            
            # å¤„ç†RGBAå›¾åƒ
            if image.mode == 'RGBA':
                # åˆ›å»ºç™½è‰²èƒŒæ™¯
                background = Image.new('RGB', image.size, (255, 255, 255))
                background.paste(image, mask=image.split()[-1])
                image = background
            elif image.mode != 'RGB':
                image = image.convert('RGB')
            
            # åº”ç”¨å˜æ¢
            if self.transform:
                image = self.transform(image)
            
            return image, 0
            
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤å›¾åƒ
            default_img = Image.new('RGB', (self.target_size, self.target_size), color='black')
            if self.transform:
                default_img = self.transform(default_img)
            return default_img, 0

def create_optimized_animediffusion_dataloader(batch_size=8, num_workers=4, max_samples=None, 
                                             input_size=224, resize_strategy='smart_crop'):
    """
    åˆ›å»ºä¼˜åŒ–çš„ AnimeDiffusion æ•°æ®åŠ è½½å™¨
    
    Args:
        resize_strategy: 'smart_crop', 'center_crop', 'resize_only'
    """
    
    print(f"ğŸŒ åˆ›å»ºä¼˜åŒ–çš„ AnimeDiffusion æ•°æ®é›†åŠ è½½å™¨...")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
    print(f"  ç¼©æ”¾ç­–ç•¥: {resize_strategy}")
    if max_samples:
        print(f"  æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    
    # åŠ è½½æ•°æ®é›†
    try:
        ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
        train_dataset = ds['train']
        print(f"âœ… AnimeDiffusion æ•°æ®é›†åŠ è½½æˆåŠŸ ({len(train_dataset)} å¼ å›¾ç‰‡)")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    # æ ¹æ®ç­–ç•¥å®šä¹‰ä¸åŒçš„å˜æ¢
    if resize_strategy == 'smart_crop':
        # æ™ºèƒ½è£å‰ªï¼šä¿æŒå®½é«˜æ¯”ï¼Œç„¶åéšæœºè£å‰ª
        transform = transforms.Compose([
            transforms.Resize(int(input_size * 1.2), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.RandomCrop(input_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    elif resize_strategy == 'center_crop':
        # ä¸­å¿ƒè£å‰ªï¼šå…ˆresizeå†ä¸­å¿ƒè£å‰ª
        transform = transforms.Compose([
            transforms.Resize(int(input_size * 1.1), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(input_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    else:  # resize_only
        # ç›´æ¥ç¼©æ”¾ï¼šå¯èƒ½ä¼šå˜å½¢ä½†ä¿æŒå®Œæ•´å†…å®¹
        transform = transforms.Compose([
            transforms.Resize((input_size, input_size), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    # åˆ›å»ºæ•°æ®é›†
    anime_dataset = OptimizedAnimeDiffusionDataset(
        train_dataset, 
        transform=transform, 
        max_samples=max_samples,
        target_size=input_size
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        anime_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=False,
        drop_last=True
    )
    
    print(f"âœ… ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  æ€»æ‰¹æ¬¡æ•°: {len(dataloader)}")
    
    return dataloader, anime_dataset

def compare_resize_strategies():
    """å¯¹æ¯”ä¸åŒçš„ç¼©æ”¾ç­–ç•¥"""
    print("ğŸ” å¯¹æ¯”ä¸åŒçš„å›¾åƒç¼©æ”¾ç­–ç•¥...")
    
    strategies = ['smart_crop', 'center_crop', 'resize_only']
    
    fig, axes = plt.subplots(len(strategies), 3, figsize=(12, len(strategies)*4))
    
    # åŠ è½½ä¸€å¼ æµ‹è¯•å›¾ç‰‡
    try:
        ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
        sample = ds['train'][0]
        original_img = sample['image']
        
        if original_img.mode != 'RGB':
            original_img = original_img.convert('RGB')
        
        print(f"åŸå§‹å›¾åƒå°ºå¯¸: {original_img.size}")
        
        for i, strategy in enumerate(strategies):
            # åˆ›å»ºå¯¹åº”çš„å˜æ¢
            if strategy == 'smart_crop':
                transform = transforms.Compose([
                    transforms.Resize(int(224 * 1.2), interpolation=transforms.InterpolationMode.BICUBIC),
                    transforms.CenterCrop(224),  # ç”¨ä¸­å¿ƒè£å‰ªä»£æ›¿éšæœºè£å‰ªç”¨äºæ¼”ç¤º
                    transforms.ToTensor()
                ])
            elif strategy == 'center_crop':
                transform = transforms.Compose([
                    transforms.Resize(int(224 * 1.1), interpolation=transforms.InterpolationMode.BICUBIC),
                    transforms.CenterCrop(224),
                    transforms.ToTensor()
                ])
            else:  # resize_only
                transform = transforms.Compose([
                    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),
                    transforms.ToTensor()
                ])
            
            # åº”ç”¨å˜æ¢
            processed_img = transform(original_img)
            
            # æ˜¾ç¤ºåŸå›¾ï¼ˆåªåœ¨ç¬¬ä¸€è¡Œæ˜¾ç¤ºï¼‰
            if i == 0:
                # ç¼©å°åŸå›¾ç”¨äºæ˜¾ç¤º
                display_original = original_img.resize((224, 126))  # ä¿æŒ16:9æ¯”ä¾‹
                axes[i, 0].imshow(display_original)
                axes[i, 0].set_title('Original\n1920Ã—1080')
                axes[i, 0].axis('off')
            else:
                axes[i, 0].axis('off')
            
            # æ˜¾ç¤ºå¤„ç†åçš„å›¾åƒ
            axes[i, 1].imshow(processed_img.permute(1, 2, 0))
            axes[i, 1].set_title(f'{strategy}\n224Ã—224')
            axes[i, 1].axis('off')
            
            # æ˜¾ç¤ºç­–ç•¥è¯´æ˜
            if strategy == 'smart_crop':
                description = "å…ˆç¼©æ”¾åˆ°1.2å€\nç„¶åéšæœº/ä¸­å¿ƒè£å‰ª\nä¿æŒç»†èŠ‚ï¼Œå¯èƒ½ä¸¢å¤±è¾¹ç¼˜"
            elif strategy == 'center_crop':
                description = "å…ˆç¼©æ”¾åˆ°1.1å€\nç„¶åä¸­å¿ƒè£å‰ª\nä¿æŒä¸­å¿ƒå†…å®¹"
            else:
                description = "ç›´æ¥ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸\nå¯èƒ½å˜å½¢ä½†ä¿æŒå®Œæ•´å†…å®¹"
            
            axes[i, 2].text(0.1, 0.5, description, transform=axes[i, 2].transAxes,
                          fontsize=10, verticalalignment='center',
                          bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
            axes[i, 2].axis('off')
            axes[i, 2].set_title(f'{strategy} è¯´æ˜')
        
        plt.tight_layout()
        plt.savefig('resize_strategy_comparison.png', dpi=150, bbox_inches='tight')
        print("âœ… ç¼©æ”¾ç­–ç•¥å¯¹æ¯”ä¿å­˜: resize_strategy_comparison.png")
        plt.close()
        
    except Exception as e:
        print(f"å¯¹æ¯”å¤±è´¥: {e}")

def test_memory_usage():
    """æµ‹è¯•ä¸åŒåˆ†è¾¨ç‡çš„å†…å­˜ä½¿ç”¨"""
    print("\nğŸ’¾ æµ‹è¯•ä¸åŒåˆ†è¾¨ç‡çš„å†…å­˜ä½¿ç”¨...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # æµ‹è¯•ä¸åŒçš„è¾“å…¥å°ºå¯¸
    input_sizes = [224, 256, 288, 320]
    batch_sizes = [8, 6, 4, 2]
    
    import models_mae
    
    for input_size, batch_size in zip(input_sizes, batch_sizes):
        try:
            print(f"\nğŸ§ª æµ‹è¯• {input_size}Ã—{input_size}, batch_size={batch_size}")
            
            # åˆ›å»ºæ¨¡å‹
            model = models_mae.mae_vit_base_patch16()
            model.to(device)
            model.eval()
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            x = torch.randn(batch_size, 3, input_size, input_size, device=device)
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            with torch.no_grad():
                loss, pred, mask = model(x, mask_ratio=0.75)
            
            print(f"  âœ… æˆåŠŸ: æŸå¤± {loss.item():.4f}")
            
            # æ¸…ç†å†…å­˜
            del model, x, loss, pred, mask
            if device.type == 'mps':
                torch.mps.empty_cache()
                
        except Exception as e:
            print(f"  âŒ å¤±è´¥: {e}")
            # æ¸…ç†å†…å­˜
            if device.type == 'mps':
                torch.mps.empty_cache()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ AnimeDiffusion åˆ†è¾¨ç‡ä¼˜åŒ–åˆ†æ")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    # 1. æµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_animediffusion_dataloader()
    
    # 2. å¯¹æ¯”ç¼©æ”¾ç­–ç•¥
    try:
        import matplotlib.pyplot as plt
        compare_resize_strategies()
    except Exception as e:
        print(f"ç¼©æ”¾ç­–ç•¥å¯¹æ¯”å¤±è´¥: {e}")
    
    # 3. æµ‹è¯•å†…å­˜ä½¿ç”¨
    test_memory_usage()
    
    print(f"\nğŸ’¡ å»ºè®®:")
    print(f"  - æ¨èè¾“å…¥å°ºå¯¸: 224Ã—224 (æ ‡å‡†)")
    print(f"  - å¯å°è¯•: 256Ã—256 (æ›´å¤šç»†èŠ‚)")
    print(f"  - é¿å…: >320Ã—320 (å†…å­˜ä¸è¶³)")
    print(f"  - æ¨èç­–ç•¥: smart_crop (ä¿æŒç»†èŠ‚)")

if __name__ == "__main__":
    main()


