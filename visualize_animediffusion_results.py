#!/usr/bin/env python3
"""
AnimeDiffusion è®­ç»ƒç»“æœä¸“ç”¨å¯è§†åŒ–å·¥å…·
å±•ç¤ºé«˜è´¨é‡åŠ¨æ¼«å›¾ç‰‡çš„MAEé‡å»ºæ•ˆæœ
"""

import os
import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import json
from datetime import datetime
from pathlib import Path
from datasets import load_dataset

# è§£å†³ OpenMP å†²çª
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import models_mae

def create_animediffusion_visualization():
    """åˆ›å»ºAnimeDiffusionä¸“ç”¨å¯è§†åŒ–"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(f"./animediffusion_visualization_{timestamp}")
    output_dir.mkdir(exist_ok=True)
    
    print(f"ğŸŒ AnimeDiffusion ç»“æœå¯è§†åŒ–")
    print(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {output_dir}")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    checkpoint_path = './output_animediffusion/checkpoint-4.pth'
    model = models_mae.mae_vit_base_patch16(norm_pix_loss=True)
    
    if os.path.exists(checkpoint_path):
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            model.load_state_dict(checkpoint['model'])
            print(f"âœ… åŠ è½½è®­ç»ƒæ¨¡å‹æˆåŠŸ (epoch: {checkpoint.get('epoch', 'unknown')})")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½checkpointå¤±è´¥: {e}, ä½¿ç”¨éšæœºæ¨¡å‹")
    else:
        print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    model.to(device)
    model.eval()
    
    # åŠ è½½AnimeDiffusionæ•°æ®é›†
    try:
        ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
        dataset = ds['train']
        print(f"âœ… AnimeDiffusion æ•°æ®é›†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return
    
    # å›¾åƒå˜æ¢
    transform = transforms.Compose([
        transforms.Resize(int(224 * 1.15), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),  # ç”¨ä¸­å¿ƒè£å‰ªç¡®ä¿ä¸€è‡´æ€§
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    # 1. åˆ›å»ºé«˜è´¨é‡é‡å»ºæ¼”ç¤º
    print(f"\nğŸ¨ åˆ›å»ºé«˜è´¨é‡é‡å»ºæ¼”ç¤º...")
    
    # é€‰æ‹©ä¸åŒé£æ ¼çš„åŠ¨æ¼«å›¾ç‰‡
    demo_indices = [0, 100, 500, 1000, 2000, 3000]
    
    fig, axes = plt.subplots(len(demo_indices), 4, figsize=(16, len(demo_indices)*3))
    
    reconstruction_losses = []
    
    for i, idx in enumerate(demo_indices):
        try:
            sample = dataset[idx]
            original_img = sample['image']
            
            # è·å–æç¤ºè¯
            long_prompt = sample.get('long_prompt', '')
            short_prompt = sample.get('short_prompt', '')
            prompt = long_prompt if long_prompt else short_prompt
            if len(prompt) > 60:
                prompt = prompt[:57] + "..."
            
            if original_img.mode != 'RGB':
                original_img = original_img.convert('RGB')
            
            # é¢„å¤„ç†
            img_tensor = transform(original_img).unsqueeze(0).to(device)
            
            # MAEå¤„ç†
            with torch.no_grad():
                loss, pred, mask = model(img_tensor, mask_ratio=0.75)
                reconstructed = model.unpatchify(pred)
                
                # æ©ç å¯è§†åŒ–
                mask_vis = mask.detach()
                mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
                mask_vis = model.unpatchify(mask_vis)
            
            # æ˜¾ç¤ºæ ¼å¼è½¬æ¢
            original_display = torch.clamp(inv_normalize(img_tensor[0]).cpu(), 0, 1)
            reconstructed_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
            masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
            
            # è®¡ç®—é‡å»ºè¯¯å·®
            error = torch.abs(original_display - reconstructed_display)
            error_display = error.mean(dim=0)
            
            # æ˜¾ç¤º
            axes[i, 0].imshow(original_display.permute(1, 2, 0))
            axes[i, 0].set_title(f'Original HD Anime {i+1}')
            axes[i, 0].axis('off')
            
            axes[i, 1].imshow(masked_img.permute(1, 2, 0))
            axes[i, 1].set_title('75% Masked')
            axes[i, 1].axis('off')
            
            axes[i, 2].imshow(reconstructed_display.permute(1, 2, 0))
            axes[i, 2].set_title(f'Reconstructed\nLoss: {loss.item():.3f}')
            axes[i, 2].axis('off')
            
            im = axes[i, 3].imshow(error_display, cmap='hot', vmin=0, vmax=0.5)
            axes[i, 3].set_title('Error Map')
            axes[i, 3].axis('off')
            
            reconstruction_losses.append(loss.item())
            print(f"  æ ·æœ¬ {i+1}: æŸå¤± {loss.item():.4f}")
            
        except Exception as e:
            print(f"å¤„ç†æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}")
            for j in range(4):
                axes[i, j].text(0.5, 0.5, f'Error: {str(e)[:30]}', 
                              ha='center', va='center')
                axes[i, j].axis('off')
    
    plt.tight_layout()
    
    # ä¿å­˜æ¼”ç¤º
    demo_path = output_dir / 'animediffusion_mae_demo.png'
    plt.savefig(demo_path, dpi=150, bbox_inches='tight')
    print(f"âœ… AnimeDiffusion MAEæ¼”ç¤ºä¿å­˜: {demo_path}")
    plt.close()
    
    # 2. åˆ†æé‡å»ºè´¨é‡
    print(f"\nğŸ” åˆ†æAnimeDiffusioné‡å»ºè´¨é‡...")
    
    if reconstruction_losses:
        print(f"ğŸ“Š é‡å»ºè´¨é‡ç»Ÿè®¡:")
        print(f"  å¹³å‡æŸå¤±: {np.mean(reconstruction_losses):.4f}")
        print(f"  æŸå¤±èŒƒå›´: {np.min(reconstruction_losses):.4f} - {np.max(reconstruction_losses):.4f}")
        print(f"  æ ‡å‡†å·®: {np.std(reconstruction_losses):.4f}")
    
    # 3. åˆ›å»ºä¸‰æ•°æ®é›†å¯¹æ¯”
    create_three_dataset_comparison(output_dir)
    
    print(f"\nğŸ‰ AnimeDiffusion å¯è§†åŒ–å®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    
    return output_dir

def create_three_dataset_comparison(output_dir):
    """åˆ›å»ºä¸‰ä¸ªæ•°æ®é›†çš„è®­ç»ƒæ•ˆæœå¯¹æ¯”"""
    print(f"\nğŸ“Š åˆ›å»ºä¸‰æ•°æ®é›†è®­ç»ƒæ•ˆæœå¯¹æ¯”...")
    
    # è¯»å–ä¸‰ä¸ªå®éªŒçš„æ•°æ®
    experiments = [
        {
            'name': 'Synthetic',
            'log_file': './output_m4/log.txt',
            'color': 'red',
            'marker': 's'
        },
        {
            'name': 'Anime-Captions',
            'log_file': './output_anime/log.txt',
            'color': 'blue',
            'marker': 'o'
        },
        {
            'name': 'AnimeDiffusion',
            'log_file': './output_animediffusion/log.txt',
            'color': 'green',
            'marker': '^'
        }
    ]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    comparison_data = []
    
    for exp in experiments:
        if os.path.exists(exp['log_file']):
            epochs, losses = [], []
            
            with open(exp['log_file'], 'r') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        epochs.append(data['epoch'])
                        losses.append(data['train_loss'])
                    except:
                        continue
            
            if epochs and losses:
                ax1.plot(epochs, losses, color=exp['color'], marker=exp['marker'], 
                        linewidth=2, markersize=6, label=f"{exp['name']} (final: {losses[-1]:.3f})")
                
                comparison_data.append({
                    'name': exp['name'],
                    'final_loss': losses[-1],
                    'epochs': len(epochs),
                    'improvement': ((1.3080 - losses[-1]) / 1.3080 * 100) if losses[-1] < 1.3080 else 0
                })
    
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Training Loss')
    ax1.set_title('Training Loss Comparison: Three Datasets')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(bottom=0.8)
    
    # æ•°æ®é›†ä¿¡æ¯å¯¹æ¯”
    dataset_info = [
        "ğŸ“Š Dataset Comparison:",
        "",
        "ğŸ¨ Synthetic Dataset:",
        "  â€¢ 250 geometric patterns",
        "  â€¢ 224Ã—224 resolution",
        "  â€¢ Final loss: 1.308",
        "",
        "ğŸŒ Anime-Captions:",
        "  â€¢ 337K anime images (used 1K)",
        "  â€¢ 512Ã—512 â†’ 224Ã—224",
        "  â€¢ Final loss: 1.074 (+18%)",
        "",
        "ğŸ­ AnimeDiffusion:",
        "  â€¢ 8.2K HD anime images (used 500)",
        "  â€¢ 1920Ã—1080 â†’ 224Ã—224",
        "  â€¢ Final loss: 0.951 (+27%)",
        "",
        "ğŸ† Winner: AnimeDiffusion!",
        "Best reconstruction quality"
    ]
    
    ax2.text(0.05, 0.95, '\n'.join(dataset_info), transform=ax2.transAxes, 
            fontsize=11, verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    ax2.axis('off')
    ax2.set_title('Dataset Information')
    
    plt.tight_layout()
    
    # ä¿å­˜å¯¹æ¯”å›¾
    comparison_path = output_dir / 'three_dataset_comparison.png'
    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')
    print(f"âœ… ä¸‰æ•°æ®é›†å¯¹æ¯”ä¿å­˜: {comparison_path}")
    plt.close()

def main():
    """ä¸»å‡½æ•°"""
    create_animediffusion_visualization()

if __name__ == "__main__":
    main()


