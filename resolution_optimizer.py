#!/usr/bin/env python3
"""
AnimeDiffusion åˆ†è¾¨ç‡ä¼˜åŒ–å™¨
æµ‹è¯•ä¸åŒåˆ†è¾¨ç‡å’Œç­–ç•¥çš„æ•ˆæœ
"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from datasets import load_dataset
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import time

# è§£å†³ OpenMP å†²çª
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import models_mae

def test_different_resolutions():
    """æµ‹è¯•ä¸åŒåˆ†è¾¨ç‡çš„å†…å­˜å’Œæ€§èƒ½"""
    print("ğŸ’¾ æµ‹è¯•ä¸åŒåˆ†è¾¨ç‡çš„å†…å­˜å’Œæ€§èƒ½...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {'size': 224, 'batch': 8, 'desc': 'æ ‡å‡†é…ç½®'},
        {'size': 256, 'batch': 6, 'desc': 'ä¸­ç­‰åˆ†è¾¨ç‡'},
        {'size': 288, 'batch': 4, 'desc': 'é«˜åˆ†è¾¨ç‡'},
        {'size': 320, 'batch': 2, 'desc': 'æé«˜åˆ†è¾¨ç‡'},
    ]
    
    results = []
    
    for config in test_configs:
        size = config['size']
        batch_size = config['batch']
        desc = config['desc']
        
        try:
            print(f"\nğŸ§ª æµ‹è¯• {desc}: {size}Ã—{size}, batch_size={batch_size}")
            
            # åˆ›å»ºæ¨¡å‹
            model = models_mae.mae_vit_base_patch16()
            model.to(device)
            model.eval()
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            x = torch.randn(batch_size, 3, size, size, device=device)
            
            # é¢„çƒ­
            with torch.no_grad():
                _ = model(x, mask_ratio=0.75)
            
            # è®¡æ—¶æµ‹è¯•
            start_time = time.time()
            num_runs = 5
            
            for _ in range(num_runs):
                with torch.no_grad():
                    loss, pred, mask = model(x, mask_ratio=0.75)
            
            if device.type == 'mps':
                torch.mps.synchronize()
            
            end_time = time.time()
            avg_time = (end_time - start_time) / num_runs
            time_per_image = avg_time / batch_size
            
            results.append({
                'size': size,
                'batch_size': batch_size,
                'desc': desc,
                'avg_time': avg_time,
                'time_per_image': time_per_image,
                'loss': loss.item(),
                'success': True
            })
            
            print(f"  âœ… æˆåŠŸ: {avg_time*1000:.1f}ms/batch, {time_per_image*1000:.1f}ms/image")
            print(f"  æŸå¤±: {loss.item():.4f}")
            
            # æ¸…ç†å†…å­˜
            del model, x, loss, pred, mask
            if device.type == 'mps':
                torch.mps.empty_cache()
                
        except Exception as e:
            print(f"  âŒ å¤±è´¥: {e}")
            results.append({
                'size': size,
                'batch_size': batch_size,
                'desc': desc,
                'success': False,
                'error': str(e)
            })
            
            # æ¸…ç†å†…å­˜
            if device.type == 'mps':
                torch.mps.empty_cache()
    
    # æ˜¾ç¤ºç»“æœæ€»ç»“
    print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•æ€»ç»“:")
    print(f"{'åˆ†è¾¨ç‡':<10} {'æ‰¹æ¬¡å¤§å°':<8} {'çŠ¶æ€':<6} {'æ—¶é—´/å›¾ç‰‡':<12} {'æè¿°'}")
    print("-" * 60)
    
    for result in results:
        if result['success']:
            status = "âœ…"
            time_str = f"{result['time_per_image']*1000:.1f}ms"
        else:
            status = "âŒ"
            time_str = "å¤±è´¥"
        
        print(f"{result['size']}Ã—{result['size']:<3} {result['batch_size']:<8} {status:<6} {time_str:<12} {result['desc']}")
    
    return results

def create_resolution_comparison():
    """åˆ›å»ºåˆ†è¾¨ç‡å¯¹æ¯”å¯è§†åŒ–"""
    print(f"\nğŸ¨ åˆ›å»ºåˆ†è¾¨ç‡å¯¹æ¯”å¯è§†åŒ–...")
    
    try:
        # åŠ è½½ä¸€å¼ ç¤ºä¾‹å›¾ç‰‡
        ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
        sample = ds['train'][0]
        original_img = sample['image']
        
        if original_img.mode != 'RGB':
            original_img = original_img.convert('RGB')
        
        # æµ‹è¯•ä¸åŒåˆ†è¾¨ç‡
        resolutions = [224, 256, 288, 320]
        
        fig, axes = plt.subplots(2, len(resolutions), figsize=(len(resolutions)*4, 8))
        
        for i, res in enumerate(resolutions):
            # æ™ºèƒ½è£å‰ªç­–ç•¥
            transform_crop = transforms.Compose([
                transforms.Resize(int(res * 1.2), interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(res),
                transforms.ToTensor()
            ])
            
            # ç›´æ¥ç¼©æ”¾ç­–ç•¥
            transform_resize = transforms.Compose([
                transforms.Resize((res, res), interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.ToTensor()
            ])
            
            # åº”ç”¨å˜æ¢
            img_crop = transform_crop(original_img)
            img_resize = transform_resize(original_img)
            
            # æ˜¾ç¤ºç»“æœ
            axes[0, i].imshow(img_crop.permute(1, 2, 0))
            axes[0, i].set_title(f'Smart Crop\n{res}Ã—{res}')
            axes[0, i].axis('off')
            
            axes[1, i].imshow(img_resize.permute(1, 2, 0))
            axes[1, i].set_title(f'Direct Resize\n{res}Ã—{res}')
            axes[1, i].axis('off')
        
        plt.tight_layout()
        plt.savefig('resolution_comparison.png', dpi=150, bbox_inches='tight')
        print("âœ… åˆ†è¾¨ç‡å¯¹æ¯”ä¿å­˜: resolution_comparison.png")
        plt.close()
        
    except Exception as e:
        print(f"åˆ†è¾¨ç‡å¯¹æ¯”å¤±è´¥: {e}")

def recommend_optimal_config():
    """æ¨èæœ€ä¼˜é…ç½®"""
    print(f"\nğŸ¯ æ¨èæœ€ä¼˜é…ç½®:")
    
    # åŸºäºApple M4 24GBçš„é…ç½®å»ºè®®
    configs = {
        'quick_test': {
            'input_size': 224,
            'batch_size': 8,
            'max_samples': 500,
            'epochs': 5,
            'description': 'å¿«é€Ÿæµ‹è¯• - 5åˆ†é’ŸéªŒè¯'
        },
        'balanced': {
            'input_size': 224,
            'batch_size': 6,
            'max_samples': 2000,
            'epochs': 20,
            'description': 'å¹³è¡¡é…ç½® - 30åˆ†é’Ÿè®­ç»ƒ'
        },
        'high_quality': {
            'input_size': 256,
            'batch_size': 4,
            'max_samples': 5000,
            'epochs': 50,
            'description': 'é«˜è´¨é‡ - 2å°æ—¶è®­ç»ƒ'
        },
        'full_dataset': {
            'input_size': 224,
            'batch_size': 8,
            'max_samples': None,  # å…¨éƒ¨8202å¼ 
            'epochs': 100,
            'description': 'å®Œæ•´æ•°æ®é›† - æ•°å°æ—¶è®­ç»ƒ'
        }
    }
    
    print(f"{'é…ç½®':<15} {'åˆ†è¾¨ç‡':<8} {'æ‰¹æ¬¡':<6} {'æ ·æœ¬æ•°':<8} {'è½®æ•°':<6} {'æè¿°'}")
    print("-" * 70)
    
    for name, config in configs.items():
        samples_str = str(config['max_samples']) if config['max_samples'] else 'All'
        print(f"{name:<15} {config['input_size']}Ã—{config['input_size']:<3} {config['batch_size']:<6} {samples_str:<8} {config['epochs']:<6} {config['description']}")
    
    return configs

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ AnimeDiffusion åˆ†è¾¨ç‡ä¼˜åŒ–åˆ†æ")
    print("=" * 50)
    
    # 1. æµ‹è¯•ä¸åŒåˆ†è¾¨ç‡çš„æ€§èƒ½
    performance_results = test_different_resolutions()
    
    # 2. åˆ›å»ºåˆ†è¾¨ç‡å¯¹æ¯”
    create_resolution_comparison()
    
    # 3. æ¨èæœ€ä¼˜é…ç½®
    configs = recommend_optimal_config()
    
    # 4. ç”Ÿæˆè®­ç»ƒå‘½ä»¤
    print(f"\nğŸš€ æ¨èçš„è®­ç»ƒå‘½ä»¤:")
    
    print(f"\n1ï¸âƒ£ å¿«é€Ÿæµ‹è¯• (æ¨èå…ˆè¯•):")
    print(f"python main_pretrain_animediffusion.py --input_size 224 --batch_size 8 --max_samples 500 --epochs 5")
    
    print(f"\n2ï¸âƒ£ å¹³è¡¡è®­ç»ƒ:")
    print(f"python main_pretrain_animediffusion.py --input_size 224 --batch_size 6 --max_samples 2000 --epochs 20")
    
    print(f"\n3ï¸âƒ£ é«˜è´¨é‡è®­ç»ƒ:")
    print(f"python main_pretrain_animediffusion.py --input_size 256 --batch_size 4 --max_samples 5000 --epochs 50")
    
    print(f"\nğŸ’¡ å…³é”®å»ºè®®:")
    print(f"  âœ… ä½¿ç”¨ 224Ã—224 åˆ†è¾¨ç‡ (å†…å­˜å‹å¥½)")
    print(f"  âœ… ä½¿ç”¨ smart_crop ç­–ç•¥ (ä¿æŒç»†èŠ‚)")
    print(f"  âœ… ä»å°æ ·æœ¬å¼€å§‹æµ‹è¯•")
    print(f"  âš ï¸  é¿å…ç›´æ¥ä½¿ç”¨ 1920Ã—1080 (å†…å­˜æº¢å‡º)")

if __name__ == "__main__":
    main()


