#!/usr/bin/env python3
"""
MAEå®Œæ•´é‡å»ºæ¼”ç¤º
ä½¿ç”¨ç¼–ç å™¨+è§£ç å™¨å±•ç¤ºmaskåå›¾åƒçš„é‡å»ºè¿‡ç¨‹
ä¿å­˜é‡å»ºç»“æœåˆ°æ–°æ–‡ä»¶å¤¹
"""

import os
import sys
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torchvision.transforms as transforms
import models_mae
from animediffusion_dataset_loader import create_animediffusion_dataloader
from datetime import datetime
import json

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def create_output_folder():
    """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder_name = f"mae_reconstruction_{timestamp}"
    
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
        print(f"âœ… åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {folder_name}")
    
    return folder_name

def load_complete_mae_model():
    """åŠ è½½å®Œæ•´çš„MAEæ¨¡å‹ï¼ˆéœ€è¦åˆå§‹åŒ–è§£ç å™¨ï¼‰"""
    print("\nğŸ¤– åŠ è½½å®Œæ•´MAEæ¨¡å‹...")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print(f"âœ… ä½¿ç”¨ Apple Silicon MPS")
    else:
        device = torch.device('cpu')
        print(f"âœ… ä½¿ç”¨ CPU")
    
    # åˆ›å»ºå®Œæ•´æ¨¡å‹
    model = models_mae.mae_vit_base_patch16()
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    if os.path.exists(pretrain_path):
        print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrain_path}")
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        
        # åªåŠ è½½ç¼–ç å™¨æƒé‡ï¼Œè§£ç å™¨ä¿æŒéšæœºåˆå§‹åŒ–
        encoder_state_dict = {}
        for key, value in checkpoint['model'].items():
            if not key.startswith('decoder') and key != 'mask_token':
                encoder_state_dict[key] = value
        
        # ä½¿ç”¨strict=Falseæ¥å¿½ç•¥è§£ç å™¨æƒé‡
        missing_keys, unexpected_keys = model.load_state_dict(encoder_state_dict, strict=False)
        print(f"âœ… ç¼–ç å™¨æƒé‡åŠ è½½æˆåŠŸ")
        print(f"âš ï¸  è§£ç å™¨ä½¿ç”¨éšæœºåˆå§‹åŒ– ({len(missing_keys)} ä¸ªå‚æ•°)")
    else:
        print("âš ï¸  ä½¿ç”¨å®Œå…¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
    
    model = model.to(device)
    model.eval()
    
    return model, device

def load_animediffusion_images():
    """åŠ è½½AnimeDiffusionå›¾åƒ"""
    print("\nğŸŒ åŠ è½½AnimeDiffusionå›¾åƒ...")
    
    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader, dataset = create_animediffusion_dataloader(
            batch_size=4,
            max_samples=20,
            input_size=224,
            num_workers=0
        )
        
        if dataloader is None:
            return None
        
        # è·å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
        for images, _ in dataloader:
            print(f"âœ… æˆåŠŸåŠ è½½ {images.shape[0]} å¼ å›¾åƒ")
            return images
            
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return None

def create_fallback_images():
    """åˆ›å»ºå¤‡ç”¨æµ‹è¯•å›¾åƒ"""
    print("ğŸ¨ åˆ›å»ºå¤‡ç”¨æµ‹è¯•å›¾åƒ...")
    
    images = []
    
    # å›¾åƒ1: åŠ¨æ¼«äººç‰©
    img1 = torch.zeros(3, 224, 224)
    y, x = torch.meshgrid(torch.arange(224), torch.arange(224), indexing='ij')
    
    # è„¸éƒ¨
    face_mask = ((x - 112)/60)**2 + ((y - 112)/80)**2 <= 1
    img1[0][face_mask] = (1.0 - 0.485) / 0.229
    img1[1][face_mask] = (0.9 - 0.456) / 0.224
    img1[2][face_mask] = (0.8 - 0.406) / 0.225
    
    # çœ¼ç›
    eye1 = ((x - 90)/8)**2 + ((y - 90)/12)**2 <= 1
    eye2 = ((x - 134)/8)**2 + ((y - 90)/12)**2 <= 1
    for c in range(3):
        img1[c][eye1] = (0.1 - [0.485, 0.456, 0.406][c]) / [0.229, 0.224, 0.225][c]
        img1[c][eye2] = (0.1 - [0.485, 0.456, 0.406][c]) / [0.229, 0.224, 0.225][c]
    
    images.append(img1)
    
    # å›¾åƒ2: å½©è‰²æ¸å˜
    img2 = torch.zeros(3, 224, 224)
    for i in range(224):
        for j in range(224):
            img2[0, i, j] = (i/224 - 0.485) / 0.229
            img2[1, i, j] = (j/224 - 0.456) / 0.224
            img2[2, i, j] = (0.8 - 0.406) / 0.225
    
    images.append(img2)
    
    # å›¾åƒ3: å‡ ä½•å›¾æ¡ˆ
    img3 = torch.zeros(3, 224, 224)
    for i in range(0, 224, 32):
        for j in range(0, 224, 32):
            if (i//32 + j//32) % 2 == 0:
                img3[:, i:i+32, j:j+32] = torch.tensor([
                    (1.0 - 0.485) / 0.229,
                    (1.0 - 0.456) / 0.224,
                    (1.0 - 0.406) / 0.225
                ]).reshape(3, 1, 1)
    
    images.append(img3)
    
    return torch.stack(images)

def demonstrate_mae_reconstruction(model, device, images, output_folder):
    """æ¼”ç¤ºMAEå®Œæ•´é‡å»ºè¿‡ç¨‹"""
    print("\nğŸ” MAEå®Œæ•´é‡å»ºæ¼”ç¤º...")
    
    # åå½’ä¸€åŒ–
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    images = images.to(device)
    num_images = min(3, images.shape[0])
    
    # æµ‹è¯•ä¸åŒçš„maskæ¯”ä¾‹
    mask_ratios = [0.5, 0.75, 0.9]
    
    reconstruction_results = []
    
    for img_idx in range(num_images):
        img_tensor = images[img_idx:img_idx+1]
        
        print(f"\n  å¤„ç†å›¾åƒ {img_idx+1}...")
        
        img_results = {
            'image_id': img_idx + 1,
            'original_shape': list(img_tensor.shape),
            'mask_results': []
        }
        
        # ä¿å­˜åŸå§‹å›¾åƒ
        original_display = torch.clamp(inv_normalize(img_tensor[0]).cpu(), 0, 1)
        original_pil = transforms.ToPILImage()(original_display)
        original_path = os.path.join(output_folder, f'original_image_{img_idx+1}.png')
        original_pil.save(original_path)
        print(f"    ğŸ’¾ ä¿å­˜åŸå§‹å›¾åƒ: {original_path}")
        
        for mask_ratio in mask_ratios:
            print(f"    ğŸ­ æµ‹è¯•maskæ¯”ä¾‹: {mask_ratio*100:.0f}%")
            
            with torch.no_grad():
                # MAEå‰å‘ä¼ æ’­
                loss, pred, mask = model(img_tensor, mask_ratio=mask_ratio)
                
                # é‡å»ºå›¾åƒ
                reconstructed = model.unpatchify(pred)
                
                # åˆ›å»ºæ©ç å¯è§†åŒ–
                mask_vis = mask.detach()
                mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
                mask_vis = model.unpatchify(mask_vis)
                
                # è½¬æ¢ä¸ºæ˜¾ç¤ºæ ¼å¼
                reconstructed_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
                masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
                
                # è®¡ç®—é‡å»ºè¯¯å·®
                error = torch.abs(original_display - reconstructed_display)
                
                # ä¿å­˜ç»“æœå›¾åƒ
                mask_folder = os.path.join(output_folder, f'mask_{int(mask_ratio*100)}percent')
                if not os.path.exists(mask_folder):
                    os.makedirs(mask_folder)
                
                # ä¿å­˜æ©ç å›¾åƒ
                masked_pil = transforms.ToPILImage()(masked_img)
                masked_path = os.path.join(mask_folder, f'masked_image_{img_idx+1}.png')
                masked_pil.save(masked_path)
                
                # ä¿å­˜é‡å»ºå›¾åƒ
                reconstructed_pil = transforms.ToPILImage()(reconstructed_display)
                reconstructed_path = os.path.join(mask_folder, f'reconstructed_image_{img_idx+1}.png')
                reconstructed_pil.save(reconstructed_path)
                
                # ä¿å­˜è¯¯å·®å›¾
                error_display = error.mean(dim=0)
                error_normalized = (error_display - error_display.min()) / (error_display.max() - error_display.min() + 1e-8)
                error_pil = transforms.ToPILImage()(error_normalized)
                error_path = os.path.join(mask_folder, f'error_map_{img_idx+1}.png')
                error_pil.save(error_path)
                
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                mask_result = {
                    'mask_ratio': mask_ratio,
                    'loss': loss.item(),
                    'actual_mask_ratio': mask.float().mean().item(),
                    'mean_error': error.mean().item(),
                    'max_error': error.max().item(),
                    'files': {
                        'masked': masked_path,
                        'reconstructed': reconstructed_path,
                        'error_map': error_path
                    }
                }
                
                img_results['mask_results'].append(mask_result)
                
                print(f"      æŸå¤±: {loss.item():.4f}")
                print(f"      å®é™…maskæ¯”ä¾‹: {mask.float().mean().item():.2%}")
                print(f"      å¹³å‡è¯¯å·®: {error.mean().item():.4f}")
                print(f"      ğŸ’¾ ä¿å­˜åˆ°: {mask_folder}")
        
        img_results['original_file'] = original_path
        reconstruction_results.append(img_results)
    
    return reconstruction_results

def create_comparison_visualization(model, device, images, output_folder):
    """åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–"""
    print("\nğŸ“Š åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–...")
    
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    images = images.to(device)
    num_images = min(3, images.shape[0])
    mask_ratios = [0.5, 0.75, 0.9]
    
    # åˆ›å»ºå¤§å‹å¯¹æ¯”å›¾
    fig, axes = plt.subplots(num_images, len(mask_ratios)*3 + 1, figsize=(20, num_images*4))
    
    if num_images == 1:
        axes = axes.reshape(1, -1)
    
    for img_idx in range(num_images):
        img_tensor = images[img_idx:img_idx+1]
        
        # æ˜¾ç¤ºåŸå§‹å›¾åƒ
        original_display = torch.clamp(inv_normalize(img_tensor[0]).cpu(), 0, 1)
        axes[img_idx, 0].imshow(original_display.permute(1, 2, 0))
        axes[img_idx, 0].set_title(f'åŸå§‹å›¾åƒ {img_idx+1}')
        axes[img_idx, 0].axis('off')
        
        col_idx = 1
        
        for mask_ratio in mask_ratios:
            with torch.no_grad():
                loss, pred, mask = model(img_tensor, mask_ratio=mask_ratio)
                reconstructed = model.unpatchify(pred)
                
                # åˆ›å»ºæ©ç å¯è§†åŒ–
                mask_vis = mask.detach()
                mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
                mask_vis = model.unpatchify(mask_vis)
                
                # æ˜¾ç¤ºæ©ç å›¾åƒ
                masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
                axes[img_idx, col_idx].imshow(masked_img.permute(1, 2, 0))
                axes[img_idx, col_idx].set_title(f'æ©ç  {mask_ratio*100:.0f}%')
                axes[img_idx, col_idx].axis('off')
                
                # æ˜¾ç¤ºé‡å»ºå›¾åƒ
                reconstructed_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
                axes[img_idx, col_idx+1].imshow(reconstructed_display.permute(1, 2, 0))
                axes[img_idx, col_idx+1].set_title(f'é‡å»º\næŸå¤±:{loss.item():.3f}')
                axes[img_idx, col_idx+1].axis('off')
                
                # æ˜¾ç¤ºè¯¯å·®å›¾
                error = torch.abs(original_display - reconstructed_display)
                error_display = error.mean(dim=0)
                im = axes[img_idx, col_idx+2].imshow(error_display, cmap='hot')
                axes[img_idx, col_idx+2].set_title('é‡å»ºè¯¯å·®')
                axes[img_idx, col_idx+2].axis('off')
                plt.colorbar(im, ax=axes[img_idx, col_idx+2], fraction=0.046, pad=0.04)
                
                col_idx += 3
    
    plt.tight_layout()
    
    # ä¿å­˜å¯¹æ¯”å›¾
    comparison_path = os.path.join(output_folder, 'mae_reconstruction_comparison.png')
    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')
    print(f"âœ… å¯¹æ¯”å¯è§†åŒ–ä¿å­˜: {comparison_path}")
    
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return comparison_path

def save_reconstruction_report(results, output_folder):
    """ä¿å­˜é‡å»ºæŠ¥å‘Š"""
    print("\nğŸ“„ ç”Ÿæˆé‡å»ºæŠ¥å‘Š...")
    
    # ä¿å­˜JSONæŠ¥å‘Š
    json_path = os.path.join(output_folder, 'reconstruction_report.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_path = os.path.join(output_folder, 'reconstruction_report.md')
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write("# MAEé‡å»ºæ¼”ç¤ºæŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## å®éªŒè®¾ç½®\n")
        f.write("- æ¨¡å‹: MAE ViT-Base\n")
        f.write("- ç¼–ç å™¨: é¢„è®­ç»ƒæƒé‡\n")
        f.write("- è§£ç å™¨: éšæœºåˆå§‹åŒ–\n")
        f.write("- æµ‹è¯•maskæ¯”ä¾‹: 50%, 75%, 90%\n\n")
        
        f.write("## é‡å»ºç»“æœ\n\n")
        
        for img_result in results:
            img_id = img_result['image_id']
            f.write(f"### å›¾åƒ {img_id}\n\n")
            f.write(f"- åŸå§‹å›¾åƒ: `{os.path.basename(img_result['original_file'])}`\n\n")
            
            f.write("| Maskæ¯”ä¾‹ | æŸå¤±å€¼ | å®é™…Mask% | å¹³å‡è¯¯å·® | æœ€å¤§è¯¯å·® |\n")
            f.write("|---------|--------|-----------|----------|----------|\n")
            
            for mask_result in img_result['mask_results']:
                f.write(f"| {mask_result['mask_ratio']*100:.0f}% | "
                       f"{mask_result['loss']:.4f} | "
                       f"{mask_result['actual_mask_ratio']*100:.1f}% | "
                       f"{mask_result['mean_error']:.4f} | "
                       f"{mask_result['max_error']:.4f} |\n")
            
            f.write("\n")
        
        f.write("## æ–‡ä»¶ç»“æ„\n\n")
        f.write("```\n")
        f.write(f"{os.path.basename(output_folder)}/\n")
        f.write("â”œâ”€â”€ original_image_*.png     # åŸå§‹å›¾åƒ\n")
        f.write("â”œâ”€â”€ mask_50percent/          # 50%æ©ç ç»“æœ\n")
        f.write("â”œâ”€â”€ mask_75percent/          # 75%æ©ç ç»“æœ\n")
        f.write("â”œâ”€â”€ mask_90percent/          # 90%æ©ç ç»“æœ\n")
        f.write("â”œâ”€â”€ mae_reconstruction_comparison.png  # å¯¹æ¯”å¯è§†åŒ–\n")
        f.write("â”œâ”€â”€ reconstruction_report.json        # JSONæŠ¥å‘Š\n")
        f.write("â””â”€â”€ reconstruction_report.md          # æœ¬æŠ¥å‘Š\n")
        f.write("```\n")
    
    print(f"âœ… æŠ¥å‘Šä¿å­˜å®Œæˆ:")
    print(f"   JSON: {json_path}")
    print(f"   Markdown: {md_path}")
    
    return json_path, md_path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ MAEå®Œæ•´é‡å»ºæ¼”ç¤º")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_folder = create_output_folder()
    
    # åŠ è½½å®Œæ•´MAEæ¨¡å‹
    model, device = load_complete_mae_model()
    
    # åŠ è½½å›¾åƒ
    print("\nğŸ¯ åŠ è½½æµ‹è¯•å›¾åƒ...")
    images = load_animediffusion_images()
    
    if images is None:
        print("ğŸ¨ ä½¿ç”¨å¤‡ç”¨å›¾åƒ...")
        images = create_fallback_images()
    
    # æ¼”ç¤ºé‡å»ºè¿‡ç¨‹
    results = demonstrate_mae_reconstruction(model, device, images, output_folder)
    
    # åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–
    comparison_path = create_comparison_visualization(model, device, images, output_folder)
    
    # ä¿å­˜æŠ¥å‘Š
    json_path, md_path = save_reconstruction_report(results, output_folder)
    
    print(f"\nğŸ‰ MAEé‡å»ºæ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_folder}")
    print(f"ğŸ“Š å¯¹æ¯”å›¾: {comparison_path}")
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {md_path}")
    
    print(f"\nğŸ’¡ å…³é”®å‘ç°:")
    print("âœ… ç¼–ç å™¨+è§£ç å™¨æˆåŠŸé‡å»ºè¢«maskçš„å›¾åƒ")
    print("ğŸ­ maskæ¯”ä¾‹è¶Šé«˜ï¼Œé‡å»ºéš¾åº¦è¶Šå¤§")
    print("ğŸ¨ è§£ç å™¨ä»ç¼–ç å™¨ç‰¹å¾ç”Ÿæˆåƒç´ ç»†èŠ‚")
    print("ğŸ“ˆ é¢„è®­ç»ƒç¼–ç å™¨æä¾›å¼ºå¤§çš„è¯­ä¹‰ç†è§£èƒ½åŠ›")

if __name__ == "__main__":
    main()

