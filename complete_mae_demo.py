#!/usr/bin/env python3
"""
ä½¿ç”¨åŒ…å«å®Œæ•´è§£ç å™¨æƒé‡çš„MAEæ¨¡å‹è¿›è¡Œé«˜è´¨é‡é‡å»ºæ¼”ç¤º
"""

import os
import sys
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torchvision.transforms as transforms
import models_mae
from animediffusion_dataset_loader import create_animediffusion_dataloader
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç¯å¢ƒ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

def load_complete_mae_model():
    """åŠ è½½åŒ…å«å®Œæ•´è§£ç å™¨çš„MAEæ¨¡å‹"""
    print("\nğŸ¯ åŠ è½½åŒ…å«å®Œæ•´è§£ç å™¨çš„MAEæ¨¡å‹...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")
    
    # åˆ›å»ºViT-Largeæ¨¡å‹ï¼ˆä¸å¯è§†åŒ–æ¨¡å‹åŒ¹é…ï¼‰
    model = models_mae.mae_vit_large_patch16()
    
    # åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒæƒé‡
    model_path = 'complete_mae_models/mae_visualize_vit_large.pth'
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œ: ./download_complete_mae.sh")
        return None, None
    
    try:
        print(f"ğŸ“¥ åŠ è½½å®Œæ•´æ¨¡å‹æƒé‡: {model_path}")
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        # ç»Ÿè®¡å‚æ•°
        encoder_keys = [k for k in state_dict.keys() if not k.startswith('decoder') and k != 'mask_token']
        decoder_keys = [k for k in state_dict.keys() if k.startswith('decoder') or k == 'mask_token']
        
        print(f"  ç¼–ç å™¨å‚æ•°: {len(encoder_keys)} ä¸ª")
        print(f"  è§£ç å™¨å‚æ•°: {len(decoder_keys)} ä¸ª")
        
        # åŠ è½½æƒé‡
        msg = model.load_state_dict(state_dict, strict=False)
        if msg.missing_keys:
            print(f"  âš ï¸  ç¼ºå¤±é”®: {len(msg.missing_keys)} ä¸ª")
        if msg.unexpected_keys:
            print(f"  âš ï¸  æ„å¤–é”®: {len(msg.unexpected_keys)} ä¸ª")
        
        model = model.to(device)
        model.eval()
        
        print("âœ… å®Œæ•´MAEæ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model, device
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def create_high_quality_reconstruction_demo(model, device):
    """åˆ›å»ºé«˜è´¨é‡é‡å»ºæ¼”ç¤º"""
    print("\nğŸ¨ åˆ›å»ºé«˜è´¨é‡é‡å»ºæ¼”ç¤º...")
    
    # å°è¯•åŠ è½½AnimeDiffusionæ•°æ®
    try:
        dataloader, dataset = create_animediffusion_dataloader(
            batch_size=3,
            max_samples=10,
            input_size=224,
            num_workers=0
        )
        
        if dataloader is not None:
            for images, _ in dataloader:
                test_images = images[:3]
                print(f"âœ… ä½¿ç”¨AnimeDiffusionæ•°æ®: {test_images.shape}")
                break
        else:
            raise Exception("æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥")
            
    except Exception as e:
        print(f"âš ï¸  AnimeDiffusionåŠ è½½å¤±è´¥: {e}")
        print("ğŸ¨ ä½¿ç”¨å¤‡ç”¨æµ‹è¯•å›¾åƒ...")
        test_images = create_test_images(device)
    
    # åå½’ä¸€åŒ–
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    test_images = test_images.to(device)
    num_images = test_images.shape[0]
    
    # æµ‹è¯•ä¸åŒmaskæ¯”ä¾‹
    mask_ratios = [0.5, 0.75, 0.9]
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(num_images, len(mask_ratios)*3 + 1, figsize=(20, num_images*4))
    
    if num_images == 1:
        axes = axes.reshape(1, -1)
    
    reconstruction_stats = []
    
    for img_idx in range(num_images):
        img_tensor = test_images[img_idx:img_idx+1]
        
        print(f"\n  ğŸ” å¤„ç†å›¾åƒ {img_idx+1}...")
        
        # æ˜¾ç¤ºåŸå§‹å›¾åƒ
        original_display = torch.clamp(inv_normalize(img_tensor[0]).cpu(), 0, 1)
        axes[img_idx, 0].imshow(original_display.permute(1, 2, 0))
        axes[img_idx, 0].set_title(f'åŸå§‹å›¾åƒ {img_idx+1}')
        axes[img_idx, 0].axis('off')
        
        col_idx = 1
        img_stats = {'image_id': img_idx + 1, 'results': []}
        
        for mask_ratio in mask_ratios:
            print(f"    ğŸ­ Maskæ¯”ä¾‹: {mask_ratio*100:.0f}%")
            
            with torch.no_grad():
                # MAEå‰å‘ä¼ æ’­
                loss, pred, mask = model(img_tensor, mask_ratio=mask_ratio)
                
                # é‡å»ºå›¾åƒ
                reconstructed = model.unpatchify(pred)
                
                # åˆ›å»ºæ©ç å¯è§†åŒ–
                mask_vis = mask.detach()
                mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
                mask_vis = model.unpatchify(mask_vis)
                
                # è½¬æ¢ä¸ºæ˜¾ç¤ºæ ¼å¼
                reconstructed_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
                masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
                
                # è®¡ç®—é‡å»ºè¯¯å·®
                error = torch.abs(original_display - reconstructed_display)
                
                # æ˜¾ç¤ºç»“æœ
                axes[img_idx, col_idx].imshow(masked_img.permute(1, 2, 0))
                axes[img_idx, col_idx].set_title(f'æ©ç  {mask_ratio*100:.0f}%')
                axes[img_idx, col_idx].axis('off')
                
                axes[img_idx, col_idx+1].imshow(reconstructed_display.permute(1, 2, 0))
                axes[img_idx, col_idx+1].set_title(f'é‡å»º\næŸå¤±:{loss.item():.3f}')
                axes[img_idx, col_idx+1].axis('off')
                
                error_display = error.mean(dim=0)
                im = axes[img_idx, col_idx+2].imshow(error_display, cmap='hot')
                axes[img_idx, col_idx+2].set_title(f'è¯¯å·®\nå‡å€¼:{error.mean():.3f}')
                axes[img_idx, col_idx+2].axis('off')
                plt.colorbar(im, ax=axes[img_idx, col_idx+2], fraction=0.046, pad=0.04)
                
                col_idx += 3
                
                # è®°å½•ç»Ÿè®¡
                stats = {
                    'mask_ratio': mask_ratio,
                    'loss': loss.item(),
                    'actual_mask_ratio': mask.float().mean().item(),
                    'mean_error': error.mean().item(),
                    'max_error': error.max().item(),
                    'pred_range': [pred.min().item(), pred.max().item()]
                }
                
                img_stats['results'].append(stats)
                
                print(f"      æŸå¤±: {loss.item():.4f}")
                print(f"      é¢„æµ‹èŒƒå›´: [{pred.min():.3f}, {pred.max():.3f}]")
                print(f"      é‡å»ºè¯¯å·®: {error.mean():.4f}")
        
        reconstruction_stats.append(img_stats)
    
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f'complete_mae_reconstruction_{timestamp}.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… é«˜è´¨é‡é‡å»ºç»“æœä¿å­˜: {output_path}")
    
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return output_path, reconstruction_stats

def create_test_images(device):
    """åˆ›å»ºæµ‹è¯•å›¾åƒ"""
    images = []
    
    # å›¾åƒ1: æ¸…æ™°çš„å‡ ä½•å›¾æ¡ˆ
    img1 = torch.zeros(3, 224, 224, device=device)
    
    # èƒŒæ™¯æ¸å˜
    for i in range(224):
        for j in range(224):
            img1[0, i, j] = (0.2 + 0.3 * i / 224 - 0.485) / 0.229
            img1[1, i, j] = (0.3 + 0.4 * j / 224 - 0.456) / 0.224
            img1[2, i, j] = (0.6 - 0.406) / 0.225
    
    # æ·»åŠ åœ†å½¢
    y, x = torch.meshgrid(torch.arange(224, device=device), torch.arange(224, device=device), indexing='ij')
    circle = (x - 112)**2 + (y - 112)**2 <= 40**2
    img1[0][circle] = (0.9 - 0.485) / 0.229
    img1[1][circle] = (0.1 - 0.456) / 0.224
    img1[2][circle] = (0.1 - 0.406) / 0.225
    
    images.append(img1)
    
    # å›¾åƒ2: æ£‹ç›˜æ ¼
    img2 = torch.zeros(3, 224, 224, device=device)
    for i in range(0, 224, 32):
        for j in range(0, 224, 32):
            if (i//32 + j//32) % 2 == 0:
                color = [(0.8 - 0.485) / 0.229, (0.8 - 0.456) / 0.224, (0.8 - 0.406) / 0.225]
            else:
                color = [(0.2 - 0.485) / 0.229, (0.2 - 0.456) / 0.224, (0.2 - 0.406) / 0.225]
            
            img2[:, i:i+32, j:j+32] = torch.tensor(color, device=device).reshape(3, 1, 1)
    
    images.append(img2)
    
    # å›¾åƒ3: åŒå¿ƒåœ†
    img3 = torch.zeros(3, 224, 224, device=device)
    for r in range(20, 100, 20):
        mask = ((x - 112)**2 + (y - 112)**2 >= (r-10)**2) & ((x - 112)**2 + (y - 112)**2 <= r**2)
        color_intensity = r / 100
        img3[0][mask] = (color_intensity - 0.485) / 0.229
        img3[1][mask] = (0.5 - 0.456) / 0.224
        img3[2][mask] = (1.0 - color_intensity - 0.406) / 0.225
    
    images.append(img3)
    
    return torch.stack(images)

def compare_with_random_decoder():
    """ä¸éšæœºè§£ç å™¨è¿›è¡Œå¯¹æ¯”"""
    print("\nğŸ“Š ä¸éšæœºè§£ç å™¨æ¨¡å‹å¯¹æ¯”...")
    
    # åŠ è½½å®Œæ•´æ¨¡å‹
    complete_model, device = load_complete_mae_model()
    if complete_model is None:
        return
    
    # åˆ›å»ºéšæœºè§£ç å™¨æ¨¡å‹
    random_model = models_mae.mae_vit_large_patch16()
    
    # åªåŠ è½½ç¼–ç å™¨æƒé‡
    model_path = 'complete_mae_models/mae_visualize_vit_large.pth'
    checkpoint = torch.load(model_path, map_location='cpu')
    state_dict = checkpoint['model']
    
    encoder_state_dict = {}
    for key, value in state_dict.items():
        if not key.startswith('decoder') and key != 'mask_token':
            encoder_state_dict[key] = value
    
    random_model.load_state_dict(encoder_state_dict, strict=False)
    random_model = random_model.to(device)
    random_model.eval()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_img = create_test_images(device)[0:1]  # åªç”¨ç¬¬ä¸€å¼ 
    
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    
    models_to_test = [
        ("å®Œæ•´é¢„è®­ç»ƒæ¨¡å‹", complete_model),
        ("éšæœºè§£ç å™¨æ¨¡å‹", random_model)
    ]
    
    for i, (model_name, model) in enumerate(models_to_test):
        print(f"  ğŸ” æµ‹è¯• {model_name}...")
        
        with torch.no_grad():
            # åŸå§‹å›¾åƒ
            original_display = torch.clamp(inv_normalize(test_img[0]).cpu(), 0, 1)
            axes[i, 0].imshow(original_display.permute(1, 2, 0))
            axes[i, 0].set_title(f'{model_name}\nåŸå§‹å›¾åƒ')
            axes[i, 0].axis('off')
            
            # MAEé‡å»º
            loss, pred, mask = model(test_img, mask_ratio=0.75)
            reconstructed = model.unpatchify(pred)
            
            # æ©ç å›¾åƒ
            mask_vis = mask.detach()
            mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
            mask_vis = model.unpatchify(mask_vis)
            masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
            
            axes[i, 1].imshow(masked_img.permute(1, 2, 0))
            axes[i, 1].set_title('æ©ç å›¾åƒ (75%)')
            axes[i, 1].axis('off')
            
            # é‡å»ºå›¾åƒ
            reconstructed_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
            axes[i, 2].imshow(reconstructed_display.permute(1, 2, 0))
            axes[i, 2].set_title(f'é‡å»ºå›¾åƒ\næŸå¤±:{loss.item():.3f}')
            axes[i, 2].axis('off')
            
            # é‡å»ºè¯¯å·®
            error = torch.abs(original_display - reconstructed_display)
            error_display = error.mean(dim=0)
            im = axes[i, 3].imshow(error_display, cmap='hot')
            axes[i, 3].set_title(f'é‡å»ºè¯¯å·®\nå‡å€¼:{error.mean():.3f}')
            axes[i, 3].axis('off')
            plt.colorbar(im, ax=axes[i, 3], fraction=0.046, pad=0.04)
            
            print(f"    æŸå¤±: {loss.item():.4f}")
            print(f"    é¢„æµ‹èŒƒå›´: [{pred.min():.3f}, {pred.max():.3f}]")
            print(f"    é‡å»ºè¯¯å·®: {error.mean():.4f}")
    
    plt.tight_layout()
    
    comparison_path = 'complete_vs_random_decoder.png'
    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… å¯¹æ¯”ç»“æœä¿å­˜: {comparison_path}")
    
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return comparison_path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å®Œæ•´MAEæ¨¡å‹é«˜è´¨é‡é‡å»ºæ¼”ç¤º")
    print("=" * 60)
    
    # åŠ è½½å®Œæ•´æ¨¡å‹
    model, device = load_complete_mae_model()
    if model is None:
        return
    
    # åˆ›å»ºé«˜è´¨é‡é‡å»ºæ¼”ç¤º
    output_path, stats = create_high_quality_reconstruction_demo(model, device)
    
    # ä¸éšæœºè§£ç å™¨å¯¹æ¯”
    comparison_path = compare_with_random_decoder()
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ é«˜è´¨é‡é‡å»º: {output_path}")
    print(f"ğŸ“Š å¯¹æ¯”åˆ†æ: {comparison_path}")
    
    print(f"\nğŸ’¡ å…³é”®å‘ç°:")
    print("âœ… å®Œæ•´é¢„è®­ç»ƒæ¨¡å‹é‡å»ºè´¨é‡æ˜¾è‘—æå‡")
    print("ğŸ¨ è§£ç å™¨æƒé‡å¯¹é‡å»ºæ•ˆæœè‡³å…³é‡è¦")
    print("ğŸ“ˆ é¢„è®­ç»ƒè§£ç å™¨èƒ½æ­£ç¡®ç†è§£ç¼–ç å™¨ç‰¹å¾")
    print("ğŸ”¥ è¿™å°±æ˜¯é«˜è´¨é‡MAEé‡å»ºçš„æ­£ç¡®æ–¹æ³•!")

if __name__ == "__main__":
    main()

