#!/usr/bin/env python3
"""
MAE è®­ç»ƒç»“æœå¯è§†åŒ–å·¥å…·
å±•ç¤ºè®­ç»ƒè¿‡ç¨‹ã€æ¨¡å‹è¾“å‡ºå’Œé‡å»ºæ•ˆæœ
"""

import os
import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import json
from pathlib import Path

# è§£å†³ OpenMP å†²çª
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import models_mae

def load_trained_model(checkpoint_path, device='mps'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {checkpoint_path}")
    
    model = models_mae.mae_vit_base_patch16(norm_pix_loss=True)
    
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œepoch: {checkpoint.get('epoch', 'unknown')}")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°checkpointï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    model.to(device)
    model.eval()
    return model

def visualize_mae_reconstruction(model, image_path, device='mps', mask_ratio=0.75):
    """å¯è§†åŒ– MAE é‡å»ºè¿‡ç¨‹"""
    print(f"ğŸ¨ å¯è§†åŒ–é‡å»ºè¿‡ç¨‹: {image_path}")
    
    # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åå½’ä¸€åŒ–ç”¨äºæ˜¾ç¤º
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    # åŠ è½½å›¾åƒ
    original_img = Image.open(image_path).convert('RGB')
    img_tensor = transform(original_img).unsqueeze(0).to(device)
    
    with torch.no_grad():
        # å‰å‘ä¼ æ’­
        loss, pred, mask = model(img_tensor, mask_ratio=mask_ratio)
        
        # é‡å»ºå›¾åƒ
        reconstructed = model.unpatchify(pred)
        
        # åˆ›å»ºæ©ç å¯è§†åŒ–
        mask = mask.detach()
        mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
        mask = model.unpatchify(mask)
    
    # è½¬æ¢ä¸ºå¯æ˜¾ç¤ºæ ¼å¼
    original_display = inv_normalize(img_tensor[0]).cpu()
    original_display = torch.clamp(original_display, 0, 1)
    
    reconstructed_display = inv_normalize(reconstructed[0]).cpu()
    reconstructed_display = torch.clamp(reconstructed_display, 0, 1)
    
    mask_display = mask[0].cpu()
    
    # åˆ›å»ºæ©ç å›¾åƒï¼ˆè¢«æ©ç›–çš„åŒºåŸŸæ˜¾ç¤ºä¸ºç°è‰²ï¼‰
    masked_img = original_display.clone()
    masked_img = masked_img * (1 - mask_display) + mask_display * 0.5
    
    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(12, 12))
    
    # åŸå§‹å›¾åƒ
    axes[0, 0].imshow(original_display.permute(1, 2, 0))
    axes[0, 0].set_title('åŸå§‹å›¾åƒ', fontsize=14)
    axes[0, 0].axis('off')
    
    # æ©ç å›¾åƒ
    axes[0, 1].imshow(masked_img.permute(1, 2, 0))
    axes[0, 1].set_title(f'æ©ç å›¾åƒ ({mask_ratio*100:.0f}% è¢«æ©ç›–)', fontsize=14)
    axes[0, 1].axis('off')
    
    # é‡å»ºå›¾åƒ
    axes[1, 0].imshow(reconstructed_display.permute(1, 2, 0))
    axes[1, 0].set_title('é‡å»ºå›¾åƒ', fontsize=14)
    axes[1, 0].axis('off')
    
    # é‡å»ºè¯¯å·®
    error = torch.abs(original_display - reconstructed_display)
    error_display = error.mean(dim=0)  # å¹³å‡RGBé€šé“
    im = axes[1, 1].imshow(error_display, cmap='hot')
    axes[1, 1].set_title('é‡å»ºè¯¯å·® (è¶Šäº®è¯¯å·®è¶Šå¤§)', fontsize=14)
    axes[1, 1].axis('off')
    plt.colorbar(im, ax=axes[1, 1])
    
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    output_path = f'mae_reconstruction_{Path(image_path).stem}.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"âœ… å¯è§†åŒ–ç»“æœä¿å­˜: {output_path}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“Š é‡å»ºç»Ÿè®¡:")
    print(f"  é‡å»ºæŸå¤±: {loss.item():.4f}")
    print(f"  æ©ç æ¯”ä¾‹: {mask.float().mean().item():.2%}")
    print(f"  å¹³å‡é‡å»ºè¯¯å·®: {error.mean().item():.4f}")
    
    plt.show()
    return loss.item(), mask.float().mean().item()

def plot_training_curves(log_file='./output_m4/log.txt'):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    print(f"ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿: {log_file}")
    
    if not os.path.exists(log_file):
        print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
        return
    
    epochs = []
    losses = []
    lrs = []
    
    with open(log_file, 'r') as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                epochs.append(data['epoch'])
                losses.append(data['train_loss'])
                lrs.append(data.get('train_lr', 0))
            except:
                continue
    
    if not epochs:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
        return
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
    
    # æŸå¤±æ›²çº¿
    ax1.plot(epochs, losses, 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('MAE è®­ç»ƒæŸå¤±æ›²çº¿')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # å­¦ä¹ ç‡æ›²çº¿
    ax2.plot(epochs, lrs, 'r-', linewidth=2, label='å­¦ä¹ ç‡')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Learning Rate')
    ax2.set_title('å­¦ä¹ ç‡å˜åŒ–æ›²çº¿')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')
    print("âœ… è®­ç»ƒæ›²çº¿ä¿å­˜: training_curves.png")
    plt.show()

def analyze_model_outputs(model, test_images_dir='./test_dataset/train', device='mps'):
    """åˆ†ææ¨¡å‹åœ¨ä¸åŒç±»åˆ«ä¸Šçš„è¡¨ç°"""
    print(f"ğŸ” åˆ†ææ¨¡å‹è¾“å‡º: {test_images_dir}")
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    class_losses = {}
    
    # éå†æ¯ä¸ªç±»åˆ«
    for class_dir in sorted(os.listdir(test_images_dir)):
        class_path = os.path.join(test_images_dir, class_dir)
        if not os.path.isdir(class_path):
            continue
        
        losses = []
        
        # æµ‹è¯•è¯¥ç±»åˆ«çš„å‰5å¼ å›¾ç‰‡
        image_files = sorted(os.listdir(class_path))[:5]
        
        for img_file in image_files:
            img_path = os.path.join(class_path, img_file)
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = transform(img).unsqueeze(0).to(device)
                
                with torch.no_grad():
                    loss, _, _ = model(img_tensor, mask_ratio=0.75)
                    losses.append(loss.item())
            except:
                continue
        
        if losses:
            class_losses[class_dir] = np.mean(losses)
    
    # å¯è§†åŒ–ç±»åˆ«æŸå¤±
    if class_losses:
        classes = list(class_losses.keys())
        losses = list(class_losses.values())
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(classes, losses, color='skyblue', edgecolor='navy', alpha=0.7)
        plt.xlabel('ç±»åˆ«')
        plt.ylabel('å¹³å‡é‡å»ºæŸå¤±')
        plt.title('ä¸åŒç±»åˆ«çš„é‡å»ºæŸå¤±å¯¹æ¯”')
        plt.xticks(rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, loss in zip(bars, losses):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{loss:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('class_analysis.png', dpi=150, bbox_inches='tight')
        print("âœ… ç±»åˆ«åˆ†æä¿å­˜: class_analysis.png")
        plt.show()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š ç±»åˆ«åˆ†æç»“æœ:")
        for class_name, loss in sorted(class_losses.items(), key=lambda x: x[1]):
            print(f"  {class_name}: {loss:.4f}")

def create_reconstruction_grid(model, test_images_dir='./test_dataset/train', device='mps'):
    """åˆ›å»ºé‡å»ºç»“æœç½‘æ ¼"""
    print(f"ğŸ¨ åˆ›å»ºé‡å»ºç½‘æ ¼")
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    # æ”¶é›†æµ‹è¯•å›¾ç‰‡
    test_images = []
    class_dirs = sorted(os.listdir(test_images_dir))[:3]  # å‰3ä¸ªç±»åˆ«
    
    for class_dir in class_dirs:
        class_path = os.path.join(test_images_dir, class_dir)
        if os.path.isdir(class_path):
            img_files = sorted(os.listdir(class_path))[:2]  # æ¯ç±»2å¼ 
            for img_file in img_files:
                test_images.append(os.path.join(class_path, img_file))
    
    # åˆ›å»ºç½‘æ ¼
    n_images = min(6, len(test_images))
    fig, axes = plt.subplots(3, n_images, figsize=(n_images*3, 9))
    
    for i, img_path in enumerate(test_images[:n_images]):
        try:
            # åŠ è½½å›¾åƒ
            img = Image.open(img_path).convert('RGB')
            img_tensor = transform(img).unsqueeze(0).to(device)
            
            with torch.no_grad():
                loss, pred, mask = model(img_tensor, mask_ratio=0.75)
                reconstructed = model.unpatchify(pred)
                
                # åˆ›å»ºæ©ç 
                mask_vis = mask.detach()
                mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
                mask_vis = model.unpatchify(mask_vis)
            
            # è½¬æ¢æ˜¾ç¤ºæ ¼å¼
            original = torch.clamp(inv_normalize(img_tensor[0]).cpu(), 0, 1)
            recon = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
            masked = original * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
            
            # æ˜¾ç¤º
            axes[0, i].imshow(original.permute(1, 2, 0))
            axes[0, i].set_title(f'åŸå›¾ {i+1}')
            axes[0, i].axis('off')
            
            axes[1, i].imshow(masked.permute(1, 2, 0))
            axes[1, i].set_title(f'æ©ç  {i+1}')
            axes[1, i].axis('off')
            
            axes[2, i].imshow(recon.permute(1, 2, 0))
            axes[2, i].set_title(f'é‡å»º {i+1}\nLoss: {loss.item():.3f}')
            axes[2, i].axis('off')
            
        except Exception as e:
            print(f"å¤„ç†å›¾åƒ {img_path} æ—¶å‡ºé”™: {e}")
    
    plt.tight_layout()
    plt.savefig('reconstruction_grid.png', dpi=150, bbox_inches='tight')
    print("âœ… é‡å»ºç½‘æ ¼ä¿å­˜: reconstruction_grid.png")
    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ MAE è®­ç»ƒç»“æœå¯è§†åŒ–å·¥å…·")
    print("=" * 50)
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves()
    
    # 2. åŠ è½½æ¨¡å‹
    checkpoint_path = './output_m4/checkpoint-1.pth'  # æœ€åä¸€ä¸ªepochçš„checkpoint
    model = load_trained_model(checkpoint_path, device)
    
    # 3. å•å¼ å›¾ç‰‡é‡å»ºå¯è§†åŒ–
    test_img = './test_dataset/train/class_00/img_0000.png'
    if os.path.exists(test_img):
        visualize_mae_reconstruction(model, test_img, device)
    
    # 4. ç±»åˆ«åˆ†æ
    analyze_model_outputs(model, device=device)
    
    # 5. é‡å»ºç½‘æ ¼
    create_reconstruction_grid(model, device=device)
    
    print("\nğŸ‰ å¯è§†åŒ–å®Œæˆï¼")
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - training_curves.png: è®­ç»ƒæ›²çº¿")
    print("  - mae_reconstruction_*.png: å•å¼ å›¾ç‰‡é‡å»º")
    print("  - class_analysis.png: ç±»åˆ«åˆ†æ")
    print("  - reconstruction_grid.png: é‡å»ºç½‘æ ¼")

if __name__ == "__main__":
    main()


