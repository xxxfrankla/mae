#!/usr/bin/env python3
"""
æ”¹è¿›çš„MAEé‡å»ºæ¼”ç¤º
å°è¯•è§£å†³å™ªå£°é—®é¢˜ï¼Œæä¾›æ›´å¥½çš„é‡å»ºè´¨é‡
"""

import os
import sys
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import models_mae
from animediffusion_dataset_loader import create_animediffusion_dataloader
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def create_improved_mae_model():
    """åˆ›å»ºæ”¹è¿›çš„MAEæ¨¡å‹"""
    print("\nğŸ› ï¸ åˆ›å»ºæ”¹è¿›çš„MAEæ¨¡å‹...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    model = models_mae.mae_vit_base_patch16()
    
    # åŠ è½½ç¼–ç å™¨é¢„è®­ç»ƒæƒé‡
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    if os.path.exists(pretrain_path):
        print("ğŸ“¥ åŠ è½½ç¼–ç å™¨é¢„è®­ç»ƒæƒé‡...")
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        
        encoder_state_dict = {}
        for key, value in checkpoint['model'].items():
            if not key.startswith('decoder') and key != 'mask_token':
                encoder_state_dict[key] = value
        
        model.load_state_dict(encoder_state_dict, strict=False)
        print("âœ… ç¼–ç å™¨æƒé‡åŠ è½½æˆåŠŸ")
    
    # æ”¹è¿›è§£ç å™¨åˆå§‹åŒ–
    print("ğŸ¨ æ”¹è¿›è§£ç å™¨åˆå§‹åŒ–...")
    
    # 1. ä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–æƒé‡
    def init_decoder_weights(m):
        if isinstance(m, nn.Linear):
            # ä½¿ç”¨æ›´å°çš„æ ‡å‡†å·®
            nn.init.normal_(m.weight, std=0.01)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias, 0)
    
    # åªå¯¹è§£ç å™¨éƒ¨åˆ†åº”ç”¨æ–°çš„åˆå§‹åŒ–
    for name, module in model.named_modules():
        if name.startswith('decoder'):
            module.apply(init_decoder_weights)
    
    # 2. ç‰¹åˆ«å¤„ç†mask_tokenå’Œdecoder_pos_embed
    if hasattr(model, 'mask_token'):
        nn.init.normal_(model.mask_token, std=0.02)
    
    if hasattr(model, 'decoder_pos_embed'):
        nn.init.normal_(model.decoder_pos_embed, std=0.02)
    
    # 3. æœ€åçš„é¢„æµ‹å±‚ä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åŒ–
    if hasattr(model, 'decoder_pred'):
        nn.init.normal_(model.decoder_pred.weight, std=0.01)
        nn.init.constant_(model.decoder_pred.bias, 0)
    
    model = model.to(device)
    model.eval()
    
    print("âœ… æ”¹è¿›æ¨¡å‹åˆ›å»ºå®Œæˆ")
    return model, device

def create_simple_decoder_model():
    """åˆ›å»ºç®€åŒ–çš„è§£ç å™¨æ¨¡å‹"""
    print("\nğŸ¯ åˆ›å»ºç®€åŒ–è§£ç å™¨æ¨¡å‹...")
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    # ä½¿ç”¨é»˜è®¤å‚æ•°åˆ›å»ºæ¨¡å‹ï¼Œç„¶åæ‰‹åŠ¨ä¿®æ”¹è§£ç å™¨
    model = models_mae.mae_vit_base_patch16()
    
    # åŠ è½½ç¼–ç å™¨æƒé‡
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    if os.path.exists(pretrain_path):
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        
        encoder_state_dict = {}
        for key, value in checkpoint['model'].items():
            if not key.startswith('decoder') and key != 'mask_token':
                encoder_state_dict[key] = value
        
        model.load_state_dict(encoder_state_dict, strict=False)
        print("âœ… ç¼–ç å™¨æƒé‡åŠ è½½æˆåŠŸ")
    
    # ç®€åŒ–çš„è§£ç å™¨åˆå§‹åŒ–
    def simple_init(m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight, gain=0.1)  # æ›´å°çš„gain
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
    
    for name, module in model.named_modules():
        if name.startswith('decoder'):
            module.apply(simple_init)
    
    model = model.to(device)
    model.eval()
    
    print("âœ… ç®€åŒ–æ¨¡å‹åˆ›å»ºå®Œæˆ")
    return model, device

def test_improved_reconstruction():
    """æµ‹è¯•æ”¹è¿›çš„é‡å»ºæ•ˆæœ"""
    print("\nğŸ§ª æµ‹è¯•æ”¹è¿›çš„é‡å»ºæ•ˆæœ...")
    
    # åˆ›å»ºä¸åŒçš„æ¨¡å‹
    models = {}
    
    # 1. åŸå§‹æ¨¡å‹ï¼ˆéšæœºè§£ç å™¨ï¼‰
    print("ğŸ“¦ åŠ è½½åŸå§‹æ¨¡å‹...")
    original_model = models_mae.mae_vit_base_patch16()
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    if os.path.exists(pretrain_path):
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        encoder_state_dict = {}
        for key, value in checkpoint['model'].items():
            if not key.startswith('decoder') and key != 'mask_token':
                encoder_state_dict[key] = value
        original_model.load_state_dict(encoder_state_dict, strict=False)
    
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    original_model = original_model.to(device)
    original_model.eval()
    models['åŸå§‹æ¨¡å‹'] = original_model
    
    # 2. æ”¹è¿›åˆå§‹åŒ–æ¨¡å‹
    improved_model, _ = create_improved_mae_model()
    models['æ”¹è¿›åˆå§‹åŒ–'] = improved_model
    
    # 3. ç®€åŒ–è§£ç å™¨æ¨¡å‹
    simple_model, _ = create_simple_decoder_model()
    models['ç®€åŒ–è§£ç å™¨'] = simple_model
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_img = create_clean_test_image(device)
    
    # åå½’ä¸€åŒ–
    inv_normalize = transforms.Normalize(
        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
        std=[1/0.229, 1/0.224, 1/0.225]
    )
    
    # æµ‹è¯•é‡å»º
    fig, axes = plt.subplots(len(models), 4, figsize=(16, len(models)*4))
    
    if len(models) == 1:
        axes = axes.reshape(1, -1)
    
    for i, (model_name, model) in enumerate(models.items()):
        print(f"\n  ğŸ” æµ‹è¯• {model_name}...")
        
        with torch.no_grad():
            # åŸå§‹å›¾åƒ
            original_display = torch.clamp(inv_normalize(test_img[0]).cpu(), 0, 1)
            axes[i, 0].imshow(original_display.permute(1, 2, 0))
            axes[i, 0].set_title(f'{model_name}\nåŸå§‹å›¾åƒ')
            axes[i, 0].axis('off')
            
            # MAEé‡å»º
            loss, pred, mask = model(test_img, mask_ratio=0.75)
            reconstructed = model.unpatchify(pred)
            
            # åˆ›å»ºæ©ç å›¾åƒ
            mask_vis = mask.detach()
            mask_vis = mask_vis.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 * 3)
            mask_vis = model.unpatchify(mask_vis)
            
            masked_img = original_display * (1 - mask_vis[0].cpu()) + mask_vis[0].cpu() * 0.5
            
            axes[i, 1].imshow(masked_img.permute(1, 2, 0))
            axes[i, 1].set_title('æ©ç å›¾åƒ (75%)')
            axes[i, 1].axis('off')
            
            # é‡å»ºå›¾åƒ
            reconstructed_display = torch.clamp(inv_normalize(reconstructed[0]).cpu(), 0, 1)
            axes[i, 2].imshow(reconstructed_display.permute(1, 2, 0))
            axes[i, 2].set_title(f'é‡å»ºå›¾åƒ\næŸå¤±:{loss.item():.3f}')
            axes[i, 2].axis('off')
            
            # é‡å»ºè¯¯å·®
            error = torch.abs(original_display - reconstructed_display)
            error_display = error.mean(dim=0)
            im = axes[i, 3].imshow(error_display, cmap='hot')
            axes[i, 3].set_title(f'é‡å»ºè¯¯å·®\nå‡å€¼:{error.mean():.3f}')
            axes[i, 3].axis('off')
            plt.colorbar(im, ax=axes[i, 3], fraction=0.046, pad=0.04)
            
            print(f"    æŸå¤±: {loss.item():.4f}")
            print(f"    é¢„æµ‹èŒƒå›´: [{pred.min():.3f}, {pred.max():.3f}]")
            print(f"    é‡å»ºè¯¯å·®: {error.mean():.4f}")
    
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    output_path = 'improved_mae_reconstruction.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… æ”¹è¿›ç»“æœä¿å­˜: {output_path}")
    
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return output_path

def create_clean_test_image(device):
    """åˆ›å»ºæ¸…æ™°çš„æµ‹è¯•å›¾åƒ"""
    img = torch.zeros(1, 3, 224, 224, device=device)
    
    # åˆ›å»ºæ›´æ¸…æ™°çš„å›¾æ¡ˆ
    # 1. èƒŒæ™¯æ¸å˜
    for i in range(224):
        for j in range(224):
            r = (0.3 - 0.485) / 0.229
            g = (0.4 - 0.456) / 0.224
            b = (0.5 - 0.406) / 0.225
            img[0, 0, i, j] = r
            img[0, 1, i, j] = g
            img[0, 2, i, j] = b
    
    # 2. æ·»åŠ æ¸…æ™°çš„å‡ ä½•å›¾æ¡ˆ
    # åœ†å½¢
    y, x = torch.meshgrid(torch.arange(224, device=device), torch.arange(224, device=device), indexing='ij')
    circle_mask = (x - 112)**2 + (y - 112)**2 <= 50**2
    
    img[0, 0][circle_mask] = (0.8 - 0.485) / 0.229  # çº¢è‰²
    img[0, 1][circle_mask] = (0.2 - 0.456) / 0.224
    img[0, 2][circle_mask] = (0.2 - 0.406) / 0.225
    
    # çŸ©å½¢
    rect_mask = (x >= 50) & (x <= 100) & (y >= 50) & (y <= 100)
    img[0, 0][rect_mask] = (0.2 - 0.485) / 0.229
    img[0, 1][rect_mask] = (0.8 - 0.456) / 0.224  # ç»¿è‰²
    img[0, 2][rect_mask] = (0.2 - 0.406) / 0.225
    
    return img

def explain_noise_problem():
    """è§£é‡Šå™ªå£°é—®é¢˜"""
    print("\nğŸ“š å™ªå£°é—®é¢˜è¯¦ç»†è§£é‡Š:")
    print("=" * 60)
    
    print("ğŸ” é—®é¢˜æ ¹æº:")
    print("1. Facebookå®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹åªåŒ…å«ç¼–ç å™¨æƒé‡")
    print("2. è§£ç å™¨ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼Œæ²¡æœ‰å­¦ä¼šå¦‚ä½•é‡å»ºåƒç´ ")
    print("3. ç¼–ç å™¨è¾“å‡ºçš„ç‰¹å¾æ˜¯æŠ½è±¡çš„ï¼Œè§£ç å™¨ä¸çŸ¥é“å¦‚ä½•è§£é‡Š")
    print()
    
    print("ğŸ¯ ä¸ºä»€ä¹ˆä¼šäº§ç”Ÿå™ªå£°:")
    print("â€¢ éšæœºåˆå§‹åŒ–çš„è§£ç å™¨æƒé‡å¯¼è‡´è¾“å‡ºä¸ç¨³å®š")
    print("â€¢ è§£ç å™¨æ²¡æœ‰å­¦ä¼šä»ç‰¹å¾åˆ°åƒç´ çš„æ˜ å°„å…³ç³»")
    print("â€¢ é¢„æµ‹å€¼çš„èŒƒå›´å’Œåˆ†å¸ƒä¸åˆç†")
    print()
    
    print("ğŸ› ï¸ æ”¹è¿›ç­–ç•¥:")
    print("1. æ›´å¥½çš„æƒé‡åˆå§‹åŒ– - ä½¿ç”¨æ›´å°çš„æ ‡å‡†å·®")
    print("2. ç®€åŒ–è§£ç å™¨æ¶æ„ - å‡å°‘å‚æ•°æ•°é‡")
    print("3. æ¸è¿›å¼è®­ç»ƒ - å…ˆè®­ç»ƒç®€å•ä»»åŠ¡å†å¤æ‚ä»»åŠ¡")
    print("4. ä½¿ç”¨å®Œæ•´é¢„è®­ç»ƒæ¨¡å‹ - åŒ…å«è§£ç å™¨æƒé‡")
    print()
    
    print("ğŸ’¡ å®é™…è§£å†³æ–¹æ¡ˆ:")
    print("â€¢ ä¸‹è½½å®Œæ•´çš„MAEé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
    print("â€¢ åœ¨ä½ çš„æ•°æ®ä¸Šå¾®è°ƒè§£ç å™¨")
    print("â€¢ ä½¿ç”¨å…¶ä»–é¢„è®­ç»ƒçš„å›¾åƒé‡å»ºæ¨¡å‹")
    print("â€¢ è€ƒè™‘ä½¿ç”¨ViTè¿›è¡Œç‰¹å¾æå–è€Œéé‡å»º")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ› ï¸ æ”¹è¿›çš„MAEé‡å»ºæ¼”ç¤º")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    # æµ‹è¯•æ”¹è¿›çš„é‡å»º
    output_path = test_improved_reconstruction()
    
    # è§£é‡Šé—®é¢˜
    explain_noise_problem()
    
    print(f"\nğŸ¯ æ€»ç»“:")
    print("âœ… é€šè¿‡æ”¹è¿›åˆå§‹åŒ–ï¼Œå™ªå£°å¯èƒ½ä¼šå‡å°‘")
    print("âŒ ä½†æ ¹æœ¬é—®é¢˜æ˜¯è§£ç å™¨æ²¡æœ‰é¢„è®­ç»ƒ")
    print("ğŸ’¡ æœ€å¥½çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨å®Œæ•´é¢„è®­ç»ƒæ¨¡å‹")
    print(f"ğŸ“ æ”¹è¿›ç»“æœ: {output_path}")

if __name__ == "__main__":
    main()
