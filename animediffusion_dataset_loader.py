#!/usr/bin/env python3
"""
AnimeDiffusion æ•°æ®é›†åŠ è½½å™¨
é€‚é…é«˜è´¨é‡åŠ¨æ¼«æ•°æ®é›†åˆ° MAE è®­ç»ƒæµç¨‹
"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from datasets import load_dataset
from PIL import Image
import numpy as np

class AnimeDiffusionDataset(Dataset):
    """AnimeDiffusion æ•°æ®é›†åŒ…è£…å™¨"""
    
    def __init__(self, hf_dataset, transform=None, max_samples=None):
        """
        Args:
            hf_dataset: HuggingFace æ•°æ®é›†
            transform: å›¾åƒå˜æ¢
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        self.dataset = hf_dataset
        self.transform = transform
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        if max_samples is not None:
            self.length = min(max_samples, len(hf_dataset))
        else:
            self.length = len(hf_dataset)
        
        print(f"ğŸ“Š AnimeDiffusion æ•°æ®é›†: {self.length} å¼ é«˜è´¨é‡åŠ¨æ¼«å›¾ç‰‡")

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        try:
            sample = self.dataset[idx]
            
            # è·å–å›¾åƒ
            image = sample['image']
            
            # ç¡®ä¿æ˜¯PILå›¾åƒ
            if not isinstance(image, Image.Image):
                if isinstance(image, np.ndarray):
                    image = Image.fromarray(image)
                else:
                    # åˆ›å»ºé»˜è®¤å›¾åƒ
                    image = Image.new('RGB', (1920, 1080), color='black')
            
            # å¤„ç†RGBAå›¾åƒ
            if image.mode == 'RGBA':
                # åˆ›å»ºç™½è‰²èƒŒæ™¯
                background = Image.new('RGB', image.size, (255, 255, 255))
                background.paste(image, mask=image.split()[-1])  # ä½¿ç”¨alphaé€šé“ä½œä¸ºmask
                image = background
            elif image.mode != 'RGB':
                image = image.convert('RGB')
            
            # åº”ç”¨å˜æ¢
            if self.transform:
                image = self.transform(image)
            
            # MAE ä¸éœ€è¦æ ‡ç­¾ï¼Œè¿”å› 0 ä½œä¸ºå ä½ç¬¦
            return image, 0
            
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤å›¾åƒ
            default_img = Image.new('RGB', (1920, 1080), color='black')
            if self.transform:
                default_img = self.transform(default_img)
            return default_img, 0

def create_animediffusion_dataloader(batch_size=8, num_workers=4, max_samples=None, input_size=224):
    """åˆ›å»º AnimeDiffusion æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨"""
    
    print(f"ğŸŒ åˆ›å»º AnimeDiffusion æ•°æ®é›†åŠ è½½å™¨...")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  è¾“å…¥å°ºå¯¸: {input_size}x{input_size}")
    if max_samples:
        print(f"  æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    
    # åŠ è½½ HuggingFace æ•°æ®é›†
    try:
        ds = load_dataset("Mercity/AnimeDiffusion_Dataset")
        train_dataset = ds['train']
        print(f"âœ… AnimeDiffusion æ•°æ®é›†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    # å®šä¹‰å›¾åƒå˜æ¢ï¼ˆé’ˆå¯¹é«˜åˆ†è¾¨ç‡å›¾åƒä¼˜åŒ–ï¼‰
    transform = transforms.Compose([
        # å…ˆresizeåˆ°è¾ƒå¤§å°ºå¯¸ä¿æŒç»†èŠ‚
        transforms.Resize((input_size*2, input_size*2), interpolation=transforms.InterpolationMode.BICUBIC),
        # ç„¶åéšæœºè£å‰ªåˆ°ç›®æ ‡å°ºå¯¸
        transforms.RandomCrop((input_size, input_size)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åˆ›å»ºåŒ…è£…æ•°æ®é›†
    anime_dataset = AnimeDiffusionDataset(train_dataset, transform=transform, max_samples=max_samples)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        anime_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=False,  # MPS ä¸æ”¯æŒ pin_memory
        drop_last=True
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  æ€»æ‰¹æ¬¡æ•°: {len(dataloader)}")
    
    return dataloader, anime_dataset

def test_animediffusion_dataloader():
    """æµ‹è¯• AnimeDiffusion æ•°æ®åŠ è½½å™¨"""
    print("ğŸ§ª æµ‹è¯• AnimeDiffusion æ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºå°è§„æ¨¡æµ‹è¯•åŠ è½½å™¨
    dataloader, dataset = create_animediffusion_dataloader(
        batch_size=4, 
        max_samples=50,  # åªæµ‹è¯•50å¼ å›¾ç‰‡
        input_size=224
    )
    
    if dataloader is None:
        return
    
    # æµ‹è¯•åŠ è½½å‡ ä¸ªæ‰¹æ¬¡
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    
    for i, (images, labels) in enumerate(dataloader):
        print(f"  æ‰¹æ¬¡ {i+1}:")
        print(f"    å›¾åƒå½¢çŠ¶: {images.shape}")
        print(f"    å›¾åƒèŒƒå›´: [{images.min():.3f}, {images.max():.3f}]")
        print(f"    æ•°æ®ç±»å‹: {images.dtype}")
        
        # æµ‹è¯•åœ¨è®¾å¤‡ä¸Šè¿è¡Œ
        images = images.to(device)
        print(f"    è®¾å¤‡: {images.device}")
        
        if i >= 2:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
            break
    
    print("âœ… AnimeDiffusion æ•°æ®åŠ è½½å™¨æµ‹è¯•é€šè¿‡!")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ AnimeDiffusion æ•°æ®é›†åŠ è½½å™¨æµ‹è¯•")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_animediffusion_dataloader()

if __name__ == "__main__":
    main()

