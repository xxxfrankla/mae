#!/usr/bin/env python3
"""
ä½¿ç”¨çœŸå®Animeæ•°æ®é›†æ¼”ç¤ºMAEç¼–ç å™¨vsè§£ç å™¨
å›ç­”é—®é¢˜ï¼šåªç”¨ç¼–ç å™¨èƒ½å¦é‡å»ºå›¾åƒï¼Ÿ
"""

import os
import sys
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torchvision.transforms as transforms
import models_mae
import random

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_real_anime_samples():
    """ä»çœŸå®animeæ•°æ®é›†åŠ è½½æ ·æœ¬"""
    print("ğŸ¨ ä»çœŸå®Animeæ•°æ®é›†åŠ è½½æ ·æœ¬...")
    
    dataset_path = '/Users/tdu/Documents/GitHub/mae/test_dataset/train'
    
    if not os.path.exists(dataset_path):
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        return []
    
    # è·å–æ‰€æœ‰ç±»åˆ«
    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]
    classes.sort()
    
    print(f"  å‘ç° {len(classes)} ä¸ªç±»åˆ«: {classes}")
    
    images = []
    
    # ä»æ¯ä¸ªç±»åˆ«éšæœºé€‰æ‹©ä¸€å¼ å›¾åƒ
    for class_name in classes[:3]:  # åªå–å‰3ä¸ªç±»åˆ«
        class_path = os.path.join(dataset_path, class_name)
        image_files = [f for f in os.listdir(class_path) if f.lower().endswith('.png')]
        
        if len(image_files) > 0:
            # éšæœºé€‰æ‹©ä¸€å¼ å›¾åƒ
            selected_file = random.choice(image_files)
            img_path = os.path.join(class_path, selected_file)
            
            try:
                img = Image.open(img_path).convert('RGB')
                # è°ƒæ•´å¤§å°åˆ°224x224
                img = img.resize((224, 224), Image.Resampling.LANCZOS)
                images.append((f"Anime_{class_name}", np.array(img) / 255.0))
                print(f"  âœ… åŠ è½½: {class_name}/{selected_file}")
            except Exception as e:
                print(f"  âŒ åŠ è½½å¤±è´¥ {img_path}: {e}")
    
    return images

def load_mae_model():
    """åŠ è½½MAEæ¨¡å‹"""
    print("\nğŸ¤– åŠ è½½MAEæ¨¡å‹...")
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print(f"âœ… ä½¿ç”¨ Apple Silicon MPS")
    else:
        device = torch.device('cpu')
        print(f"âœ… ä½¿ç”¨ CPU")
    
    # åˆ›å»ºæ¨¡å‹
    model = models_mae.mae_vit_base_patch16()
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆåªæœ‰ç¼–ç å™¨éƒ¨åˆ†ï¼‰
    pretrain_path = 'pretrained_models/mae_pretrain_vit_base.pth'
    if os.path.exists(pretrain_path):
        print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrain_path}")
        checkpoint = torch.load(pretrain_path, map_location='cpu')
        
        # åªåŠ è½½ç¼–ç å™¨æƒé‡
        encoder_state_dict = {}
        for key, value in checkpoint['model'].items():
            if not key.startswith('decoder') and key != 'mask_token':
                encoder_state_dict[key] = value
        
        model.load_state_dict(encoder_state_dict, strict=False)
        print("âœ… ç¼–ç å™¨æƒé‡åŠ è½½æˆåŠŸ")
    else:
        print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
    
    model = model.to(device)
    model.eval()
    
    return model, device

def demonstrate_why_encoder_cannot_reconstruct(model, device, images):
    """è¯¦ç»†æ¼”ç¤ºä¸ºä»€ä¹ˆç¼–ç å™¨æ— æ³•é‡å»ºå›¾åƒ"""
    print("\nğŸ” è¯¦ç»†åˆ†æï¼šä¸ºä»€ä¹ˆç¼–ç å™¨æ— æ³•é‡å»ºå›¾åƒï¼Ÿ")
    
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    num_images = len(images)
    fig, axes = plt.subplots(5, num_images, figsize=(num_images*4, 20))
    
    if num_images == 1:
        axes = axes.reshape(-1, 1)
    
    for i, (name, img) in enumerate(images):
        print(f"\n  åˆ†æå›¾åƒ: {name}")
        
        # è½¬æ¢ä¸ºtensor
        img_tensor = transform(img.astype(np.float32)).unsqueeze(0).to(device)
        
        with torch.no_grad():
            # 1. åŸå§‹å›¾åƒ
            axes[0, i].imshow(img)
            axes[0, i].set_title(f'{name}\nåŸå§‹å›¾åƒ (224Ã—224Ã—3)')
            axes[0, i].axis('off')
            
            # 2. å›¾åƒåˆ†å—å¯è§†åŒ–
            print("    ğŸ“¦ å›¾åƒåˆ†å—è¿‡ç¨‹...")
            patches = model.patchify(img_tensor)  # (N, L, patch_size**2 * 3)
            print(f"      åˆ†å—åå½¢çŠ¶: {patches.shape}")  # (1, 196, 768)
            
            # å¯è§†åŒ–å‰16ä¸ªpatches
            patch_grid = np.zeros((4*16, 4*16, 3))
            for p in range(16):
                patch_data = patches[0, p].cpu().numpy().reshape(16, 16, 3)
                # åå½’ä¸€åŒ–æ˜¾ç¤º
                patch_data = patch_data * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
                patch_data = np.clip(patch_data, 0, 1)
                
                row, col = p // 4, p % 4
                patch_grid[row*16:(row+1)*16, col*16:(col+1)*16] = patch_data
            
            axes[1, i].imshow(patch_grid)
            axes[1, i].set_title('å‰16ä¸ªPatches\n(16Ã—16åƒç´ å—)')
            axes[1, i].axis('off')
            
            # 3. ç¼–ç å™¨ç‰¹å¾æå–
            print("    ğŸ§  ç¼–ç å™¨ç‰¹å¾æå–...")
            x = model.patch_embed(img_tensor)  # (N, L, D)
            x = x + model.pos_embed[:, 1:, :]  # æ·»åŠ ä½ç½®ç¼–ç 
            
            # æ·»åŠ cls token
            cls_token = model.cls_token + model.pos_embed[:, :1, :]
            cls_tokens = cls_token.expand(x.shape[0], -1, -1)
            x = torch.cat((cls_tokens, x), dim=1)
            
            # é€šè¿‡ç¼–ç å™¨å±‚
            for blk in model.blocks:
                x = blk(x)
            encoded_features = model.norm(x)
            
            print(f"      ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {encoded_features.shape}")  # (1, 197, 768)
            
            # å¯è§†åŒ–ç¼–ç å™¨ç‰¹å¾
            patch_features = encoded_features[:, 1:, :].cpu().numpy()  # å»æ‰cls token
            feature_mean = patch_features[0].mean(axis=1).reshape(14, 14)
            
            im1 = axes[2, i].imshow(feature_mean, cmap='viridis')
            axes[2, i].set_title('ç¼–ç å™¨ç‰¹å¾\n(æŠ½è±¡è¯­ä¹‰è¡¨ç¤º)')
            axes[2, i].axis('off')
            plt.colorbar(im1, ax=axes[2, i], fraction=0.046, pad=0.04)
            
            # 4. å°è¯•"é‡å»º"çš„é—®é¢˜
            print("    âŒ å°è¯•ä»ç‰¹å¾é‡å»º...")
            
            # ç¼–ç å™¨ç‰¹å¾æ˜¯768ç»´çš„æŠ½è±¡è¡¨ç¤º
            # è€ŒåŸå§‹patchæ˜¯16Ã—16Ã—3=768ç»´çš„åƒç´ å€¼
            # è™½ç„¶ç»´åº¦ç›¸åŒï¼Œä½†è¯­ä¹‰å®Œå…¨ä¸åŒï¼
            
            # é”™è¯¯çš„"é‡å»º"å°è¯•ï¼šç›´æ¥å°†ç‰¹å¾å½“ä½œåƒç´ 
            fake_reconstruction = patch_features[0].reshape(14, 14, 768)[:, :, :3]
            fake_reconstruction = (fake_reconstruction - fake_reconstruction.min()) / (fake_reconstruction.max() - fake_reconstruction.min())
            
            axes[3, i].imshow(fake_reconstruction)
            axes[3, i].set_title('é”™è¯¯çš„"é‡å»º"\n(ç‰¹å¾â‰ åƒç´ !)')
            axes[3, i].axis('off')
            
            # 5. è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦è§£ç å™¨
            axes[4, i].text(0.5, 0.9, 'ğŸ§  ç¼–ç å™¨è¾“å‡º:', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=12, weight='bold')
            axes[4, i].text(0.5, 0.8, '768ç»´æŠ½è±¡ç‰¹å¾', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=11)
            axes[4, i].text(0.5, 0.7, '(è¯­ä¹‰ä¿¡æ¯)', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=10, style='italic')
            
            axes[4, i].text(0.5, 0.5, 'â‰ ', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=20, color='red')
            
            axes[4, i].text(0.5, 0.3, 'ğŸ¨ éœ€è¦çš„è¾“å‡º:', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=12, weight='bold')
            axes[4, i].text(0.5, 0.2, '768ç»´åƒç´ å€¼', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=11)
            axes[4, i].text(0.5, 0.1, '(16Ã—16Ã—3 RGB)', ha='center', va='center',
                           transform=axes[4, i].transAxes, fontsize=10, style='italic')
            
            axes[4, i].set_xlim(0, 1)
            axes[4, i].set_ylim(0, 1)
            axes[4, i].axis('off')
            axes[4, i].set_title('éœ€è¦è§£ç å™¨è½¬æ¢!')
            
            print(f"      ç‰¹å¾ç»Ÿè®¡: å‡å€¼={encoded_features.mean().item():.4f}, æ ‡å‡†å·®={encoded_features.std().item():.4f}")
    
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    output_path = 'real_anime_encoder_analysis.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… åˆ†æç»“æœå·²ä¿å­˜: {output_path}")
    
    try:
        plt.show()
    except:
        print("ğŸ’¡ å¦‚æœè¦æŸ¥çœ‹å›¾åƒï¼Œè¯·åœ¨æ”¯æŒå›¾å½¢ç•Œé¢çš„ç¯å¢ƒä¸­è¿è¡Œ")
    
    return output_path

def explain_mae_detailed():
    """è¯¦ç»†è§£é‡ŠMAEå·¥ä½œåŸç†"""
    print("\nğŸ“š MAEè¯¦ç»†å·¥ä½œåŸç†:")
    print("=" * 60)
    
    print("ğŸ” 1. ç¼–ç å™¨ (Encoder) - ç‰¹å¾æå–å™¨:")
    print("   è¾“å…¥: å¯è§çš„å›¾åƒpatches (25%)")
    print("   å¤„ç†: å¤šå±‚Transformer â†’ æŠ½è±¡è¯­ä¹‰ç‰¹å¾")
    print("   è¾“å‡º: 768ç»´ç‰¹å¾å‘é‡ (æ¯ä¸ªpatch)")
    print("   ä½œç”¨: ç†è§£å›¾åƒå†…å®¹ï¼Œä½†ä¸èƒ½ç”Ÿæˆåƒç´ ")
    print()
    
    print("ğŸ¨ 2. è§£ç å™¨ (Decoder) - åƒç´ ç”Ÿæˆå™¨:")
    print("   è¾“å…¥: ç¼–ç å™¨ç‰¹å¾ + mask tokens")
    print("   å¤„ç†: è½»é‡çº§Transformer â†’ åƒç´ é¢„æµ‹")
    print("   è¾“å‡º: 768ç»´åƒç´ å€¼ (16Ã—16Ã—3)")
    print("   ä½œç”¨: ä»è¯­ä¹‰ç‰¹å¾é‡å»ºå…·ä½“åƒç´ ")
    print()
    
    print("ğŸ­ 3. ä¸ºä»€ä¹ˆç¼–ç å™¨æ— æ³•é‡å»º:")
    print("   â€¢ ç¼–ç å™¨å­¦ä¹ çš„æ˜¯æŠ½è±¡è¯­ä¹‰ç‰¹å¾")
    print("   â€¢ ç‰¹å¾è¡¨ç¤ºç‰©ä½“ã€çº¹ç†ã€å…³ç³»ç­‰é«˜çº§æ¦‚å¿µ")
    print("   â€¢ åƒç´ æ˜¯å…·ä½“çš„é¢œè‰²å€¼ (0-255)")
    print("   â€¢ è¯­ä¹‰ç‰¹å¾ â‰  åƒç´ å€¼ (è™½ç„¶ç»´åº¦å¯èƒ½ç›¸åŒ)")
    print()
    
    print("ğŸ”„ 4. å®Œæ•´MAEè®­ç»ƒè¿‡ç¨‹:")
    print("   Step 1: éšæœºæ©ç 75%çš„patches")
    print("   Step 2: ç¼–ç å™¨å¤„ç†å¯è§patches â†’ ç‰¹å¾")
    print("   Step 3: è§£ç å™¨æ¥æ”¶ç‰¹å¾+mask tokens â†’ é‡å»º")
    print("   Step 4: è®¡ç®—é‡å»ºæŸå¤±ï¼Œåå‘ä¼ æ’­è®­ç»ƒ")
    print()
    
    print("ğŸ’¡ 5. å…³é”®æ´å¯Ÿ:")
    print("   â€¢ ç¼–ç å™¨ä¸“æ³¨äºç†è§£ (understanding)")
    print("   â€¢ è§£ç å™¨ä¸“æ³¨äºç”Ÿæˆ (generation)")
    print("   â€¢ ä¸¤è€…åˆ†å·¥åˆä½œï¼Œç¼ºä¸€ä¸å¯")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ çœŸå®Animeæ•°æ®é›† - MAEç¼–ç å™¨æ·±åº¦åˆ†æ")
    print("=" * 60)
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    random.seed(42)  # ç¡®ä¿å¯é‡ç°
    
    # åŠ è½½çœŸå®animeæ•°æ®é›†æ ·æœ¬
    anime_images = load_real_anime_samples()
    
    if len(anime_images) == 0:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°å›¾åƒï¼Œè¯·æ£€æŸ¥æ•°æ®é›†è·¯å¾„")
        return
    
    # åŠ è½½MAEæ¨¡å‹
    model, device = load_mae_model()
    
    # è¯¦ç»†æ¼”ç¤ºåˆ†æ
    output_path = demonstrate_why_encoder_cannot_reconstruct(model, device, anime_images)
    
    # è¯¦ç»†è§£é‡Š
    explain_mae_detailed()
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
    print(f"ğŸ“ ç»“æœå›¾åƒ: {output_path}")
    print("\nğŸ¯ æ ¸å¿ƒç»“è®º:")
    print("âŒ ç¼–ç å™¨è¾“å‡ºæŠ½è±¡è¯­ä¹‰ç‰¹å¾ï¼Œä¸æ˜¯åƒç´ å€¼")
    print("ğŸ¨ è§£ç å™¨è´Ÿè´£ä»ç‰¹å¾ç”Ÿæˆå…·ä½“åƒç´ ")
    print("ğŸ”„ MAE = ç¼–ç å™¨(ç†è§£) + è§£ç å™¨(ç”Ÿæˆ)")
    print("ğŸ’¡ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåªæœ‰ç¼–ç å™¨æ— æ³•é‡å»ºå›¾åƒ!")

if __name__ == "__main__":
    main()

